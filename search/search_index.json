{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Capfinder: Advanced RNA Cap Type Prediction Framework","text":"<p>Capfinder is a cutting-edge deep learning framework designed for accurate prediction of RNA cap types in mRNAs sequenced using Oxford Nanopore Technologies (ONT) SQK-RNA004 chemistry. By leveraging the power of native RNA sequencing data, Capfinder predicts the cap type on individual transcript molecules with high accuracy.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Pre-trained Model: Ready-to-use classifier for immediate cap type prediction on ONT RNA-seq data.</li> <li>Extensible Architecture: Advanced users can train the classifier on additional cap classes, allowing for customization and expansion.</li> <li>Comprehensive ML Pipeline: Includes data preparation, hyperparameter tuning, and model training.</li> <li>High Accuracy: State-of-the-art performance in distinguishing between various cap types.</li> <li>Flexibility: Supports both CNN-LSTM, CNN-LSTML-Attention, ResNet, and Transformer-based Encoder model architectures.</li> <li>Scalability: Designed to efficiently handle large-scale RNA sequencing datasets.</li> </ul>"},{"location":"#supported-cap-types","title":"Supported Cap Types","text":"<p>Capfinder's pre-trained model offers accurate out-of-the-box predictions for the following RNA cap structures:</p> <ol> <li>Cap0: Unmodified cap structure</li> <li>Cap1: Methylated at the 2'-O position of the first nucleotide</li> <li>Cap2: Methylated at the 2'-O position of the first and second nucleotides</li> <li>Cap2,-1: Methylated at the 2'-O position of the first and second nucleotides, with an additional methylation at the -1 position</li> </ol> <p>These cap types represent the most common modifications found in eukaryotic mRNAs. Capfinder's ability to distinguish between these structures enables researchers to gain valuable insights into RNA processing and regulation.</p> <p>For advanced users, Capfinder's extensible architecture allows for training on additional cap types, expanding its capabilities to meet specific research needs.</p>"},{"location":"all_caps_data/","title":"5. Making Training Data","text":""},{"location":"all_caps_data/#overview","title":"Overview","text":"<p>We now have cap signal data for the cap0-m6A class (<code>data__cap_0m6A.csv</code>). To retrain our classifier, we need cap signal data for all classes, and not just cap0-m6A class:</p> <ul> <li>Cap 0 (need data for this cap)</li> <li>Cap 1 (need data for this cap)</li> <li>Cap 2 (need data for this cap)</li> <li>Cap 2-1 (need data for this cap)</li> <li>Cap 0-m6A (you made this data <code>data__cap_0m6A.csv</code> in previous steps)</li> </ul>"},{"location":"all_caps_data/#why-retrain","title":"Why Retrain?","text":"<p>Capfinder uses retraining instead of transfer learning. We believe retraining the classifier from scratch is preferable to transfer learning for the following reasons:</p> <ol> <li>It's a simpler approach</li> <li>It avoids potential biases from the pretrained model</li> <li>It allows the model to learn optimal representations for all classes simultaneously</li> </ol>"},{"location":"all_caps_data/#steps-to-prepare-data","title":"Steps to Prepare Data","text":"<ol> <li> <p>Download cap signal data for existing caps (Cap 0, cap 1, cap 2, and cap 2-1) from these two links below:</p> <ul> <li>capfinder_training_data_p1</li> <li>capfinder_training_data_p2</li> </ul> </li> <li> <p>Create a data directory:</p> <p>Create a new directory (name it as, lets say, <code>caps_data_dir</code>)</p> </li> <li> <p>Extract downloaded zipped data:</p> <p>Extract the downloaded files -- <code>capfinder_training_data_p1.zip</code> and <code>capfinder_training_data_p2.zip</code> -- into <code>caps_data_dir</code> you created in step 2.</p> <p>Ensure that this directory has all 21 parts of the data (<code>data.tar.gz.part00.gpg</code> -- <code>data.tar.gz.part20.gpg</code>). These 21 parts are encrypted and you need a password to decrypt them first.</p> </li> <li> <p>Getting the password:</p> <p>For now we only want to share data with people who want to collaborate. If you wish you collaborate, please send an email to Eivind Valen and you will be sent the password.</p> </li> <li> <p>Decrypting the data:</p> <p>To decrypt the data use the following script:</p> script <pre><code>#!/bin/bash\n\n# Function to read password securely\nread_password() {\n    read -s -p \"Enter password for decryption: \" password\n    echo\n}\n\n# Function to get directory path\nget_directory() {\n    read -p \"Enter the path to the directory containing encrypted files: \" directory\n    if [ ! -d \"$directory\" ]; then\n        echo \"The specified path does not exist or is not a directory.\"\n        exit 1\n    fi\n}\n\n# Main script\necho \"Welcome to the Decrypt, Extract, and Cleanup script!\"\n\n# Get directory path\nget_directory\n\n# Get password\nread_password\n\n# Decrypt files\necho \"Decrypting files...\"\nfor file in \"$directory\"/data.tar.gz.part*.gpg; do\n    if [ -f \"$file\" ]; then\n        gpg --batch --yes --passphrase \"$password\" --decrypt \"$file\" &gt; \"${file%.gpg}\"\n        if [ $? -eq 0 ]; then\n            echo \"Decrypted: $file\"\n            rm \"$file\"\n        else\n            echo \"Failed to decrypt: $file\"\n            exit 1\n        fi\n    fi\ndone\n\n# Extract files\necho \"Extracting files...\"\ncat \"$directory\"/data.tar.gz.part* | tar xzvf - --transform='s|.*/||' -C \"$directory\"\n\n# Check if extraction was successful\nif [ $? -eq 0 ]; then\n    echo \"Extraction completed successfully.\"\n\n    # Remove the decrypted compressed files\n    echo \"Removing decrypted compressed files...\"\n    rm \"$directory\"/data.tar.gz.part*\n\n    if [ $? -eq 0 ]; then\n        echo \"Decrypted compressed files have been removed.\"\n    else\n        echo \"Failed to remove some or all of the decrypted compressed files.\"\n    fi\nelse\n    echo \"Extraction failed. Decrypted files have not been removed.\"\n    exit 1\nfi\n\necho \"Process completed successfully.\"\n</code></pre> <p>Just download the script and run it. It will ask for the password for decrytion and the path of the directory where encrypted data is currently residing. The script will decrypt, combine, and extract the tar files.</p> <p>If you are successful, you should see the following contents in your <code>caps_data_dir</code>:</p> <ul> <li><code>data__cap_0_run1.csv</code></li> <li><code>data__cap_0_run2.csv</code></li> <li><code>data__cap_1_run1.csv</code></li> <li><code>data__cap_2_run1.csv</code></li> <li><code>data__cap_2-1_run1.csv</code></li> <li><code>data__cap_2-1_run2.csv</code></li> </ul> </li> <li> <p>Add new cap0-m6A data:</p> <p>Place the <code>data__cap_0m6A.csv</code> file in the same data directory.</p> </li> <li> <p>Verify data:</p> <p>Ensure your <code>caps_data_dir</code> directory now contains CSV files for all cap classes:</p> <ul> <li><code>data__cap_0_run1.csv</code></li> <li><code>data__cap_0_run2.csv</code></li> <li><code>data__cap_1_run1.csv</code></li> <li><code>data__cap_2_run1.csv</code></li> <li><code>data__cap_2-1_run1.csv</code></li> <li><code>data__cap_2-1_run2.csv</code></li> <li><code>data__cap_0m6A.csv</code></li> </ul> </li> </ol> <p>The suffix <code>run1</code> and <code>run2</code> shows that the data was acquired from two different sequencing runs. If later on, you acquire more data for <code>cap0-m6A</code> class, you can rename the two files as <code>data__cap_0m6A_run1.csv</code> and <code>data__cap_0m6A_run2.csv</code></p>"},{"location":"all_caps_data/#next-steps","title":"Next Steps","text":"<p>With all cap signal data prepared in a single directory, you're now ready to proceed with retraining the Capfinder classifier. We will next use a training pipeline that processes all these files in batches, does hyperparameter tuning, and final training.</p>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#src.capfinder.align","title":"<code>align</code>","text":"<p>The module aligns a reference sequence to a read sequence using Parasail. The module also provides functions to generate alignment strings and chunks for pretty printing.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.align.PairwiseAlignment","title":"<code>PairwiseAlignment</code>  <code>dataclass</code>","text":"<p>Pairwise alignment with semi-global alignment allowing for gaps at the start and end of the query sequence.</p> Source code in <code>src/capfinder/align.py</code> <pre><code>@dataclass\nclass PairwiseAlignment:\n    \"\"\"\n    Pairwise alignment with semi-global alignment allowing for gaps at the\n    start and end of the query sequence.\n    \"\"\"\n\n    ref_start: int\n    ref_end: int\n    query_start: int\n    query_end: int\n    cigar_pysam: CigarTuplesPySam\n    cigar_sam: CigarTuplesSam\n\n    def __init__(\n        self,\n        ref_start: int,\n        ref_end: int,\n        query_start: int,\n        query_end: int,\n        cigar_pysam: CigarTuplesPySam,\n        cigar_sam: CigarTuplesSam,\n    ):\n        \"\"\"\n        Initializes a PairwiseAlignment object.\n\n        Args:\n            ref_start (int): The starting position of the alignment in the reference sequence.\n            ref_end (int): The ending position of the alignment in the reference sequence.\n            query_start (int): The starting position of the alignment in the query sequence.\n            query_end (int): The ending position of the alignment in the query sequence.\n            cigar_pysam (CigarTuplesPySam): A list of tuples representing the CIGAR string in the Pysam format.\n            cigar_sam (CigarTuplesSam): A list of tuples representing the CIGAR string in the SAM format.\n        \"\"\"\n        self.ref_start = ref_start\n        self.ref_end = ref_end\n        self.query_start = query_start\n        self.query_end = query_end\n        self.cigar_pysam = cigar_pysam\n        self.cigar_sam = cigar_sam\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.PairwiseAlignment.__init__","title":"<code>__init__(ref_start: int, ref_end: int, query_start: int, query_end: int, cigar_pysam: CigarTuplesPySam, cigar_sam: CigarTuplesSam)</code>","text":"<p>Initializes a PairwiseAlignment object.</p> <p>Parameters:</p> Name Type Description Default <code>ref_start</code> <code>int</code> <p>The starting position of the alignment in the reference sequence.</p> required <code>ref_end</code> <code>int</code> <p>The ending position of the alignment in the reference sequence.</p> required <code>query_start</code> <code>int</code> <p>The starting position of the alignment in the query sequence.</p> required <code>query_end</code> <code>int</code> <p>The ending position of the alignment in the query sequence.</p> required <code>cigar_pysam</code> <code>CigarTuplesPySam</code> <p>A list of tuples representing the CIGAR string in the Pysam format.</p> required <code>cigar_sam</code> <code>CigarTuplesSam</code> <p>A list of tuples representing the CIGAR string in the SAM format.</p> required Source code in <code>src/capfinder/align.py</code> <pre><code>def __init__(\n    self,\n    ref_start: int,\n    ref_end: int,\n    query_start: int,\n    query_end: int,\n    cigar_pysam: CigarTuplesPySam,\n    cigar_sam: CigarTuplesSam,\n):\n    \"\"\"\n    Initializes a PairwiseAlignment object.\n\n    Args:\n        ref_start (int): The starting position of the alignment in the reference sequence.\n        ref_end (int): The ending position of the alignment in the reference sequence.\n        query_start (int): The starting position of the alignment in the query sequence.\n        query_end (int): The ending position of the alignment in the query sequence.\n        cigar_pysam (CigarTuplesPySam): A list of tuples representing the CIGAR string in the Pysam format.\n        cigar_sam (CigarTuplesSam): A list of tuples representing the CIGAR string in the SAM format.\n    \"\"\"\n    self.ref_start = ref_start\n    self.ref_end = ref_end\n    self.query_start = query_start\n    self.query_end = query_end\n    self.cigar_pysam = cigar_pysam\n    self.cigar_sam = cigar_sam\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.align","title":"<code>align(query_seq: str, target_seq: str, pretty_print_alns: bool) -&gt; Tuple[str, str, str, int]</code>","text":"<p>Main function call to align two sequences and print the alignment.</p> <p>Parameters:</p> Name Type Description Default <code>query_seq</code> <code>str</code> <p>The query sequence.</p> required <code>target_seq</code> <code>str</code> <p>The target/reference sequence.</p> required <code>pretty_print_alns</code> <code>bool</code> <p>Whether to print the alignment in a pretty format.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str, str, int]</code> <p>Tuple[str, str, str]: A tuple containing three strings: 1. The aligned query sequence with gaps. 2. The visual representation of the alignment with '|' for matches, '/' for mismatches,     and ' ' for gaps or insertions. 3. The aligned target sequence with gaps. 4. The alignment score.</p> Source code in <code>src/capfinder/align.py</code> <pre><code>def align(\n    query_seq: str, target_seq: str, pretty_print_alns: bool\n) -&gt; Tuple[str, str, str, int]:\n    \"\"\"\n    Main function call to align two sequences and print the alignment.\n\n    Args:\n        query_seq (str): The query sequence.\n        target_seq (str): The target/reference sequence.\n        pretty_print_alns (bool): Whether to print the alignment in a pretty format.\n\n    Returns:\n        Tuple[str, str, str]: A tuple containing three strings:\n            1. The aligned query sequence with gaps.\n            2. The visual representation of the alignment with '|' for matches, '/' for mismatches,\n                and ' ' for gaps or insertions.\n            3. The aligned target sequence with gaps.\n            4. The alignment score.\n\n    \"\"\"\n    # Perform the alignment\n    alignment = parasail_align(query=query_seq, ref=target_seq)\n    alignment_score = alignment.score\n    alignment = trim_parasail_alignment(alignment)\n    # Generate the aligned strings\n    aln_query, aln, aln_target = make_alignment_strings(\n        query_seq, target_seq, alignment\n    )\n\n    # Print the alignment in a pretty format if required\n    if pretty_print_alns:\n        print(\"Alignment score:\", alignment_score)\n        chunked_aln_str = make_alignment_chunks(\n            aln_target, aln_query, aln, chunk_size=40\n        )\n        print(chunked_aln_str)\n        return (\n            \"\",\n            \"\",\n            chunked_aln_str,\n            alignment_score,\n        )\n    else:\n        return aln_query, aln, aln_target, alignment_score\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.cigartuples_from_string","title":"<code>cigartuples_from_string(cigarstring: str) -&gt; CigarTuplesPySam</code>","text":"<p>Returns pysam-style list of (op, count) tuples from a cigarstring.</p> Source code in <code>src/capfinder/align.py</code> <pre><code>def cigartuples_from_string(cigarstring: str) -&gt; CigarTuplesPySam:\n    \"\"\"\n    Returns pysam-style list of (op, count) tuples from a cigarstring.\n    \"\"\"\n    return [\n        (CODE_TO_OP[m.group(2)], int(m.group(1)))\n        for m in re.finditer(CIGAR_STRING_PATTERN, cigarstring)\n    ]\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.make_alignment_chunks","title":"<code>make_alignment_chunks(target: str, query: str, alignment: str, chunk_size: int) -&gt; str</code>","text":"<p>Divide three strings (target, query, and alignment) into chunks of the specified length and print them as triplets with the specified prefixes and a one-line gap between each triplet.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>The target/reference string.</p> required <code>query</code> <code>str</code> <p>The query string.</p> required <code>alignment</code> <code>str</code> <p>The alignment string.</p> required <code>chunk_size</code> <code>int</code> <p>The desired chunk size.</p> required <p>Returns:</p> Name Type Description <code>aln_string</code> <code>str</code> <p>The aligned strings in chunks with the specified prefix.</p> Source code in <code>src/capfinder/align.py</code> <pre><code>def make_alignment_chunks(\n    target: str, query: str, alignment: str, chunk_size: int\n) -&gt; str:\n    \"\"\"\n    Divide three strings (target, query, and alignment) into chunks of the specified length\n    and print them as triplets with the specified prefixes and a one-line gap between each triplet.\n\n    Args:\n        target (str): The target/reference string.\n        query (str): The query string.\n        alignment (str): The alignment string.\n        chunk_size (int): The desired chunk size.\n\n    Returns:\n        aln_string (str): The aligned strings in chunks with the specified prefix.\n    \"\"\"\n    # Check if chunk size is valid\n    if chunk_size &lt;= 0:\n        raise ValueError(\"Chunk size must be greater than zero\")\n\n    # Divide the strings into chunks\n    target_chunks = [\n        target[i : i + chunk_size] for i in range(0, len(target), chunk_size)\n    ]\n    query_chunks = [query[i : i + chunk_size] for i in range(0, len(query), chunk_size)]\n    alignment_chunks = [\n        alignment[i : i + chunk_size] for i in range(0, len(alignment), chunk_size)\n    ]\n\n    # Iterate over the triplets and print them\n    aln_string = \"\"\n    for t_chunk, q_chunk, a_chunk in zip(target_chunks, query_chunks, alignment_chunks):\n        aln_string += f\"QRY: {q_chunk}\\n\"\n        aln_string += f\"ALN: {a_chunk}\\n\"\n        aln_string += f\"REF: {t_chunk}\\n\\n\"\n\n    return aln_string\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.make_alignment_strings","title":"<code>make_alignment_strings(query: str, target: str, alignment: PairwiseAlignment) -&gt; Tuple[str, str, str]</code>","text":"<p>Generate alignment strings for the given query and target sequences based on a PairwiseAlignment object.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query sequence.</p> required <code>target</code> <code>str</code> <p>The target/reference sequence.</p> required <code>alignment</code> <code>PairwiseAlignment</code> <p>An object representing the alignment between query and target sequences.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str, str]</code> <p>Tuple[str, str, str]: A tuple containing three strings: 1. The aligned target sequence with gaps. 2. The aligned query sequence with gaps. 3. The visual representation of the alignment with '|' for matches, '/' for mismatches,    and ' ' for gaps or insertions.</p> Source code in <code>src/capfinder/align.py</code> <pre><code>def make_alignment_strings(\n    query: str, target: str, alignment: PairwiseAlignment\n) -&gt; Tuple[str, str, str]:\n    \"\"\"\n    Generate alignment strings for the given query and target sequences based on a PairwiseAlignment object.\n\n    Args:\n        query (str): The query sequence.\n        target (str): The target/reference sequence.\n        alignment (PairwiseAlignment): An object representing the alignment between query and target sequences.\n\n    Returns:\n        Tuple[str, str, str]: A tuple containing three strings:\n            1. The aligned target sequence with gaps.\n            2. The aligned query sequence with gaps.\n            3. The visual representation of the alignment with '|' for matches, '/' for mismatches,\n               and ' ' for gaps or insertions.\n    \"\"\"\n    ref_start = alignment.ref_start\n    ref_end = alignment.ref_end\n    query_start = alignment.query_start\n    cigar_sam = alignment.cigar_sam\n\n    # Initialize the strings\n    aln_query = \"\"\n    aln_target = \"\"\n    aln = \"\"\n    target_count = 0\n    query_count = 0\n\n    # Handle the start\n    if query_start != 0:\n        aln_target += \"-\" * query_start\n        aln_query += query[:query_start]\n        aln += \" \" * query_start\n        query_count += query_start\n\n    if ref_start != 0:\n        aln_target += target[:ref_start]\n        target_count = ref_start\n        aln_query += \"-\" * ref_start\n        aln += \" \" * ref_start\n\n    # Handle the middle\n    for operation, length in cigar_sam:\n        # Match: advance both target and query counts\n        if operation in (\"=\", \"M\"):\n            aln_target += target[target_count : target_count + length]\n            aln_query += query[query_count : query_count + length]\n            aln += \"|\" * length\n            target_count += length\n            query_count += length\n\n        # Insertion: advance query count only\n        elif operation == \"I\":\n            aln_target += \"-\" * length\n            aln_query += query[query_count : query_count + length]\n            aln += \" \" * length\n            query_count += length\n\n        # Deletion or gaps: advance target count only\n        # see: https://jef.works/blog/2017/03/28/CIGAR-strings-for-dummies/\n        elif operation in (\"D\", \"N\"):\n            aln_target += target[target_count : target_count + length]\n            aln_query += \"-\" * length\n            aln += \" \" * length\n            target_count += length\n\n        # Mismatch: advance both target and query counts\n        elif operation == \"X\":\n            aln_target += target[target_count : target_count + length]\n            aln_query += query[query_count : query_count + length]\n            aln += \"/\" * length\n            target_count += length\n            query_count += length\n\n    # Handle the end\n    ql = len(query)\n    tl = len(target)\n    target_remainder = tl - ref_end\n    if target_remainder:\n        aln_target += target[target_count:]\n        aln_query += target_remainder * \"-\"\n        aln += target_remainder * \" \"\n\n    end_dash_len = ql - query_count\n    if end_dash_len:\n        aln_target += \"-\" * end_dash_len\n        aln_query += query[query_count:]\n        aln += \" \" * end_dash_len\n        query_count += query_start\n\n    return aln_query, aln, aln_target\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.parasail_align","title":"<code>parasail_align(*, query: str, ref: str) -&gt; Any</code>","text":"<p>Semi-global alignment allowing for gaps at the start and end of the query sequence.</p> <p>:param query: str :param ref: str :return: PairwiseAlignment</p> Source code in <code>src/capfinder/align.py</code> <pre><code>def parasail_align(*, query: str, ref: str) -&gt; Any:\n    \"\"\"\n    Semi-global alignment allowing for gaps at the start and end of the query\n    sequence.\n\n    :param query: str\n    :param ref: str\n    :return: PairwiseAlignment\n    \"\"\"\n    alignment_result = parasail.sg_trace_scan_32(query, ref, 10, 2, parasail.dnafull)\n    return alignment_result\n</code></pre>"},{"location":"api_docs/#src.capfinder.align.trim_parasail_alignment","title":"<code>trim_parasail_alignment(alignment_result: Any) -&gt; PairwiseAlignment</code>","text":"<p>Trim the alignment result to remove leading and trailing gaps.</p> Source code in <code>src/capfinder/align.py</code> <pre><code>def trim_parasail_alignment(alignment_result: Any) -&gt; PairwiseAlignment:\n    \"\"\"\n    Trim the alignment result to remove leading and trailing gaps.\n    \"\"\"\n\n    try:\n        ref_start = 0\n        ref_end = alignment_result.len_ref\n        query_start = 0\n        query_end = alignment_result.len_query\n        fixed_start = False\n        fixed_end = False\n\n        cigar_string = alignment_result.cigar.decode.decode()\n        cigar_tuples = deque(cigartuples_from_string(cigar_string))\n\n        while not (fixed_start and fixed_end):\n            first_op, first_length = cigar_tuples[0]\n            if first_op in (1, 4):  # insert, soft-clip, increment query start\n                query_start += first_length\n                cigar_tuples.popleft()\n            elif first_op == 2:  # delete, increment reference start\n                ref_start += first_length\n                cigar_tuples.popleft()\n            else:\n                fixed_start = True\n\n            last_op, last_length = cigar_tuples[-1]\n            if last_op in (1, 4):  # decrement the query end\n                query_end -= last_length\n                cigar_tuples.pop()\n            elif last_op == 2:  # decrement the ref_end\n                ref_end -= last_length\n                cigar_tuples.pop()\n            else:\n                fixed_end = True\n\n        cigar_pysam = list(cigar_tuples)\n        cigar_sam = [(OP_TO_CODE[str(k)], v) for k, v in cigar_pysam]\n\n        return PairwiseAlignment(\n            ref_start=ref_start,\n            ref_end=ref_end,\n            query_start=query_start,\n            query_end=query_end,\n            cigar_pysam=cigar_pysam,\n            cigar_sam=cigar_sam,\n        )\n    except IndexError as e:\n        raise RuntimeError(\n            \"failed to find match operations in pairwise alignment\"\n        ) from e\n</code></pre>"},{"location":"api_docs/#src.capfinder.attention_cnnlstm_model","title":"<code>attention_cnnlstm_model</code>","text":""},{"location":"api_docs/#src.capfinder.attention_cnnlstm_model.CapfinderHyperModel","title":"<code>CapfinderHyperModel</code>","text":"<p>               Bases: <code>HyperModel</code></p> <p>Hypermodel for the Capfinder CNN-LSTM with Attention architecture.</p> <p>This model is designed for time series classification tasks, specifically for identifying RNA cap types. It combines Convolutional Neural Networks (CNNs) for local feature extraction, Long Short-Term Memory (LSTM) networks for sequence processing, and an attention mechanism to focus on the most relevant parts of the input sequence.</p> <p>The architecture is flexible and allows for hyperparameter tuning of the number of layers, units, and other key parameters.</p> <p>Attributes:</p> Name Type Description <code>input_shape</code> <code>Tuple[int, ...]</code> <p>The shape of the input data.</p> <code>n_classes</code> <code>int</code> <p>The number of classes for classification.</p> <code>encoder_model</code> <code>Optional[Model]</code> <p>Placeholder for a potential encoder model.</p> <p>Methods:</p> Name Description <code>build</code> <p>Constructs and returns a Keras model based on the provided        hyperparameters.</p> Source code in <code>src/capfinder/attention_cnnlstm_model.py</code> <pre><code>class CapfinderHyperModel(HyperModel):\n    \"\"\"\n    Hypermodel for the Capfinder CNN-LSTM with Attention architecture.\n\n    This model is designed for time series classification tasks, specifically for\n    identifying RNA cap types. It combines Convolutional Neural Networks (CNNs) for\n    local feature extraction, Long Short-Term Memory (LSTM) networks for sequence\n    processing, and an attention mechanism to focus on the most relevant parts of\n    the input sequence.\n\n    The architecture is flexible and allows for hyperparameter tuning of the number\n    of layers, units, and other key parameters.\n\n    Attributes:\n        input_shape (Tuple[int, ...]): The shape of the input data.\n        n_classes (int): The number of classes for classification.\n        encoder_model (Optional[Model]): Placeholder for a potential encoder model.\n\n    Methods:\n        build(hp): Constructs and returns a Keras model based on the provided\n                   hyperparameters.\n    \"\"\"\n\n    def __init__(self, input_shape: Tuple[int, int], n_classes: int) -&gt; None:\n        self.input_shape = input_shape\n        self.n_classes = n_classes\n        self.encoder_model = None\n\n    def build(self, hp: Any) -&gt; Model:\n        inputs = Input(shape=self.input_shape)\n        x = inputs\n\n        # Calculate the maximum number of conv layers based on input size\n        max_conv_layers = min(\n            int(math.log2(self.input_shape[0])) - 1, 5\n        )  # Limit to 5 layers max\n        conv_layers = hp.Int(\"conv_layers\", 1, max_conv_layers)\n\n        # Convolutional layers\n        for i in range(conv_layers):\n            # Dynamically adjust the range for filters based on the layer depth\n            max_filters = min(256, 32 * (2 ** (i + 1)))\n            filters = hp.Int(f\"filters_{i}\", 32, max_filters, step=32)\n\n            # Dynamically adjust the kernel size based on the current feature map size\n            current_size = x.shape[1]\n            max_kernel_size = min(7, current_size)\n            kernel_size = hp.Choice(\n                f\"kernel_size_{i}\", list(range(3, max_kernel_size + 1, 2))\n            )\n\n            x = Conv1D(\n                filters=filters,\n                kernel_size=kernel_size,\n                activation=\"relu\",\n                padding=\"same\",\n            )(x)\n\n            # Only apply MaxPooling if the current size is greater than 2\n            if current_size &gt; 2:\n                x = MaxPooling1D(pool_size=2)(x)\n\n            x = Dropout(hp.Float(f\"dropout_{i}\", 0.1, 0.5, step=0.1))(x)\n            x = BatchNormalization()(x)\n\n        # Calculate the maximum number of LSTM layers based on remaining sequence length\n        current_seq_length = x.shape[1]\n        max_lstm_layers = min(\n            int(math.log2(current_seq_length)) + 1, 3\n        )  # Limit to 3 LSTM layers max\n        lstm_layers = hp.Int(\"lstm_layers\", 1, max_lstm_layers)\n\n        # LSTM layers\n        for i in range(lstm_layers):\n            return_sequences = i &lt; lstm_layers - 1\n            max_lstm_units = min(256, 32 * (2 ** (i + 1)))\n            lstm_units = hp.Int(f\"lstm_units_{i}\", 32, max_lstm_units, step=32)\n            x = LSTM(\n                units=lstm_units,\n                return_sequences=return_sequences or i == lstm_layers - 1,\n            )(x)\n            x = Dropout(hp.Float(f\"lstm_dropout_{i}\", 0.1, 0.5, step=0.1))(x)\n            x = BatchNormalization()(x)\n\n        # Attention layer\n        x = AttentionLayer()(x)\n\n        # Fully connected layer\n        max_dense_units = min(256, x.shape[-1] * 2)\n        dense_units = hp.Int(\"dense_units\", 16, max_dense_units, step=16)\n        x = Dense(units=dense_units, activation=\"relu\")(x)\n        x = Dropout(hp.Float(\"dense_dropout\", 0.1, 0.5, step=0.1))(x)\n        outputs = Dense(self.n_classes, activation=\"softmax\")(x)\n\n        model = Model(inputs=inputs, outputs=outputs)\n\n        model.compile(\n            optimizer=Adam(\n                learning_rate=hp.Choice(\"learning_rate\", [1e-2, 1e-3, 1e-4])\n            ),\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"sparse_categorical_accuracy\"],\n        )\n\n        return model\n</code></pre>"},{"location":"api_docs/#src.capfinder.bam","title":"<code>bam</code>","text":"<p>We can only read BAM records one at a time from a BAM file. PySAM does not allow random access of BAM records. The module prepares and yields the BAM record information for each read.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.bam.generate_bam_records","title":"<code>generate_bam_records(bam_filepath: str) -&gt; Generator[pysam.AlignedSegment, None, None]</code>","text":"<p>Yield each record from a BAM file. Also creates an index (.bai) file if one does not exist already.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>str Path to the BAM file.</p> required <p>Yields:</p> Name Type Description <code>record</code> <code>AlignedSegment</code> <p>pysam.AlignedSegment A BAM record.</p> Source code in <code>src/capfinder/bam.py</code> <pre><code>def generate_bam_records(\n    bam_filepath: str,\n) -&gt; Generator[pysam.AlignedSegment, None, None]:\n    \"\"\"Yield each record from a BAM file. Also creates an index (.bai)\n    file if one does not exist already.\n\n    Params:\n        bam_filepath: str\n            Path to the BAM file.\n\n    Yields:\n        record: pysam.AlignedSegment\n            A BAM record.\n    \"\"\"\n    index_filepath = f\"{bam_filepath}.bai\"\n\n    if not os.path.exists(index_filepath):\n        pysam.index(bam_filepath)  # type: ignore\n\n    with pysam.AlignmentFile(bam_filepath, \"rb\") as bam_file:\n        yield from bam_file\n</code></pre>"},{"location":"api_docs/#src.capfinder.bam.get_signal_info","title":"<code>get_signal_info(record: pysam.AlignedSegment) -&gt; Dict[str, Any]</code>","text":"<p>Returns the signal info from a BAM record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>AlignedSegment</code> <p>pysam.AlignedSegment A BAM record.</p> required <p>Returns:</p> Name Type Description <code>signal_info</code> <code>Dict[str, Any]</code> <p>Dict[str, Any] Dictionary containing signal info for a read.</p> Source code in <code>src/capfinder/bam.py</code> <pre><code>def get_signal_info(record: pysam.AlignedSegment) -&gt; Dict[str, Any]:\n    \"\"\"Returns the signal info from a BAM record.\n\n    Params:\n        record: pysam.AlignedSegment\n            A BAM record.\n\n    Returns:\n        signal_info: Dict[str, Any]\n            Dictionary containing signal info for a read.\n    \"\"\"\n    signal_info = {}\n    tags_dict = dict(record.tags)  # type: ignore\n    moves_table = tags_dict[\"mv\"]\n    moves_step = moves_table.pop(0)\n    signal_info[\"moves_table\"] = moves_table\n    signal_info[\"moves_step\"] = moves_step\n    signal_info[\"read_id\"] = record.query_name\n    signal_info[\"start_sample\"] = tags_dict[\"ts\"]\n    signal_info[\"num_samples\"] = tags_dict[\"ns\"]\n    signal_info[\"quality_score\"] = tags_dict[\"qs\"]\n    signal_info[\"channel\"] = tags_dict[\"ch\"]\n    signal_info[\"signal_mean\"] = tags_dict[\"sm\"]\n    signal_info[\"signal_sd\"] = tags_dict[\"sd\"]\n    signal_info[\"is_qcfail\"] = record.is_qcfail\n    signal_info[\"is_reverse\"] = record.is_reverse\n    signal_info[\"is_forward\"] = record.is_forward\n    signal_info[\"is_mapped\"] = record.is_mapped\n    signal_info[\"is_supplementary\"] = record.is_supplementary\n    signal_info[\"is_secondary\"] = record.is_secondary\n    signal_info[\"read_quality\"] = record.qual  # type: ignore\n    signal_info[\"read_fasta\"] = record.query_sequence\n    signal_info[\"mapping_quality\"] = record.mapping_quality\n    signal_info[\"parent_read_id\"] = tags_dict.get(\"pi\", \"\")\n    signal_info[\"split_point\"] = tags_dict.get(\"sp\", 0)\n    signal_info[\"time_stamp\"] = tags_dict.get(\"st\")\n    signal_info[\"pod5_filename\"] = tags_dict.get(\"fn\")\n    (\n        signal_info[\"num_left_clipped_bases\"],\n        signal_info[\"num_right_clipped_bases\"],\n    ) = find_hard_clipped_bases(str(record.cigarstring))\n    return signal_info\n</code></pre>"},{"location":"api_docs/#src.capfinder.bam.get_total_records","title":"<code>get_total_records(bam_filepath: str) -&gt; int</code>","text":"<p>Returns the total number of records in a BAM file.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>str Path to the BAM file.</p> required <p>Returns:</p> Name Type Description <code>total_records</code> <code>int</code> <p>int Total number of records in the BAM file.</p> Source code in <code>src/capfinder/bam.py</code> <pre><code>def get_total_records(bam_filepath: str) -&gt; int:\n    \"\"\"Returns the total number of records in a BAM file.\n\n    Params:\n        bam_filepath: str\n            Path to the BAM file.\n\n    Returns:\n        total_records: int\n            Total number of records in the BAM file.\n    \"\"\"\n    bam_file = pysam.AlignmentFile(bam_filepath)\n    total_records = sum(1 for _ in bam_file)\n    bam_file.close()\n    return total_records\n</code></pre>"},{"location":"api_docs/#src.capfinder.bam.process_bam_records","title":"<code>process_bam_records(bam_filepath: str) -&gt; Generator[Dict[str, Any], None, None]</code>","text":"<p>Top level function to process a BAM file. Yields signal info for each read in the BAM file.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>str Path to the BAM file to process.</p> required <p>Yields:</p> Name Type Description <code>signal_info</code> <code>Dict[str, Any]</code> <p>Generator[Dict[str, Any], None, None] Dictionary containing signal info for a read.</p> Source code in <code>src/capfinder/bam.py</code> <pre><code>def process_bam_records(bam_filepath: str) -&gt; Generator[Dict[str, Any], None, None]:\n    \"\"\"Top level function to process a BAM file.\n    Yields signal info for each read in the BAM file.\n\n    Params:\n        bam_filepath: str\n            Path to the BAM file to process.\n\n    Yields:\n        signal_info: Generator[Dict[str, Any], None, None]\n            Dictionary containing signal info for a read.\n    \"\"\"\n    for record in generate_bam_records(bam_filepath):\n        yield get_signal_info(record)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli","title":"<code>cli</code>","text":""},{"location":"api_docs/#src.capfinder.cli.add_cap","title":"<code>add_cap(cap_int: int, cap_name: str) -&gt; None</code>","text":"<p>Add a new cap mapping or update an existing one.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@caps_app.command(\"add\")\ndef add_cap(cap_int: int, cap_name: str) -&gt; None:\n    \"\"\"Add a new cap mapping or update an existing one.\"\"\"\n    global CAP_MAPPING\n\n    next_available = get_next_available_cap_number()\n\n    # Check if the cap name is unique\n    existing_cap_int = is_cap_name_unique(cap_name)\n    if existing_cap_int is not None and existing_cap_int != cap_int:\n        typer.echo(\n            f\"Error: The cap name '{cap_name}' is already used for cap number {existing_cap_int}.\"\n        )\n        typer.echo(\"Please use a unique name for each cap.\")\n        return\n\n    if cap_int in CAP_MAPPING:\n        update_cap_mapping({cap_int: cap_name})\n        typer.echo(f\"Updated existing mapping: {cap_int} -&gt; {cap_name}\")\n    elif cap_int == next_available:\n        update_cap_mapping({cap_int: cap_name})\n        typer.echo(f\"Added new mapping: {cap_int} -&gt; {cap_name}\")\n    else:\n        typer.echo(f\"Error: The next available cap number is {next_available}.\")\n        typer.echo(\n            f\"Please use {next_available} as the cap number to maintain continuity.\"\n        )\n        return\n    typer.echo(f\"Custom mappings saved to: {CUSTOM_MAPPING_PATH}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.cap_help","title":"<code>cap_help() -&gt; None</code>","text":"<p>Display help information about cap mapping management.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@caps_app.command(\"help\")\ndef cap_help() -&gt; None:\n    \"\"\"Display help information about cap mapping management.\"\"\"\n    typer.echo(\"Cap Mapping Management Help\")\n    typer.echo(\"----------------------------\")\n    typer.echo(\n        \"Capfinder allows you to customize cap mappings. These mappings persist across runs.\"\n    )\n    typer.echo(f\"\\nYour custom mappings are stored in: {CUSTOM_MAPPING_PATH}\")\n    typer.echo(\"\\nAvailable commands:\")\n    typer.echo(\"  capfinder capmap add &lt;int&gt; &lt;name&gt;  : Add or update a cap mapping\")\n    typer.echo(\"  capfinder capmap remove &lt;int&gt;      : Remove a cap mapping\")\n    typer.echo(\"  capfinder capmap list              : List all current cap mappings\")\n    typer.echo(\"  capfinder capmap reset             : Reset cap mappings to default\")\n    typer.echo(\n        \"  capfinder capmap config            : Show the location of the configuration file\"\n    )\n    typer.echo(\"\\nExamples:\")\n    typer.echo(\"  capfinder capmap add 7 new_cap_type\")\n    typer.echo(\"  capfinder capmap remove 7\")\n    typer.echo(\"  capfinder capmap list\")\n    typer.echo(\n        \"\\nNote: Changes to cap mappings are immediately saved and will persist across runs.\"\n    )\n    typer.echo(\n        \"When adding a new cap, you must use the next available number in the sequence.\"\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.create_train_config","title":"<code>create_train_config(file_path: Annotated[str, typer.Option(--file_path, -f, help='File path to save the JSON configuration file')] = '') -&gt; None</code>","text":"<p>Creates a dummy JSON configuration file at the specified path. Edit it to suit your needs.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@app.command()\ndef create_train_config(\n    file_path: Annotated[\n        str,\n        typer.Option(\n            \"--file_path\", \"-f\", help=\"File path to save the JSON configuration file\"\n        ),\n    ] = \"\",\n) -&gt; None:\n    \"\"\"Creates a dummy JSON configuration file at the specified path. Edit it to suit your needs.\"\"\"\n    config = {\n        \"etl_params\": {\n            \"use_remote_dataset_version\": \"latest\",  # Version of the remote dataset to use, e.g., \"latest\", \"1.0.0\", etc. If set to \"\", then a local dataset will be used/made and/or uploaded to the remote dataset\n            \"caps_data_dir\": \"/dir/\",  # Directory containing cap signal data files for all cap classes in the model\n            \"examples_per_class\": 100000,  # Maximum number of examples to use per class\n            \"comet_project_name\": \"dataset\",  # Name of the Comet ML project for dataset logging\n        },\n        \"tune_params\": {\n            \"comet_project_name\": \"capfinder_tune\",  # Name of the Comet ML project for hyperparameter tuning\n            \"patience\": 0,  # Number of epochs with no improvement after which training will be stopped\n            \"max_epochs_hpt\": 3,  # Maximum number of epochs for each trial during hyperparameter tuning\n            \"max_trials\": 5,  # Maximum number of trials for hyperparameter search\n            \"factor\": 2,  # Reduction factor for Hyperband algorithm\n            \"seed\": 42,  # Random seed for reproducibility\n            \"tuning_strategy\": \"hyperband\",  # Options: \"hyperband\", \"random_search\", \"bayesian_optimization\"\n            \"overwrite\": False,  # Whether to overwrite previous tuning results. All hyperparameter tuning results will be lost if set to True\n        },  # Added comma here\n        \"train_params\": {\n            \"comet_project_name\": \"capfinder_train\",  # Name of the Comet ML project for model training\n            \"patience\": 120,  # Number of epochs with no improvement after which training will be stopped\n            \"max_epochs_final_model\": 300,  # Maximum number of epochs for training the final model\n        },\n        \"shared_params\": {\n            \"num_classes\": 4,  # Number of classes in the dataset\n            \"model_type\": \"cnn_lstm\",  # Options: \"attention_cnn_lstm\", \"cnn_lstm\", \"encoder\", \"resnet\"\n            \"batch_size\": 32,  # Batch size for training\n            \"target_length\": 500,  # Target length for input sequences\n            \"dtype\": \"float16\",  # Data type for model parameters. Options: \"float16\", \"float32\", \"float64\"\n            \"train_test_fraction\": 0.95,  # Fraction of total data to use for training (vs. testing)\n            \"train_val_fraction\": 0.8,  # Fraction of training data to use for training (vs. validation)\n            \"use_augmentation\": False,  # Whether to include time warped versions of original training examples in the dataset\n            \"output_dir\": \"/dir/\",  # Directory to save output files\n        },\n        \"lr_scheduler_params\": {\n            \"type\": \"reduce_lr_on_plateau\",  # Options: \"reduce_lr_on_plateau\", \"cyclic_lr\", \"sgdr\"\n            \"reduce_lr_on_plateau\": {\n                \"factor\": 0.5,  # Factor by which the learning rate will be reduced\n                \"patience\": 5,  # Number of epochs with no improvement after which learning rate will be reduced\n                \"min_lr\": 1e-6,  # Lower bound on the learning rate\n            },\n            \"cyclic_lr\": {\n                \"base_lr\": 1e-3,  # Initial learning rate which is the lower boundary in the cycle\n                \"max_lr\": 5e-2,  # Upper boundary in the cycle for learning rate\n                \"step_size_factor\": 8,  # Number of training iterations in the increasing half of a cycle\n                \"mode\": \"triangular2\",  # One of {triangular, triangular2, exp_range}\n            },\n            \"sgdr\": {\n                \"min_lr\": 1e-3,  # Minimum learning rate\n                \"max_lr\": 2e-3,  # Maximum learning rate\n                \"lr_decay\": 0.9,  # Decay factor for learning rate\n                \"cycle_length\": 5,  # Number of epochs in a cycle\n                \"mult_factor\": 1.5,  # Multiplication factor for cycle length after each restart\n            },\n        },\n        \"debug_code\": False,  # Whether to run in debug mode\n    }\n    import json\n\n    from capfinder.logger_config import configure_logger, configure_prefect_logging\n    from capfinder.utils import log_header, log_output\n\n    log_filepath = configure_logger(\n        os.path.join(os.path.dirname(file_path), \"logs\"), show_location=False\n    )\n    configure_prefect_logging(show_location=False)\n    version_info = version(\"capfinder\")\n    log_header(f\"Using Capfinder v{version_info}\")\n\n    with open(file_path, \"w\") as file:\n        json.dump(config, file, indent=4)\n\n    grey = \"\\033[90m\"\n    reset = \"\\033[0m\"\n    log_output(\n        f\"The training config JSON file has been saved to:\\n {grey}{file_path}{reset}\\nThe log file has been saved to:\\n {grey}{log_filepath}{reset}\"\n    )\n    log_header(\"Processing finished!\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.extract_cap_signal","title":"<code>extract_cap_signal(bam_filepath: Annotated[str, typer.Option(--bam_filepath, -b, help='Path to the BAM file')] = '', pod5_dir: Annotated[str, typer.Option(--pod5_dir, -p, help='Path to directory containing POD5 files')] = '', reference: Annotated[str, typer.Option(--reference, -r, help=\"Reference Sequence (5' -&gt; 3')\")] = 'GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT', cap_class: Annotated[int, typer.Option(--cap_class, -c, help='\\n\\n    Integer-based class label for the RNA cap type. \\n\\n    - -99 represents an unknown cap(s). \\n\\n    - 0 represents Cap_0 \\n\\n    - 1 represents Cap 1 \\n\\n    - 2 represents Cap 2 \\n\\n    - 3 represents Cap2-1 \\n\\n    You can use the capmap command to manage cap mappings and use additional interger label for additional caps. \\n\\n    ')] = -99, cap_n1_pos0: Annotated[int, typer.Option(--cap_n1_pos0, -p, help='0-based index of 1st nucleotide (N1) of cap in the reference')] = 52, train_or_test: Annotated[str, typer.Option(--train_or_test, -t, help='set to train or test depending on whether it is training or testing data')] = 'test', output_dir: Annotated[str, typer.Option(--output_dir, -o, help=textwrap.dedent('\\n        Path to the output directory which will contain: \\n\\n            \u251c\u2500\u2500 A CSV file (data__cap_x.csv) containing the extracted ROI signal data.\\n\\n            \u251c\u2500\u2500 A CSV file (metadata__cap_x.csv) containing the complete metadata information.\\n\\n            \u251c\u2500\u2500 A log file (capfinder_vXYZ_datatime.log) containing the logs of the program.\\n\\n            \u2514\u2500\u2500 (Optional) plots directory containing cap signal plots, if --plot-signal is used.\\n\\n \\u200b    \u251c\u2500\u2500 good_reads: Directory that contains the plots for the good reads.\\n\\n \\u200b    \u251c\u2500\u2500 bad_reads: Directory that contains the plots for the bad reads.\\n\\n \\u200b    \u2514\u2500\u2500 plotpaths.csv: CSV file containing the paths to the plots based on the read ID.\\n'))] = '', n_workers: Annotated[int, typer.Option(--n_workers, -n, help='Number of CPUs to use for parallel processing')] = 1, plot_signal: Annotated[Optional[bool], typer.Option(--plot - signal / --no - plot - signal, help='Whether to plot extracted cap signal or not')] = None, debug_code: Annotated[bool, typer.Option(--debug / --no - debug, help='Enable debug mode for more detailed logging')] = False) -&gt; None</code>","text":"<p>Extracts signal corresponding to the RNA cap type using BAM and POD5 files. Also, generates plots if required.</p> <p>Example command (for training data): capfinder extract-cap-signal \\     --bam_filepath /path/to/sorted.bam \\     --pod5_dir /path/to/pod5_dir \\     --reference GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGTNNNNNNCGATGTAACTGGGACATGGTGAGCAATCAGGGAAAAAAAAAAAAAAA \\     --cap_class 0 \\     --cap_n1_pos0 52 \\     --train_or_test train \\     --output_dir /path/to/output_dir \\     --n_workers 10 \\     --no-plot-signal \\     --no-debug</p> <p>Example command (for testing data): capfinder extract-cap-signal \\     --bam_filepath /path/to/sorted.bam \\     --pod5_dir /path/to/pod5_dir \\     --reference GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT \\     --cap_class -99 \\     --cap_n1_pos0 52 \\     --train_or_test test \\     --output_dir /path/to/output_dir \\     --n_workers 10 \\     --no-plot-signal \\     --no-debug</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@app.command()\ndef extract_cap_signal(\n    bam_filepath: Annotated[\n        str, typer.Option(\"--bam_filepath\", \"-b\", help=\"Path to the BAM file\")\n    ] = \"\",\n    pod5_dir: Annotated[\n        str,\n        typer.Option(\n            \"--pod5_dir\", \"-p\", help=\"Path to directory containing POD5 files\"\n        ),\n    ] = \"\",\n    reference: Annotated[\n        str,\n        typer.Option(\"--reference\", \"-r\", help=\"Reference Sequence (5' -&gt; 3')\"),\n    ] = \"GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT\",\n    cap_class: Annotated[\n        int,\n        typer.Option(\n            \"--cap_class\",\n            \"-c\",\n            help=\"\"\"\\n\n    Integer-based class label for the RNA cap type. \\n\n    - -99 represents an unknown cap(s). \\n\n    - 0 represents Cap_0 \\n\n    - 1 represents Cap 1 \\n\n    - 2 represents Cap 2 \\n\n    - 3 represents Cap2-1 \\n\n    You can use the capmap command to manage cap mappings and use additional interger label for additional caps. \\n\n    \"\"\",\n        ),\n    ] = -99,\n    cap_n1_pos0: Annotated[\n        int,\n        typer.Option(\n            \"--cap_n1_pos0\",\n            \"-p\",\n            help=\"0-based index of 1st nucleotide (N1) of cap in the reference\",\n        ),\n    ] = 52,\n    train_or_test: Annotated[\n        str,\n        typer.Option(\n            \"--train_or_test\",\n            \"-t\",\n            help=\"set to train or test depending on whether it is training or testing data\",\n        ),\n    ] = \"test\",\n    output_dir: Annotated[\n        str,\n        typer.Option(\n            \"--output_dir\",\n            \"-o\",\n            help=textwrap.dedent(\n                \"\"\"\n        Path to the output directory which will contain: \\n\n            \u251c\u2500\u2500 A CSV file (data__cap_x.csv) containing the extracted ROI signal data.\\n\n            \u251c\u2500\u2500 A CSV file (metadata__cap_x.csv) containing the complete metadata information.\\n\n            \u251c\u2500\u2500 A log file (capfinder_vXYZ_datatime.log) containing the logs of the program.\\n\n            \u2514\u2500\u2500 (Optional) plots directory containing cap signal plots, if --plot-signal is used.\\n\n            \\u200B    \u251c\u2500\u2500 good_reads: Directory that contains the plots for the good reads.\\n\n            \\u200B    \u251c\u2500\u2500 bad_reads: Directory that contains the plots for the bad reads.\\n\n            \\u200B    \u2514\u2500\u2500 plotpaths.csv: CSV file containing the paths to the plots based on the read ID.\\n\"\"\"\n            ),\n        ),\n    ] = \"\",\n    n_workers: Annotated[\n        int,\n        typer.Option(\n            \"--n_workers\", \"-n\", help=\"Number of CPUs to use for parallel processing\"\n        ),\n    ] = 1,\n    plot_signal: Annotated[\n        Optional[bool],\n        typer.Option(\n            \"--plot-signal/--no-plot-signal\",\n            help=\"Whether to plot extracted cap signal or not\",\n        ),\n    ] = None,\n    debug_code: Annotated[\n        bool,\n        typer.Option(\n            \"--debug/--no-debug\",\n            help=\"Enable debug mode for more detailed logging\",\n        ),\n    ] = False,\n) -&gt; None:\n    \"\"\"\n    Extracts signal corresponding to the RNA cap type using BAM and POD5 files. Also, generates plots if required.\n\n    Example command (for training data):\n    capfinder extract-cap-signal \\\\\n        --bam_filepath /path/to/sorted.bam \\\\\n        --pod5_dir /path/to/pod5_dir \\\\\n        --reference GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGTNNNNNNCGATGTAACTGGGACATGGTGAGCAATCAGGGAAAAAAAAAAAAAAA \\\\\n        --cap_class 0 \\\\\n        --cap_n1_pos0 52 \\\\\n        --train_or_test train \\\\\n        --output_dir /path/to/output_dir \\\\\n        --n_workers 10 \\\\\n        --no-plot-signal \\\\\n        --no-debug\n\n    Example command (for testing data):\n    capfinder extract-cap-signal \\\\\n        --bam_filepath /path/to/sorted.bam \\\\\n        --pod5_dir /path/to/pod5_dir \\\\\n        --reference GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT \\\\\n        --cap_class -99 \\\\\n        --cap_n1_pos0 52 \\\\\n        --train_or_test test \\\\\n        --output_dir /path/to/output_dir \\\\\n        --n_workers 10 \\\\\n        --no-plot-signal \\\\\n        --no-debug\n    \"\"\"\n    from capfinder.collate import collate_bam_pod5_wrapper\n\n    ps = False\n    if plot_signal is None:\n        ps = False\n    elif plot_signal:\n        ps = True\n    else:\n        ps = False\n\n    global formatted_command_global\n\n    collate_bam_pod5_wrapper(\n        bam_filepath=bam_filepath,\n        pod5_dir=pod5_dir,\n        num_processes=n_workers,\n        reference=reference,\n        cap_class=cap_class,\n        cap0_pos=cap_n1_pos0,\n        train_or_test=train_or_test,\n        plot_signal=ps,\n        output_dir=output_dir,\n        debug_code=debug_code,\n        formatted_command=formatted_command_global,\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.list_caps","title":"<code>list_caps() -&gt; None</code>","text":"<p>List all current cap mappings.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@caps_app.command(\"list\")\ndef list_caps() -&gt; None:\n    \"\"\"List all current cap mappings.\"\"\"\n    load_custom_mapping()  # Reload the mappings from the file\n    global CAP_MAPPING\n\n    if not CAP_MAPPING:\n        typer.echo(\"No cap mappings found. Using default mappings:\")\n        for cap_int, cap_name in sorted(DEFAULT_CAP_MAPPING.items()):\n            typer.echo(f\"{cap_int}: {cap_name}\")\n    else:\n        typer.echo(\"Current cap mappings:\")\n        for cap_int, cap_name in sorted(CAP_MAPPING.items()):\n            typer.echo(f\"{cap_int}: {cap_name}\")\n\n    next_available = get_next_available_cap_number()\n    typer.echo(f\"\\nNext available cap number: {next_available}\")\n    typer.echo(f\"\\nCustom mappings file location: {CUSTOM_MAPPING_PATH}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.make_train_dataset","title":"<code>make_train_dataset(caps_data_dir: Annotated[str, typer.Option(--caps_data_dir, -c, help='Directory containing all the cap signal data files (data__cap_x.csv)')] = '', output_dir: Annotated[str, typer.Option(--output_dir, -o, help='A dataset directory will be created inside this directory automatically and the dataset will be saved there as CSV files.')] = '', target_length: Annotated[int, typer.Option(--target_length, -t, help='Number of signal points in cap signal to consider. If the signal is shorter, it will be padded with zeros. If the signal is longer, it will be truncated.')] = 500, dtype: Annotated[str, typer.Option(--dtype, -d, help=\"Data type to transform the dataset to. Valid values are 'float16', 'float32', or 'float64'.\")] = 'float16', examples_per_class: Annotated[int, typer.Option(--examples_per_class, -e, help='Number of examples to include per class in the dataset')] = 1000, train_test_fraction: Annotated[float, typer.Option(--train_test_fraction, -tt, help='Fraction of data out of all data to use for training (0.0 to 1.0)')] = 0.95, train_val_fraction: Annotated[float, typer.Option(--train_val_fraction, -tv, help='Fraction of data out all the training split to use for validation (0.0 to 1.0)')] = 0.8, num_classes: Annotated[int, typer.Option(--num_classes, help='Number of classes in the dataset')] = 4, batch_size: Annotated[int, typer.Option(--batch_size, -b, help='Batch size for processing data')] = 1024, comet_project_name: Annotated[str, typer.Option(--comet_project_name, help='Name of the Comet ML project for logging')] = 'dataset', use_remote_dataset_version: Annotated[str, typer.Option(--use_remote_dataset_version, help='Version of the remote dataset to use. If not provided at all, the local dataset will be used/made and/or uploaded')] = '', use_augmentation: Annotated[bool, typer.Option(--use - augmentation / --no - use - augmentation, help='Whether to augment original data with time warped data')] = False) -&gt; None</code>","text":"<p>Prepares dataset for training the ML model. This command can be run independently from here or is automatically invoked by the <code>train-model</code> command.</p> <p>This command processes cap signal data files, applies necessary transformations, and prepares a dataset suitable for training machine learning models. It supports both local data processing and fetching from a remote dataset.</p> <p>Example command: capfinder make-train-dataset \\     --caps_data_dir /path/to/caps_data \\     --output_dir /path/to/output \\     --target_length 500 \\     --dtype float16 \\     --examples_per_class 1000 \\     --train_test_fraction 0.95 \\     --train_val_fraction 0.8 \\     --num_classes 4 \\     --batch_size 32 \\     --comet_project_name my-capfinder-project \\     --use_remote_dataset_version latest     --use-augmentation</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@app.command()\ndef make_train_dataset(\n    caps_data_dir: Annotated[\n        str,\n        typer.Option(\n            \"--caps_data_dir\",\n            \"-c\",\n            help=\"Directory containing all the cap signal data files (data__cap_x.csv)\",\n        ),\n    ] = \"\",\n    output_dir: Annotated[\n        str,\n        typer.Option(\n            \"--output_dir\",\n            \"-o\",\n            help=\"A dataset directory will be created inside this directory automatically and the dataset will be saved there as CSV files.\",\n        ),\n    ] = \"\",\n    target_length: Annotated[\n        int,\n        typer.Option(\n            \"--target_length\",\n            \"-t\",\n            help=\"Number of signal points in cap signal to consider. If the signal is shorter, it will be padded with zeros. If the signal is longer, it will be truncated.\",\n        ),\n    ] = 500,\n    dtype: Annotated[\n        str,\n        typer.Option(\n            \"--dtype\",\n            \"-d\",\n            help=\"Data type to transform the dataset to. Valid values are 'float16', 'float32', or 'float64'.\",\n        ),\n    ] = \"float16\",\n    examples_per_class: Annotated[\n        int,\n        typer.Option(\n            \"--examples_per_class\",\n            \"-e\",\n            help=\"Number of examples to include per class in the dataset\",\n        ),\n    ] = 1000,\n    train_test_fraction: Annotated[\n        float,\n        typer.Option(\n            \"--train_test_fraction\",\n            \"-tt\",\n            help=\"Fraction of data out of all data to use for training (0.0 to 1.0)\",\n        ),\n    ] = 0.95,\n    train_val_fraction: Annotated[\n        float,\n        typer.Option(\n            \"--train_val_fraction\",\n            \"-tv\",\n            help=\"Fraction of data out all the training split to use for validation (0.0 to 1.0)\",\n        ),\n    ] = 0.8,\n    num_classes: Annotated[\n        int,\n        typer.Option(\n            \"--num_classes\",\n            help=\"Number of classes in the dataset\",\n        ),\n    ] = 4,\n    batch_size: Annotated[\n        int,\n        typer.Option(\n            \"--batch_size\",\n            \"-b\",\n            help=\"Batch size for processing data\",\n        ),\n    ] = 1024,\n    comet_project_name: Annotated[\n        str,\n        typer.Option(\n            \"--comet_project_name\",\n            help=\"Name of the Comet ML project for logging\",\n        ),\n    ] = \"dataset\",\n    use_remote_dataset_version: Annotated[\n        str,\n        typer.Option(\n            \"--use_remote_dataset_version\",\n            help=\"Version of the remote dataset to use. If not provided at all, the local dataset will be used/made and/or uploaded\",\n        ),\n    ] = \"\",\n    use_augmentation: Annotated[\n        bool,\n        typer.Option(\n            \"--use-augmentation/--no-use-augmentation\",\n            help=\"Whether to augment original data with time warped data\",\n        ),\n    ] = False,\n) -&gt; None:\n    \"\"\"\n    Prepares dataset for training the ML model. This command can be run independently\n    from here or is automatically invoked by the `train-model` command.\n\n    This command processes cap signal data files, applies necessary transformations,\n    and prepares a dataset suitable for training machine learning models. It supports\n    both local data processing and fetching from a remote dataset.\n\n    Example command:\n    capfinder make-train-dataset \\\\\n        --caps_data_dir /path/to/caps_data \\\\\n        --output_dir /path/to/output \\\\\n        --target_length 500 \\\\\n        --dtype float16 \\\\\n        --examples_per_class 1000 \\\\\n        --train_test_fraction 0.95 \\\\\n        --train_val_fraction 0.8 \\\\\n        --num_classes 4 \\\\\n        --batch_size 32 \\\\\n        --comet_project_name my-capfinder-project \\\\\n        --use_remote_dataset_version latest\n        --use-augmentation\n\n    \"\"\"\n    from typing import cast\n\n    from capfinder.logger_config import configure_logger, configure_prefect_logging\n    from capfinder.train_etl import DtypeLiteral, train_etl\n    from capfinder.utils import log_header, log_output\n\n    global formatted_command_global\n\n    dataset_dir = os.path.join(output_dir, \"dataset\")\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n    log_filepath = configure_logger(\n        os.path.join(dataset_dir, \"logs\"), show_location=False\n    )\n    configure_prefect_logging(show_location=False)\n    version_info = version(\"capfinder\")\n    log_header(f\"Using Capfinder v{version_info}\")\n    logger.info(formatted_command_global)\n\n    dt: DtypeLiteral = \"float32\"\n    if dtype in {\"float16\", \"float32\", \"float64\"}:\n        dt = cast(DtypeLiteral, dtype)\n    else:\n        logger.warning(\n            f\"Invalid dtype literal: {dtype}. Allowed values are 'float16', 'float32', 'float64'. Using 'float32' as default.\"\n        )\n\n    train_etl(\n        caps_data_dir=caps_data_dir,\n        dataset_dir=dataset_dir,\n        target_length=target_length,\n        dtype=dt,\n        examples_per_class=examples_per_class,\n        train_test_fraction=train_test_fraction,\n        train_val_fraction=train_val_fraction,\n        num_classes=num_classes,\n        batch_size=batch_size,\n        comet_project_name=comet_project_name,\n        use_remote_dataset_version=use_remote_dataset_version,\n        use_augmentation=use_augmentation,\n    )\n\n    grey = \"\\033[90m\"\n    reset = \"\\033[0m\"\n    log_output(f\"The log file has been saved to:\\n {grey}{log_filepath}{reset}\")\n    log_header(\"Processing finished!\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.predict_cap_types","title":"<code>predict_cap_types(bam_filepath: Annotated[str, typer.Option(--bam_filepath, -b, help='Path to the BAM file')] = '', pod5_dir: Annotated[str, typer.Option(--pod5_dir, -p, help='Path to directory containing POD5 files')] = '', output_dir: Annotated[str, typer.Option(--output_dir, -o, help='Path to the output directory for prediction results and logs')] = '', n_cpus: Annotated[int, typer.Option(--n_cpus, -n, help=textwrap.dedent(\"            Number of CPUs to use for parallel processing.\\n            We use multiple CPUs during processing for POD5 file and BAM data (Step 1/5).\\n            For faster processing of this data (POD5 &amp; BAM), increase the number of CPUs.\\n            For inference (Step 4/5), only a single CPU is used no matter how many CPUs you have specified.\\n            For faster inference, have a GPU available (it will be detected automatically) and set dtype to 'float16'.\"))] = 1, dtype: Annotated[str, typer.Option(--dtype, -d, help=textwrap.dedent(\"            Data type for model input. Valid values are 'float16', 'float32', or 'float64'.\\n            If you do not have a GPU, use 'float32' or 'float64' for better performance.\\n            If you have a GPU, use 'float16' for faster inference.\"))] = 'float16', batch_size: Annotated[int, typer.Option(--batch_size, -bs, help=textwrap.dedent('            Batch size for model inference.\\n            Larger batch sizes can speed up inference but require more memory.'))] = 128, custom_model_path: Annotated[Optional[str], typer.Option(--custom_model_path, -m, help='Path to a custom model (.keras) file. If not provided, the default pre-packaged model will be used.')] = None, plot_signal: Annotated[bool, typer.Option(--plot - signal / --no - plot - signal, help=textwrap.dedent('                \"Whether to plot extracted cap signal or not.\\n                Saving plots can help you plot the read\\'s signal, and plot the signal for cap and flanking bases(\u00b15).'))] = False, debug_code: Annotated[bool, typer.Option(--debug / --no - debug, help='Enable debug mode for more detailed logging')] = False, refresh_cache: Annotated[bool, typer.Option(--refresh - cache / --no - refresh - cache, help='Refresh the cache for intermediate results')] = False) -&gt; None</code>","text":"<p>Predicts RNA cap types using BAM and POD5 files.</p> Example command <p>capfinder predict-cap-types \\ --bam_filepath /path/to/sorted.bam \\ --pod5_dir /path/to/pod5_dir \\ --output_dir /path/to/output_dir \\ --n_cpus 10 \\ --dtype float16 \\ --batch_size 256 \\ --no-plot-signal \\ --no-debug \\ --no-refresh-cache</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@app.command()\ndef predict_cap_types(\n    bam_filepath: Annotated[\n        str, typer.Option(\"--bam_filepath\", \"-b\", help=\"Path to the BAM file\")\n    ] = \"\",\n    pod5_dir: Annotated[\n        str,\n        typer.Option(\n            \"--pod5_dir\", \"-p\", help=\"Path to directory containing POD5 files\"\n        ),\n    ] = \"\",\n    output_dir: Annotated[\n        str,\n        typer.Option(\n            \"--output_dir\",\n            \"-o\",\n            help=\"Path to the output directory for prediction results and logs\",\n        ),\n    ] = \"\",\n    # reference: Annotated[\n    #     str,\n    #     typer.Option(\"--reference\", \"-r\", help=\"Reference Sequence (5' -&gt; 3')\"),\n    # ] = \"GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT\",\n    # cap_n1_pos0: Annotated[\n    #     int,\n    #     typer.Option(\n    #         \"--cap_n1_pos0\",\n    #         \"-p\",\n    #         help=\"0-based index of 1st nucleotide (N1) of cap in the reference\",\n    #     ),\n    # ] = 52,\n    n_cpus: Annotated[\n        int,\n        typer.Option(\n            \"--n_cpus\",\n            \"-n\",\n            help=textwrap.dedent(\n                \"\"\"\\\n            Number of CPUs to use for parallel processing.\n            We use multiple CPUs during processing for POD5 file and BAM data (Step 1/5).\n            For faster processing of this data (POD5 &amp; BAM), increase the number of CPUs.\n            For inference (Step 4/5), only a single CPU is used no matter how many CPUs you have specified.\n            For faster inference, have a GPU available (it will be detected automatically) and set dtype to 'float16'.\"\"\"\n            ),\n        ),\n    ] = 1,\n    dtype: Annotated[\n        str,\n        typer.Option(\n            \"--dtype\",\n            \"-d\",\n            help=textwrap.dedent(\n                \"\"\"\\\n            Data type for model input. Valid values are 'float16', 'float32', or 'float64'.\n            If you do not have a GPU, use 'float32' or 'float64' for better performance.\n            If you have a GPU, use 'float16' for faster inference.\"\"\"\n            ),\n        ),\n    ] = \"float16\",\n    # target_length: Annotated[\n    #     int,\n    #     typer.Option(\n    #         \"--target_length\",\n    #         \"-t\",\n    #         help=\"Number of signal points in cap signal to consider\",\n    #     ),\n    # ] = 500,\n    batch_size: Annotated[\n        int,\n        typer.Option(\n            \"--batch_size\",\n            \"-bs\",\n            help=textwrap.dedent(\n                \"\"\"\\\n            Batch size for model inference.\n            Larger batch sizes can speed up inference but require more memory.\"\"\"\n            ),\n        ),\n    ] = 128,\n    custom_model_path: Annotated[\n        Optional[str],\n        typer.Option(\n            \"--custom_model_path\",\n            \"-m\",\n            help=\"Path to a custom model (.keras) file. If not provided, the default pre-packaged model will be used.\",\n        ),\n    ] = None,\n    plot_signal: Annotated[\n        bool,\n        typer.Option(\n            \"--plot-signal/--no-plot-signal\",\n            help=textwrap.dedent(\n                \"\"\"\\\n                \"Whether to plot extracted cap signal or not.\n                Saving plots can help you plot the read's signal, and plot the signal for cap and flanking bases(&amp;#177;5).\"\"\"\n            ),\n        ),\n    ] = False,\n    debug_code: Annotated[\n        bool,\n        typer.Option(\n            \"--debug/--no-debug\",\n            help=\"Enable debug mode for more detailed logging\",\n        ),\n    ] = False,\n    refresh_cache: Annotated[\n        bool,\n        typer.Option(\n            \"--refresh-cache/--no-refresh-cache\",\n            help=\"Refresh the cache for intermediate results\",\n        ),\n    ] = False,\n) -&gt; None:\n    \"\"\"\n    Predicts RNA cap types using BAM and POD5 files.\n\n    Example command:\n        capfinder predict-cap-types \\\\\n        --bam_filepath /path/to/sorted.bam \\\\\n        --pod5_dir /path/to/pod5_dir \\\\\n        --output_dir /path/to/output_dir \\\\\n        --n_cpus 10 \\\\\n        --dtype float16 \\\\\n        --batch_size 256 \\\\\n        --no-plot-signal \\\\\n        --no-debug \\\\\n        --no-refresh-cache\n    \"\"\"\n    from typing import cast\n\n    from capfinder.inference import predict_cap_types\n    from capfinder.train_etl import DtypeLiteral\n\n    dt: DtypeLiteral = \"float16\"\n    if dtype in {\"float16\", \"float32\", \"float64\"}:\n        dt = cast(\n            DtypeLiteral, dtype\n        )  # This is safe because input_str must be one of the Literal values\n    else:\n        logger.warning(\n            f\"Invalid dtype literal: {dtype}. Allowed values are 'float16', 'float32', 'float64'. Using 'float16' as default.\"\n        )\n\n    global formatted_command_global\n\n    predict_cap_types(\n        bam_filepath=bam_filepath,\n        pod5_dir=pod5_dir,\n        num_cpus=n_cpus,\n        output_dir=output_dir,\n        dtype=dt,\n        reference=\"GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT\",\n        cap0_pos=52,\n        train_or_test=\"test\",\n        plot_signal=plot_signal,\n        cap_class=-99,\n        target_length=500,\n        batch_size=batch_size,\n        custom_model_path=custom_model_path,\n        debug_code=debug_code,\n        refresh_cache=refresh_cache,\n        formatted_command=formatted_command_global,\n    )\n    logger.success(\"Finished predicting cap types!\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.remove_cap","title":"<code>remove_cap(cap_int: int) -&gt; None</code>","text":"<p>Remove a cap mapping.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@caps_app.command(\"remove\")\ndef remove_cap(cap_int: int) -&gt; None:\n    \"\"\"Remove a cap mapping.\"\"\"\n    global CAP_MAPPING\n\n    if cap_int in CAP_MAPPING:\n        del CAP_MAPPING[cap_int]\n        save_custom_mapping(CAP_MAPPING)\n        typer.echo(f\"Removed mapping for cap integer: {cap_int}\")\n        typer.echo(f\"Custom mappings saved to: {CUSTOM_MAPPING_PATH}\")\n    else:\n        typer.echo(f\"No mapping found for cap integer: {cap_int}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.reset_caps","title":"<code>reset_caps() -&gt; None</code>","text":"<p>Reset cap mappings to default.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@caps_app.command(\"reset\")\ndef reset_caps() -&gt; None:\n    \"\"\"Reset cap mappings to default.\"\"\"\n    global CAP_MAPPING\n    CAP_MAPPING = DEFAULT_CAP_MAPPING.copy()\n    save_custom_mapping(CAP_MAPPING)\n    typer.echo(\"Cap mappings reset to default.\")\n    typer.echo(f\"Default mappings saved to: {CUSTOM_MAPPING_PATH}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.show_config","title":"<code>show_config() -&gt; None</code>","text":"<p>Show the location of the configuration file.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@caps_app.command(\"config\")\ndef show_config() -&gt; None:\n    \"\"\"Show the location of the configuration file.\"\"\"\n    typer.echo(f\"Custom mappings file location: {CUSTOM_MAPPING_PATH}\")\n    if CUSTOM_MAPPING_PATH.exists():\n        typer.echo(\"The file exists and contains custom mappings.\")\n    else:\n        typer.echo(\n            \"The file does not exist yet. It will be created when you add a custom mapping.\"\n        )\n        logger.warning(f\"Config file does not exist at {CUSTOM_MAPPING_PATH}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cli.train_model","title":"<code>train_model(config_file: Annotated[str, typer.Option(--config_file, -c, help='Path to the JSON configuration file containing the parameters for the training pipeline.')] = '') -&gt; None</code>","text":"<p>Trains the model using the parameters in the JSON configuration file.</p> Source code in <code>src/capfinder/cli.py</code> <pre><code>@app.command()\ndef train_model(\n    config_file: Annotated[\n        str,\n        typer.Option(\n            \"--config_file\",\n            \"-c\",\n            help=\"Path to the JSON configuration file containing the parameters for the training pipeline.\",\n        ),\n    ] = \"\",\n) -&gt; None:\n    \"\"\"Trains the model using the parameters in the JSON configuration file.\"\"\"\n    import json\n\n    from capfinder.training import run_training_pipeline\n\n    # Load the configuration file\n    with open(config_file) as file:\n        config = json.load(file)\n\n    etl_params = config[\"etl_params\"]\n    tune_params = config[\"tune_params\"]\n    train_params = config[\"train_params\"]\n    shared_params = config[\"shared_params\"]\n    lr_scheduler_params = config[\"lr_scheduler_params\"]\n    debug_code = config.get(\"debug_code\", False)\n\n    # Create a formatted command string with all parameters\n    formatted_command = f\"capfinder train-model --config_file {config_file}\\n\\n\"\n    formatted_command += \"Configuration:\\n\"\n    formatted_command += json.dumps(config, indent=2)\n\n    # Run the training pipeline with the loaded parameters\n    run_training_pipeline(\n        etl_params=etl_params,\n        tune_params=tune_params,\n        train_params=train_params,\n        shared_params=shared_params,\n        lr_scheduler_params=lr_scheduler_params,\n        debug_code=debug_code,\n        formatted_command=formatted_command,\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate","title":"<code>collate</code>","text":"<p>The main workhorse which collates information from the BAM file and the POD5 files, aligns OTE to extracts the signal for the region of interest (ROI) for training or testing purposes. It also plots the ROI signal if requested.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.collate.DatabaseHandler","title":"<code>DatabaseHandler</code>","text":"Source code in <code>src/capfinder/collate.py</code> <pre><code>class DatabaseHandler:\n    def __init__(\n        self,\n        cap_class: int,\n        num_processes: int,\n        database_path: str,\n        plots_csv_filepath: Union[str, None],\n        output_dir: str,\n    ) -&gt; None:\n        \"\"\"Initializes the index database handler\"\"\"\n        self.cap_class = cap_class\n        self.database_path = database_path\n        self.plots_csv_filepath = plots_csv_filepath\n        self.num_processes = num_processes\n        self.output_dir = output_dir\n\n        # Open the plots CSV file in append mode\n        if self.plots_csv_filepath:\n            self.csvfile = open(self.plots_csv_filepath, \"a\", newline=\"\")\n\n    def init_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Opens the database connection and CSV files\"\"\"\n\n        # 1. Open the database connection and cursor\n        worker_state[\"db_connection\"], worker_state[\"db_cursor\"] = open_database(\n            self.database_path\n        )\n\n        # 2. Write the header row to the plots CSV file\n        if self.plots_csv_filepath:\n            csv_writer = csv.writer(self.csvfile)\n            csv_writer.writerow([\"read_id\", \"plot_filepath\"])\n            worker_state[\"csv_writer\"] = csv_writer\n            worker_state[\"csvfile\"] = self.csvfile  # Store csvfile in worker_state\n\n        # Define paths to data and metadata CSV files\n        data_file_path = os.path.join(self.output_dir, f\"data_tmp_{worker_id}.csv\")\n        metadata_file_path = os.path.join(\n            self.output_dir, f\"metadata_tmp_{worker_id}.csv\"\n        )\n\n        # 3. Open data_file_path in append mode and write the header row if the file is empty\n        data_file = open(data_file_path, \"a\", newline=\"\")\n        data_writer = csv.writer(data_file)\n        if data_file.tell() == 0:  # Check if the file is empty\n            data_writer.writerow(\n                [\"read_id\", \"cap_class\", \"timeseries\"]\n            )  # Replace with your actual header\n\n        # Save the data file path and writer to worker_state\n        worker_state[\"data_file\"] = data_file\n        worker_state[\"data_writer\"] = data_writer\n\n        # 4. Open metadata_file_path in append mode and write the header row if the file is empty\n        metadata_file = open(metadata_file_path, \"a\", newline=\"\")\n        metadata_writer = csv.writer(metadata_file)\n        if metadata_file.tell() == 0:  # Check if the file is empty\n            metadata_writer.writerow(\n                [\n                    \"read_id\",\n                    \"parent_read_id\",\n                    \"pod5_file\",\n                    \"read_type\",\n                    \"roi_fasta\",\n                    \"roi_start\",\n                    \"roi_end\",\n                    \"fasta_length\",\n                    \"fasta\",\n                ]\n            )  # Replace with your actual header\n\n        # Save the metadata file path and writer to worker_state\n        worker_state[\"metadata_file\"] = metadata_file\n        worker_state[\"metadata_writer\"] = metadata_writer\n\n    def exit_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Closes the database connection and the CSV files.\"\"\"\n        conn = worker_state.get(\"db_connection\")\n        if conn:\n            conn.close()\n\n        # Close the plots csv file\n        csvfile = worker_state.get(\"csvfile\")\n        if self.plots_csv_filepath and csvfile:\n            csvfile.close()\n\n        # Close the data file\n        worker_state[\"data_file\"].close()\n\n        # Close the metadata file\n        worker_state[\"metadata_file\"].close()\n\n    def merge_data(self) -&gt; Tuple[str, str]:\n        \"\"\"Merges the data and metadata CSV files.\"\"\"\n        data_path = self._merge_csv_files(data_or_metadata=\"data\")\n        metadata_path = self._merge_csv_files(data_or_metadata=\"metadata\")\n        return data_path, metadata_path\n\n    def _merge_csv_files(self, data_or_metadata: str) -&gt; str:\n        \"\"\"Merges the data and metadata CSV files.\n\n        Args:\n            data_or_metadata (str): Whether to merge data or metadata CSV files.\n\n        Returns:\n            str: Path to the merged CSV file.\n        \"\"\"\n        cap_name = map_cap_int_to_name(self.cap_class)\n        data_path = os.path.join(self.output_dir, f\"{data_or_metadata}__{cap_name}.csv\")\n        # delete if the file already exists\n        if os.path.exists(data_path):\n            logger.info(f\"Overwriting existing {data_or_metadata} CSV file.\")\n            os.remove(data_path)\n        with open(data_path, \"w\", newline=\"\") as output_csv:\n            writer = csv.writer(output_csv)\n            for i in range(self.num_processes):\n                ind_csv_file = os.path.join(\n                    self.output_dir, f\"{data_or_metadata}_tmp_{i}.csv\"\n                )\n                # Open each CSV file and read its contents\n                with open(ind_csv_file) as input_csv:\n                    reader = csv.reader(input_csv)\n\n                    # If it's the first file, write the header to the output file\n                    if i == 0:\n                        header = next(reader)\n                        writer.writerow(header)\n                    else:\n                        next(reader)\n\n                    # Write the remaining rows to the output file\n                    for row in reader:\n                        writer.writerow(row)\n                os.remove(ind_csv_file)\n        logger.info(f\"Successfully merged {data_or_metadata} CSV file.\")\n        return data_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.DatabaseHandler.__init__","title":"<code>__init__(cap_class: int, num_processes: int, database_path: str, plots_csv_filepath: Union[str, None], output_dir: str) -&gt; None</code>","text":"<p>Initializes the index database handler</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def __init__(\n    self,\n    cap_class: int,\n    num_processes: int,\n    database_path: str,\n    plots_csv_filepath: Union[str, None],\n    output_dir: str,\n) -&gt; None:\n    \"\"\"Initializes the index database handler\"\"\"\n    self.cap_class = cap_class\n    self.database_path = database_path\n    self.plots_csv_filepath = plots_csv_filepath\n    self.num_processes = num_processes\n    self.output_dir = output_dir\n\n    # Open the plots CSV file in append mode\n    if self.plots_csv_filepath:\n        self.csvfile = open(self.plots_csv_filepath, \"a\", newline=\"\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.DatabaseHandler.exit_func","title":"<code>exit_func(worker_id: int, worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Closes the database connection and the CSV files.</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def exit_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Closes the database connection and the CSV files.\"\"\"\n    conn = worker_state.get(\"db_connection\")\n    if conn:\n        conn.close()\n\n    # Close the plots csv file\n    csvfile = worker_state.get(\"csvfile\")\n    if self.plots_csv_filepath and csvfile:\n        csvfile.close()\n\n    # Close the data file\n    worker_state[\"data_file\"].close()\n\n    # Close the metadata file\n    worker_state[\"metadata_file\"].close()\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.DatabaseHandler.init_func","title":"<code>init_func(worker_id: int, worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Opens the database connection and CSV files</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def init_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Opens the database connection and CSV files\"\"\"\n\n    # 1. Open the database connection and cursor\n    worker_state[\"db_connection\"], worker_state[\"db_cursor\"] = open_database(\n        self.database_path\n    )\n\n    # 2. Write the header row to the plots CSV file\n    if self.plots_csv_filepath:\n        csv_writer = csv.writer(self.csvfile)\n        csv_writer.writerow([\"read_id\", \"plot_filepath\"])\n        worker_state[\"csv_writer\"] = csv_writer\n        worker_state[\"csvfile\"] = self.csvfile  # Store csvfile in worker_state\n\n    # Define paths to data and metadata CSV files\n    data_file_path = os.path.join(self.output_dir, f\"data_tmp_{worker_id}.csv\")\n    metadata_file_path = os.path.join(\n        self.output_dir, f\"metadata_tmp_{worker_id}.csv\"\n    )\n\n    # 3. Open data_file_path in append mode and write the header row if the file is empty\n    data_file = open(data_file_path, \"a\", newline=\"\")\n    data_writer = csv.writer(data_file)\n    if data_file.tell() == 0:  # Check if the file is empty\n        data_writer.writerow(\n            [\"read_id\", \"cap_class\", \"timeseries\"]\n        )  # Replace with your actual header\n\n    # Save the data file path and writer to worker_state\n    worker_state[\"data_file\"] = data_file\n    worker_state[\"data_writer\"] = data_writer\n\n    # 4. Open metadata_file_path in append mode and write the header row if the file is empty\n    metadata_file = open(metadata_file_path, \"a\", newline=\"\")\n    metadata_writer = csv.writer(metadata_file)\n    if metadata_file.tell() == 0:  # Check if the file is empty\n        metadata_writer.writerow(\n            [\n                \"read_id\",\n                \"parent_read_id\",\n                \"pod5_file\",\n                \"read_type\",\n                \"roi_fasta\",\n                \"roi_start\",\n                \"roi_end\",\n                \"fasta_length\",\n                \"fasta\",\n            ]\n        )  # Replace with your actual header\n\n    # Save the metadata file path and writer to worker_state\n    worker_state[\"metadata_file\"] = metadata_file\n    worker_state[\"metadata_writer\"] = metadata_writer\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.DatabaseHandler.merge_data","title":"<code>merge_data() -&gt; Tuple[str, str]</code>","text":"<p>Merges the data and metadata CSV files.</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def merge_data(self) -&gt; Tuple[str, str]:\n    \"\"\"Merges the data and metadata CSV files.\"\"\"\n    data_path = self._merge_csv_files(data_or_metadata=\"data\")\n    metadata_path = self._merge_csv_files(data_or_metadata=\"metadata\")\n    return data_path, metadata_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.FASTQRecord","title":"<code>FASTQRecord</code>  <code>dataclass</code>","text":"<p>Simulates a FASTQ record object.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Read ID.</p> <code>seq</code> <code>str</code> <p>Read sequence.</p> Example <p>record = FASTQRecord(id=\"read1\", seq=\"ATCG\")</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>@dataclass\nclass FASTQRecord:\n    \"\"\"\n    Simulates a FASTQ record object.\n\n    Attributes:\n        id (str): Read ID.\n        seq (str): Read sequence.\n\n    Example:\n        &gt;&gt;&gt; record = FASTQRecord(id=\"read1\", seq=\"ATCG\")\n    \"\"\"\n\n    id: str\n    seq: str\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.collate_bam_pod5","title":"<code>collate_bam_pod5(bam_filepath: str, pod5_dir: str, num_processes: int, reference: str, cap_class: int, cap0_pos: int, train_or_test: str, plot_signal: bool, output_dir: str) -&gt; Tuple[str, str]</code>","text":"<p>Collates information from the BAM file and the POD5 files, aligns OTE to extracts the signal for the region of interest (ROI) for training or testing purposes. It also plots the ROI signal if requested.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <code>pod5_dir</code> <code>str</code> <p>Path to the directory containing the POD5 files.</p> required <code>num_processes</code> <code>int</code> <p>Number of processes to use for parallel processing.</p> required <code>reference</code> <code>str</code> <p>Reference sequence.</p> required <code>cap_class</code> <code>int</code> <p>Class label for the RNA cap.</p> required <code>cap0_pos</code> <code>int</code> <p>Position of the cap N1 base in the reference sequence (0-based).</p> required <code>train_or_test</code> <code>str</code> <p>Whether to extract ROI for training or testing.</p> required <code>plot_signal</code> <code>bool</code> <p>Whether to plot the ROI signal.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>Tuple[str, str]: Paths to the data and metadata CSV files.</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def collate_bam_pod5(\n    bam_filepath: str,\n    pod5_dir: str,\n    num_processes: int,\n    reference: str,\n    cap_class: int,\n    cap0_pos: int,\n    train_or_test: str,\n    plot_signal: bool,\n    output_dir: str,\n) -&gt; Tuple[str, str]:\n    \"\"\"\n    Collates information from the BAM file and the POD5 files,\n    aligns OTE to extracts the signal for the\n    region of interest (ROI) for training or testing purposes.\n    It also plots the ROI signal if requested.\n\n    Args:\n        bam_filepath (str): Path to the BAM file.\n        pod5_dir (str): Path to the directory containing the POD5 files.\n        num_processes (int): Number of processes to use for parallel processing.\n        reference (str): Reference sequence.\n        cap_class (int): Class label for the RNA cap.\n        cap0_pos (int): Position of the cap N1 base in the reference sequence (0-based).\n        train_or_test (str): Whether to extract ROI for training or testing.\n        plot_signal (bool): Whether to plot the ROI signal.\n        output_dir (str): Path to the output directory.\n\n    Returns:\n        Tuple[str, str]: Paths to the data and metadata CSV files.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    logger.info(\"Computing BAM total records...\")\n    num_bam_records = get_total_records(bam_filepath)\n    logger.info(f\"Found {num_bam_records} BAM records!\")\n\n    # Make index database\n    database_path = os.path.join(output_dir, \"database.db\")\n    index(pod5_dir, output_dir)\n\n    # If plots are requested, create the CSV file and the directories\n    plots_csv_filepath = None\n    if plot_signal:\n        good_reads_plots_dir = os.path.join(output_dir, \"plots\", \"good_reads\", \"0\")\n        bad_reads_plots_dir = os.path.join(output_dir, \"plots\", \"bad_reads\", \"0\")\n        os.makedirs(good_reads_plots_dir, exist_ok=True)\n        os.makedirs(bad_reads_plots_dir, exist_ok=True)\n        plots_csv_filepath = os.path.join(output_dir, \"plots\", \"plotpaths.csv\")\n\n    # Initialize the database handler\n    db_handler = DatabaseHandler(\n        cap_class, num_processes, database_path, plots_csv_filepath, output_dir\n    )\n\n    try:\n        logger.info(\"Processing BAM file using multiple processes...\")\n        with WorkerPool(\n            n_jobs=num_processes, use_worker_state=True, pass_worker_id=True\n        ) as pool:\n            iterator = zip(\n                generate_pickled_bam_records(bam_filepath),\n                repeat(reference),\n                repeat(cap_class),\n                repeat(cap0_pos),\n                repeat(train_or_test),\n                repeat(plot_signal),\n                repeat(output_dir),\n            )\n            for _ in pool.imap_unordered(\n                collate_bam_pod5_worker,\n                iterator,\n                worker_init=db_handler.init_func,\n                worker_exit=db_handler.exit_func,\n                progress_bar=True,\n                iterable_len=num_bam_records,\n            ):\n                pass  # We don't need to do anything with the results\n\n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n    finally:\n        if plots_csv_filepath:\n            csvfile = db_handler.csvfile\n            if csvfile:\n                csvfile.close()\n\n        # Merge the data and metadata CSV files\n        data_path, metadata_path = db_handler.merge_data()\n        logger.info(\"Cap signal data extracted successfully!\")\n    return data_path, metadata_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.collate_bam_pod5_worker","title":"<code>collate_bam_pod5_worker(worker_id: int, worker_state: Dict[str, Any], pickled_bam_data: bytes, reference: str, cap_class: int, cap0_pos: int, train_or_test: str, plot_signal: bool, output_dir: str) -&gt; None</code>","text":"<p>Worker function that collates information from POD5 and BAM file, finds the FASTA coordinates of  region of interest (ROI) and and extracts its signal.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>int Worker ID.</p> required <code>worker_state</code> <code>Dict[str, Any]</code> <p>dict Dictionary containing the database connection and cursor.</p> required <code>pickled_bam_data</code> <code>bytes</code> <p>bytes Pickled dictionary containing the BAM record information.</p> required <code>reference</code> <code>str</code> <p>str Reference sequence.</p> required <code>cap_class</code> <code>int</code> <p>int Class label for the RNA cap</p> required <code>cap0_pos</code> <code>int</code> <p>int Position of the cap0 base in the reference sequence.</p> required <code>train_or_test</code> <code>str</code> <p>str Whether to extract ROI for training or testing.</p> required <code>plot_signal</code> <code>bool</code> <p>bool Whether to plot the ROI signal.</p> required <code>output_dir</code> <code>str</code> <p>str Path to the output directory.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def collate_bam_pod5_worker(\n    worker_id: int,\n    worker_state: Dict[str, Any],\n    pickled_bam_data: bytes,\n    reference: str,\n    cap_class: int,\n    cap0_pos: int,\n    train_or_test: str,\n    plot_signal: bool,\n    output_dir: str,\n) -&gt; None:\n    \"\"\"Worker function that collates information from POD5 and BAM file, finds the\n    FASTA coordinates of  region of interest (ROI) and and extracts its signal.\n\n    Params:\n        worker_id: int\n            Worker ID.\n        worker_state: dict\n            Dictionary containing the database connection and cursor.\n        pickled_bam_data: bytes\n            Pickled dictionary containing the BAM record information.\n        reference: str\n            Reference sequence.\n        cap_class: int\n            Class label for the RNA cap\n        cap0_pos: int\n            Position of the cap0 base in the reference sequence.\n        train_or_test: str\n            Whether to extract ROI for training or testing.\n        plot_signal: bool\n            Whether to plot the ROI signal.\n        output_dir: str\n            Path to the output directory.\n\n    Returns:\n        None\n    \"\"\"\n    # 1. Get read info from bam record\n    bam_data = pickle.loads(pickled_bam_data)\n    read_id = bam_data[\"read_id\"]\n    pod5_filename = bam_data[\"pod5_filename\"]\n    parent_read_id = bam_data[\"parent_read_id\"]\n    # 2. Find the pod5 filepath corresponding to the pod5_filename in the database\n    pod5_filepath = fetch_filepath_using_filename(\n        worker_state[\"db_connection\"], worker_state[\"db_cursor\"], pod5_filename\n    )\n\n    # 3. Pull the read data from the multi-pod5 file\n    # If the read is a split read, pull the parent read data\n    if parent_read_id == \"\":\n        pod5_data = pull_read_from_pod5(read_id, pod5_filepath)\n    else:\n        pod5_data = pull_read_from_pod5(parent_read_id, pod5_filepath)\n\n    # 4. Extract the locations of each new base in signal coordinates\n    base_locs_in_signal = find_base_locs_in_signal(bam_data)\n\n    # 5. Get alignment of OTE with the read\n    # Simulate a FASTQ record object\n    read_fasta = bam_data[\"read_fasta\"]\n\n    # Check that the read is not empty\n    if read_fasta is None:\n        logger.warning(f\"Read {read_id} has empty FASTA. Skipping the read.\")\n        return None\n\n    fastq_record = FASTQRecord(read_id, read_fasta)\n    if train_or_test.lower() == \"train\":\n        aln_res = extract_roi_coords_train(\n            record=fastq_record, reference=reference, cap0_pos=cap0_pos\n        )\n    elif train_or_test.lower() == \"test\":\n        aln_res = extract_roi_coords_test(\n            record=fastq_record, reference=reference, cap0_pos=cap0_pos\n        )\n    else:\n        logger.warning(\n            \"Invalid train_or_test argument. Must be either 'train' or 'test'.\"\n        )\n        return None\n\n    # 6. Extract signal data for the ROI\n    start_base_idx_in_fasta = aln_res[\"left_flanking_region_start_fastq_pos\"]\n    end_base_idx_in_fasta = aln_res[\"right_flanking_region_start_fastq_pos\"]\n\n    roi_data = extract_roi_signal(\n        signal=pod5_data[\"signal_pa\"],\n        base_locs_in_signal=base_locs_in_signal,\n        fasta=read_fasta,\n        experiment_type=pod5_data[\"experiment_type\"],\n        start_base_idx_in_fasta=start_base_idx_in_fasta,\n        end_base_idx_in_fasta=end_base_idx_in_fasta,\n        num_left_clipped_bases=bam_data[\"num_left_clipped_bases\"],\n    )\n\n    # 7. Add additional information to the ROI data\n    roi_data[\"start_base_idx_in_fasta\"] = start_base_idx_in_fasta\n    roi_data[\"end_base_idx_in_fasta\"] = end_base_idx_in_fasta\n    roi_data[\"read_id\"] = read_id\n\n    # 8. Find if a read is good or bad\n    read_type = (\n        \"bad_reads\"\n        if start_base_idx_in_fasta is None and end_base_idx_in_fasta is None\n        else \"good_reads\"\n    )\n\n    # 9. Save the train/test and metadata information\n    # We need to store train/test data only for the good reads\n    precision = 8\n\n    # Define a vectorized function for formatting (if applicable)\n    def format_value(x: float) -&gt; str:\n        return f\"{x:.{precision}f}\"\n\n    vectorized_formatter = np.vectorize(format_value)\n\n    if read_type == \"good_reads\":\n        roi_signal: np.ndarray = roi_data[\"roi_signal\"]\n        if roi_signal.size == 0:\n            read_type = \"bad_reads\"\n        else:\n            timeseries_str = \",\".join(vectorized_formatter(roi_data[\"roi_signal\"]))\n            worker_state[\"data_writer\"].writerow([read_id, cap_class, timeseries_str])\n\n    # We need to store metadata for all reads (good and bad)\n    if read_fasta is not None:\n        read_length = len(read_fasta)\n    else:\n        read_length = 0\n\n    worker_state[\"metadata_writer\"].writerow(\n        [\n            read_id,\n            parent_read_id,\n            pod5_filepath,\n            read_type.rstrip(\"s\"),\n            roi_data[\"roi_fasta\"],\n            roi_data[\"start_base_idx_in_fasta\"],\n            roi_data[\"end_base_idx_in_fasta\"],\n            read_length,\n            read_fasta,\n        ]\n    )\n\n    # 10. Plot the ROI signal if requested\n    # Save plot in directories of 100 plots each separated into\n    # good and bad categories. Good reads mean those that have\n    # the OTE in them and bad reads mean those that do not.\n    if plot_signal:\n        count_key = f\"{read_type}_count\"\n        dir_key = f\"{read_type}_dir\"\n        with lock:\n            shared_dict[count_key] = shared_dict.get(count_key, 0) + 1\n            if shared_dict[count_key] &gt; 100:\n                worker_state[\n                    \"csvfile\"\n                ].flush()  # write the rows in the buffer to the csv file\n                shared_dict[dir_key] = shared_dict.get(dir_key, 0) + 1\n                shared_dict[count_key] = 1\n                os.makedirs(\n                    os.path.join(\n                        output_dir, \"plots\", read_type, str(shared_dict[dir_key])\n                    ),\n                    exist_ok=True,\n                )\n            # Get the current timestamp\n            # We append the timestamp to the name of the plot file\n            # so that we can handle multiple plots for the same read\n            # due to multiple alignments (secondary/supp.) in SAM files\n            timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n            plot_filepath = os.path.join(\n                output_dir,\n                \"plots\",\n                read_type,\n                str(shared_dict[dir_key]),\n                f\"{read_id}__{timestamp}.html\",\n            )\n            # Write the data for this plot\n            worker_state[\"csv_writer\"].writerow([read_id, plot_filepath])\n        # Suppress the output of align() function\n        with contextlib.redirect_stdout(None):\n            _, _, chunked_aln_str, alignment_score = align(\n                query_seq=read_fasta, target_seq=reference, pretty_print_alns=True\n            )\n        plot_roi_signal(\n            pod5_data,\n            bam_data,\n            roi_data,\n            start_base_idx_in_fasta,\n            end_base_idx_in_fasta,\n            plot_filepath,\n            chunked_aln_str,\n            alignment_score,\n        )\n\n    return None\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.collate_bam_pod5_wrapper","title":"<code>collate_bam_pod5_wrapper(bam_filepath: str, pod5_dir: str, num_processes: int, reference: str, cap_class: int, cap0_pos: int, train_or_test: str, plot_signal: bool, output_dir: str, debug_code: bool, formatted_command: Optional[str]) -&gt; None</code>","text":"<p>Wrapper function for collate_bam_pod5 that sets up logging and handles output.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <code>pod5_dir</code> <code>str</code> <p>Path to the directory containing the POD5 files.</p> required <code>num_processes</code> <code>int</code> <p>Number of processes to use for parallel processing.</p> required <code>reference</code> <code>str</code> <p>Reference sequence.</p> required <code>cap_class</code> <code>int</code> <p>Class label for the RNA cap.</p> required <code>cap0_pos</code> <code>int</code> <p>Position of the cap N1 base in the reference sequence (0-based).</p> required <code>train_or_test</code> <code>str</code> <p>Whether to extract ROI for training or testing.</p> required <code>plot_signal</code> <code>bool</code> <p>Whether to plot the ROI signal.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <code>debug_code</code> <code>bool</code> <p>Whether to show debug information in logs.</p> required <code>formatted_command</code> <code>Optional[str]</code> <p>Formatted command string for logging.</p> required Source code in <code>src/capfinder/collate.py</code> <pre><code>def collate_bam_pod5_wrapper(\n    bam_filepath: str,\n    pod5_dir: str,\n    num_processes: int,\n    reference: str,\n    cap_class: int,\n    cap0_pos: int,\n    train_or_test: str,\n    plot_signal: bool,\n    output_dir: str,\n    debug_code: bool,\n    formatted_command: Optional[str],\n) -&gt; None:\n    \"\"\"\n    Wrapper function for collate_bam_pod5 that sets up logging and handles output.\n\n    Args:\n        bam_filepath (str): Path to the BAM file.\n        pod5_dir (str): Path to the directory containing the POD5 files.\n        num_processes (int): Number of processes to use for parallel processing.\n        reference (str): Reference sequence.\n        cap_class (int): Class label for the RNA cap.\n        cap0_pos (int): Position of the cap N1 base in the reference sequence (0-based).\n        train_or_test (str): Whether to extract ROI for training or testing.\n        plot_signal (bool): Whether to plot the ROI signal.\n        output_dir (str): Path to the output directory.\n        debug_code (bool): Whether to show debug information in logs.\n        formatted_command (Optional[str]): Formatted command string for logging.\n    \"\"\"\n    log_filepath = configure_logger(\n        os.path.join(output_dir, \"logs\"), show_location=debug_code\n    )\n    configure_prefect_logging(show_location=debug_code)\n    version_info = version(\"capfinder\")\n    log_header(f\"Using Capfinder v{version_info}\")\n    logger.info(formatted_command)\n\n    data_path, metadata_path = collate_bam_pod5(\n        bam_filepath,\n        pod5_dir,\n        num_processes,\n        reference,\n        cap_class,\n        cap0_pos,\n        train_or_test,\n        plot_signal,\n        output_dir,\n    )\n    grey = \"\\033[90m\"\n    reset = \"\\033[0m\"\n    log_output(\n        f\"Cap data has been saved to the following path:\\n {grey}{data_path}{reset}\\nCap metadata have been saved to the following path:\\n {grey}{metadata_path}{reset}\\nThe log file has been saved to:\\n {grey}{log_filepath}{reset}\"\n    )\n    log_header(\"Processing finished!\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.collate.generate_pickled_bam_records","title":"<code>generate_pickled_bam_records(bam_filepath: str) -&gt; Generator[bytes, None, None]</code>","text":"<p>Generate pickled BAM records from a BAM file.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <p>Yields:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>Pickled BAM record.</p> Source code in <code>src/capfinder/collate.py</code> <pre><code>def generate_pickled_bam_records(bam_filepath: str) -&gt; Generator[bytes, None, None]:\n    \"\"\"\n    Generate pickled BAM records from a BAM file.\n\n    Args:\n        bam_filepath (str): Path to the BAM file.\n\n    Yields:\n        bytes: Pickled BAM record.\n    \"\"\"\n    with pysam.AlignmentFile(bam_filepath, \"rb\") as bam_file:\n        for record in bam_file:\n            yield pickle.dumps(get_signal_info(record))\n</code></pre>"},{"location":"api_docs/#src.capfinder.constants","title":"<code>constants</code>","text":"<p>The module contains constants used in the capfinder package.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate","title":"<code>cyclic_learing_rate</code>","text":""},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CometLRLogger","title":"<code>CometLRLogger</code>","text":"<p>               Bases: <code>Callback</code></p> <p>A callback to log the learning rate to Comet.ml during training.</p> <p>This callback logs the learning rate at the beginning of each epoch and at the end of each batch to a Comet.ml experiment.</p> <p>Attributes:</p> Name Type Description <code>experiment</code> <code>Experiment</code> <p>The Comet.ml experiment to log to.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>class CometLRLogger(Callback):\n    \"\"\"\n    A callback to log the learning rate to Comet.ml during training.\n\n    This callback logs the learning rate at the beginning of each epoch\n    and at the end of each batch to a Comet.ml experiment.\n\n    Attributes:\n        experiment (Experiment): The Comet.ml experiment to log to.\n    \"\"\"\n\n    def __init__(self, experiment: Experiment) -&gt; None:\n        \"\"\"\n        Initialize the CometLRLogger.\n\n        Args:\n            experiment (Experiment): The Comet.ml experiment to log to.\n        \"\"\"\n        super().__init__()\n        self.experiment: Experiment = experiment\n\n    def on_epoch_begin(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Log the learning rate at the beginning of each epoch.\n\n        Args:\n            epoch (int): The current epoch number.\n            logs (Optional[Dict[str, Any]]): The logs dictionary.\n        \"\"\"\n        lr: Union[float, np.ndarray] = self.model.optimizer.learning_rate\n        if hasattr(lr, \"numpy\"):\n            lr = lr.numpy()\n        self.experiment.log_metric(\"learning_rate\", lr, step=epoch)\n\n    def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Log the learning rate at the end of each batch.\n\n        Args:\n            batch (int): The current batch number.\n            logs (Optional[Dict[str, Any]]): The logs dictionary.\n        \"\"\"\n        lr: Union[float, np.ndarray] = self.model.optimizer.learning_rate\n        if hasattr(lr, \"numpy\"):\n            lr = lr.numpy()\n        self.experiment.log_metric(\n            \"learning_rate\", lr, step=self.model.optimizer.iterations.numpy()\n        )\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CometLRLogger.__init__","title":"<code>__init__(experiment: Experiment) -&gt; None</code>","text":"<p>Initialize the CometLRLogger.</p> <p>Parameters:</p> Name Type Description Default <code>experiment</code> <code>Experiment</code> <p>The Comet.ml experiment to log to.</p> required Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def __init__(self, experiment: Experiment) -&gt; None:\n    \"\"\"\n    Initialize the CometLRLogger.\n\n    Args:\n        experiment (Experiment): The Comet.ml experiment to log to.\n    \"\"\"\n    super().__init__()\n    self.experiment: Experiment = experiment\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CometLRLogger.on_batch_end","title":"<code>on_batch_end(batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Log the learning rate at the end of each batch.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>The current batch number.</p> required <code>logs</code> <code>Optional[Dict[str, Any]]</code> <p>The logs dictionary.</p> <code>None</code> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Log the learning rate at the end of each batch.\n\n    Args:\n        batch (int): The current batch number.\n        logs (Optional[Dict[str, Any]]): The logs dictionary.\n    \"\"\"\n    lr: Union[float, np.ndarray] = self.model.optimizer.learning_rate\n    if hasattr(lr, \"numpy\"):\n        lr = lr.numpy()\n    self.experiment.log_metric(\n        \"learning_rate\", lr, step=self.model.optimizer.iterations.numpy()\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CometLRLogger.on_epoch_begin","title":"<code>on_epoch_begin(epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Log the learning rate at the beginning of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required <code>logs</code> <code>Optional[Dict[str, Any]]</code> <p>The logs dictionary.</p> <code>None</code> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_epoch_begin(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Log the learning rate at the beginning of each epoch.\n\n    Args:\n        epoch (int): The current epoch number.\n        logs (Optional[Dict[str, Any]]): The logs dictionary.\n    \"\"\"\n    lr: Union[float, np.ndarray] = self.model.optimizer.learning_rate\n    if hasattr(lr, \"numpy\"):\n        lr = lr.numpy()\n    self.experiment.log_metric(\"learning_rate\", lr, step=epoch)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CustomProgressCallback","title":"<code>CustomProgressCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>A custom callback to print the learning rate at the end of each epoch.</p> <p>This callback prints the current learning rate after Keras' built-in progress bar for each epoch.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>class CustomProgressCallback(keras.callbacks.Callback):\n    \"\"\"\n    A custom callback to print the learning rate at the end of each epoch.\n\n    This callback prints the current learning rate after Keras' built-in\n    progress bar for each epoch.\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the CustomProgressCallback.\"\"\"\n        super().__init__()\n\n    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"\n        Print the learning rate at the end of each epoch.\n\n        Args:\n            epoch (int): The current epoch number.\n            logs (Optional[Dict[str, Any]]): The logs dictionary.\n        \"\"\"\n        lr: Union[float, np.ndarray] = self.model.optimizer.learning_rate\n        if hasattr(lr, \"numpy\"):\n            lr = lr.numpy()\n        print(f\"\\nLearning rate: {lr:.6f}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CustomProgressCallback.__init__","title":"<code>__init__() -&gt; None</code>","text":"<p>Initialize the CustomProgressCallback.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the CustomProgressCallback.\"\"\"\n    super().__init__()\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CustomProgressCallback.on_epoch_end","title":"<code>on_epoch_end(epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Print the learning rate at the end of each epoch.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch number.</p> required <code>logs</code> <code>Optional[Dict[str, Any]]</code> <p>The logs dictionary.</p> <code>None</code> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"\n    Print the learning rate at the end of each epoch.\n\n    Args:\n        epoch (int): The current epoch number.\n        logs (Optional[Dict[str, Any]]): The logs dictionary.\n    \"\"\"\n    lr: Union[float, np.ndarray] = self.model.optimizer.learning_rate\n    if hasattr(lr, \"numpy\"):\n        lr = lr.numpy()\n    print(f\"\\nLearning rate: {lr:.6f}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CyclicLR","title":"<code>CyclicLR</code>","text":"<p>               Bases: <code>Callback</code></p> <p>This callback implements a cyclical learning rate policy (CLR). The method cycles the learning rate between two boundaries with some constant frequency.</p>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CyclicLR--arguments","title":"Arguments","text":"<pre><code>base_lr: initial learning rate which is the\n    lower boundary in the cycle.\nmax_lr: upper boundary in the cycle. Functionally,\n    it defines the cycle amplitude (max_lr - base_lr).\n    The lr at any cycle is the sum of base_lr\n    and some scaling of the amplitude; therefore\n    max_lr may not actually be reached depending on\n    scaling function.\nstep_size: number of training iterations per\n    half cycle. Authors suggest setting step_size\n    2-8 x training iterations in epoch.\nmode: one of {triangular, triangular2, exp_range}.\n    Default 'triangular'.\n    Values correspond to policies detailed above.\n    If scale_fn is not None, this argument is ignored.\ngamma: constant in 'exp_range' scaling function:\n    gamma**(cycle iterations)\nscale_fn: Custom scaling policy defined by a single\n    argument lambda function, where\n    0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0.\n    mode paramater is ignored\nscale_mode: {'cycle', 'iterations'}.\n    Defines whether scale_fn is evaluated on\n    cycle number or cycle iterations (training\n    iterations since start of cycle). Default is 'cycle'.\n</code></pre> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>class CyclicLR(Callback):\n    \"\"\"\n    This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency.\n\n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore\n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where\n            0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0.\n            mode paramater is ignored\n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on\n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_lr: float = 0.001,\n        max_lr: float = 0.006,\n        step_size: float = 2000.0,\n        mode: str = \"triangular\",\n        gamma: float = 1.0,\n        scale_fn: Optional[Callable[[float], float]] = None,\n        scale_mode: str = \"cycle\",\n    ) -&gt; None:\n        super().__init__()\n\n        self.base_lr: float = base_lr\n        self.max_lr: float = max_lr\n        self.step_size: float = step_size\n        self.mode: str = mode\n        self.gamma: float = gamma\n\n        if scale_fn is None:\n            if self.mode == \"triangular\":\n                self.scale_fn = lambda x: 1.0\n                self.scale_mode = \"cycle\"\n            elif self.mode == \"triangular2\":\n                self.scale_fn = lambda x: 1 / (2.0 ** (x - 1))\n                self.scale_mode = \"cycle\"\n            elif self.mode == \"exp_range\":\n                self.scale_fn = lambda x: gamma**x\n                self.scale_mode = \"iterations\"\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.clr_iterations: float = 0.0\n        self.trn_iterations: float = 0.0\n        self.history: Dict[str, list] = {}\n\n        self._reset()\n\n    def _reset(\n        self,\n        new_base_lr: Optional[float] = None,\n        new_max_lr: Optional[float] = None,\n        new_step_size: Optional[float] = None,\n    ) -&gt; None:\n        \"\"\"Resets cycle iterations.\n        Optional boundary/step size adjustment.\n        \"\"\"\n        if new_base_lr is not None:\n            self.base_lr = new_base_lr\n        if new_max_lr is not None:\n            self.max_lr = new_max_lr\n        if new_step_size is not None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.0\n\n    def clr(self) -&gt; Union[float, NDArray[np.float64]]:\n        cycle: float = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n        x: float = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n        clr_value: float = (\n            self.base_lr\n            + (self.max_lr - self.base_lr)\n            * np.maximum(0, (1 - x))\n            * self.scale_fn(cycle)\n            if self.scale_mode == \"cycle\"\n            else self.scale_fn(self.clr_iterations)\n        )\n        return (\n            float(clr_value)\n            if isinstance(clr_value, float)\n            else np.array(clr_value, dtype=np.float64)\n        )\n\n    def on_train_begin(self, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Initialize the learning rate to the base learning rate.\"\"\"\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            self.model.optimizer.learning_rate.assign(self.base_lr)\n        else:\n            self.model.optimizer.learning_rate.assign(self.clr())\n\n    def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Record previous batch statistics and update the learning rate.\"\"\"\n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault(\"lr\", []).append(\n            self.model.optimizer.learning_rate.numpy()\n        )\n        self.history.setdefault(\"iterations\", []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.model.optimizer.learning_rate.assign(self.clr())\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CyclicLR.on_batch_end","title":"<code>on_batch_end(batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Record previous batch statistics and update the learning rate.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Record previous batch statistics and update the learning rate.\"\"\"\n    logs = logs or {}\n    self.trn_iterations += 1\n    self.clr_iterations += 1\n\n    self.history.setdefault(\"lr\", []).append(\n        self.model.optimizer.learning_rate.numpy()\n    )\n    self.history.setdefault(\"iterations\", []).append(self.trn_iterations)\n\n    for k, v in logs.items():\n        self.history.setdefault(k, []).append(v)\n\n    self.model.optimizer.learning_rate.assign(self.clr())\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.CyclicLR.on_train_begin","title":"<code>on_train_begin(logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Initialize the learning rate to the base learning rate.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_train_begin(self, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Initialize the learning rate to the base learning rate.\"\"\"\n    logs = logs or {}\n\n    if self.clr_iterations == 0:\n        self.model.optimizer.learning_rate.assign(self.base_lr)\n    else:\n        self.model.optimizer.learning_rate.assign(self.clr())\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler","title":"<code>SGDRScheduler</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Cosine annealing learning rate scheduler with periodic restarts.</p> <p>Parameters:</p> Name Type Description Default <code>min_lr</code> <code>float</code> <p>The lower bound of the learning rate range for the experiment.</p> required <code>max_lr</code> <code>float</code> <p>The upper bound of the learning rate range for the experiment.</p> required <code>steps_per_epoch</code> <code>int</code> <p>Number of mini-batches in the dataset.</p> required <code>lr_decay</code> <code>float</code> <p>Reduce the max_lr after the completion of each cycle.</p> <code>1.0</code> <code>cycle_length</code> <code>int</code> <p>Initial number of epochs in a cycle.</p> <code>10</code> <code>mult_factor</code> <code>float</code> <p>Scale epochs_to_restart after each full cycle completion.</p> <code>2.0</code> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>class SGDRScheduler(Callback):\n    \"\"\"\n    Cosine annealing learning rate scheduler with periodic restarts.\n\n    Args:\n        min_lr: The lower bound of the learning rate range for the experiment.\n        max_lr: The upper bound of the learning rate range for the experiment.\n        steps_per_epoch: Number of mini-batches in the dataset.\n        lr_decay: Reduce the max_lr after the completion of each cycle.\n        cycle_length: Initial number of epochs in a cycle.\n        mult_factor: Scale epochs_to_restart after each full cycle completion.\n    \"\"\"\n\n    def __init__(\n        self,\n        min_lr: float,\n        max_lr: float,\n        steps_per_epoch: int,\n        lr_decay: float = 1.0,\n        cycle_length: int = 10,\n        mult_factor: float = 2.0,\n    ) -&gt; None:\n        super().__init__()\n        self.min_lr: float = min_lr\n        self.max_lr: float = max_lr\n        self.lr_decay: float = lr_decay\n        self.batch_since_restart: int = 0\n        self.next_restart: int = cycle_length\n        self.steps_per_epoch: int = steps_per_epoch\n        self.cycle_length: float = cycle_length\n        self.mult_factor: float = mult_factor\n        self.history: Dict[str, list] = {}\n        self.best_weights: Optional[list] = None\n\n    def clr(self) -&gt; float:\n        \"\"\"Calculate the learning rate.\"\"\"\n        fraction_to_restart: float = self.batch_since_restart / (\n            self.steps_per_epoch * self.cycle_length\n        )\n        lr: float = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (\n            1 + np.cos(fraction_to_restart * np.pi)\n        )\n        return float(lr)\n\n    def set_lr(self, lr: float) -&gt; None:\n        \"\"\"Set the learning rate for the optimizer.\"\"\"\n        self.model.optimizer.learning_rate.assign(lr)\n\n    def get_lr(self) -&gt; float:\n        \"\"\"Get the current learning rate.\"\"\"\n        return float(self.model.optimizer.learning_rate.value)\n\n    def on_train_begin(self, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Initialize the learning rate to the maximum value at the start of training.\"\"\"\n        logs = logs or {}\n        self.set_lr(self.max_lr)\n\n    def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Record previous batch statistics and update the learning rate.\"\"\"\n        logs = logs or {}\n        self.history.setdefault(\"lr\", []).append(self.get_lr())\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n\n        self.batch_since_restart += 1\n        new_lr: float = self.clr()\n        self.set_lr(new_lr)\n\n    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Check for end of current cycle, apply restarts when necessary.\"\"\"\n        if epoch + 1 == self.next_restart:\n            self.batch_since_restart = 0\n            self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n            self.next_restart += int(self.cycle_length)\n            self.max_lr *= self.lr_decay\n            self.best_weights = self.model.get_weights()\n\n    def on_train_end(self, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n        \"\"\"Set weights to the values from the end of the most recent cycle for best performance.\"\"\"\n        if self.best_weights is not None:\n            self.model.set_weights(self.best_weights)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.clr","title":"<code>clr() -&gt; float</code>","text":"<p>Calculate the learning rate.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def clr(self) -&gt; float:\n    \"\"\"Calculate the learning rate.\"\"\"\n    fraction_to_restart: float = self.batch_since_restart / (\n        self.steps_per_epoch * self.cycle_length\n    )\n    lr: float = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (\n        1 + np.cos(fraction_to_restart * np.pi)\n    )\n    return float(lr)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.get_lr","title":"<code>get_lr() -&gt; float</code>","text":"<p>Get the current learning rate.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def get_lr(self) -&gt; float:\n    \"\"\"Get the current learning rate.\"\"\"\n    return float(self.model.optimizer.learning_rate.value)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.on_batch_end","title":"<code>on_batch_end(batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Record previous batch statistics and update the learning rate.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_batch_end(self, batch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Record previous batch statistics and update the learning rate.\"\"\"\n    logs = logs or {}\n    self.history.setdefault(\"lr\", []).append(self.get_lr())\n    for k, v in logs.items():\n        self.history.setdefault(k, []).append(v)\n\n    self.batch_since_restart += 1\n    new_lr: float = self.clr()\n    self.set_lr(new_lr)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.on_epoch_end","title":"<code>on_epoch_end(epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Check for end of current cycle, apply restarts when necessary.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Check for end of current cycle, apply restarts when necessary.\"\"\"\n    if epoch + 1 == self.next_restart:\n        self.batch_since_restart = 0\n        self.cycle_length = np.ceil(self.cycle_length * self.mult_factor)\n        self.next_restart += int(self.cycle_length)\n        self.max_lr *= self.lr_decay\n        self.best_weights = self.model.get_weights()\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.on_train_begin","title":"<code>on_train_begin(logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Initialize the learning rate to the maximum value at the start of training.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_train_begin(self, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Initialize the learning rate to the maximum value at the start of training.\"\"\"\n    logs = logs or {}\n    self.set_lr(self.max_lr)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.on_train_end","title":"<code>on_train_end(logs: Optional[Dict[str, Any]] = None) -&gt; None</code>","text":"<p>Set weights to the values from the end of the most recent cycle for best performance.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def on_train_end(self, logs: Optional[Dict[str, Any]] = None) -&gt; None:\n    \"\"\"Set weights to the values from the end of the most recent cycle for best performance.\"\"\"\n    if self.best_weights is not None:\n        self.model.set_weights(self.best_weights)\n</code></pre>"},{"location":"api_docs/#src.capfinder.cyclic_learing_rate.SGDRScheduler.set_lr","title":"<code>set_lr(lr: float) -&gt; None</code>","text":"<p>Set the learning rate for the optimizer.</p> Source code in <code>src/capfinder/cyclic_learing_rate.py</code> <pre><code>def set_lr(self, lr: float) -&gt; None:\n    \"\"\"Set the learning rate for the optimizer.\"\"\"\n    self.model.optimizer.learning_rate.assign(lr)\n</code></pre>"},{"location":"api_docs/#src.capfinder.data_loader","title":"<code>data_loader</code>","text":""},{"location":"api_docs/#src.capfinder.data_loader.combine_datasets","title":"<code>combine_datasets(features_dataset: tf.data.Dataset, labels_dataset: tf.data.Dataset, batch_size: int, num_timesteps: int) -&gt; tf.data.Dataset</code>","text":"<p>Combine feature and label datasets with padded batching.</p>"},{"location":"api_docs/#src.capfinder.data_loader.combine_datasets--parameters","title":"Parameters:","text":"<p>features_dataset : tf.data.Dataset     The dataset containing features. labels_dataset : tf.data.Dataset     The dataset containing labels. batch_size : int     The size of each batch. num_timesteps : int     The number of time steps in each time series.</p>"},{"location":"api_docs/#src.capfinder.data_loader.combine_datasets--returns","title":"Returns:","text":"<p>tf.data.Dataset     A combined dataset with features and labels, padded and batched.</p> Source code in <code>src/capfinder/data_loader.py</code> <pre><code>def combine_datasets(\n    features_dataset: tf.data.Dataset,\n    labels_dataset: tf.data.Dataset,\n    batch_size: int,\n    num_timesteps: int,\n) -&gt; tf.data.Dataset:\n    \"\"\"Combine feature and label datasets with padded batching.\n\n    Parameters:\n    -----------\n    features_dataset : tf.data.Dataset\n        The dataset containing features.\n    labels_dataset : tf.data.Dataset\n        The dataset containing labels.\n    batch_size : int\n        The size of each batch.\n    num_timesteps : int\n        The number of time steps in each time series.\n\n    Returns:\n    --------\n    tf.data.Dataset\n        A combined dataset with features and labels, padded and batched.\n    \"\"\"\n    dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n    dataset = dataset.padded_batch(\n        batch_size, padded_shapes=([num_timesteps, 1], []), drop_remainder=True\n    )\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.data_loader.load_datasets","title":"<code>load_datasets(train_x_path: str, train_y_path: str, val_x_path: str, val_y_path: str, batch_size: int, num_timesteps: int) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]</code>","text":"<p>Load and combine train and validation datasets.</p>"},{"location":"api_docs/#src.capfinder.data_loader.load_datasets--parameters","title":"Parameters:","text":"<p>train_x_path : str     Path to the CSV file containing training features. train_y_path : str     Path to the CSV file containing training labels. val_x_path : str     Path to the CSV file containing validation features. val_y_path : str     Path to the CSV file containing validation labels. batch_size : int     The size of each batch. num_timesteps : int     The number of time steps in each time series.</p>"},{"location":"api_docs/#src.capfinder.data_loader.load_datasets--returns","title":"Returns:","text":"<p>Tuple[tf.data.Dataset, tf.data.Dataset]     A tuple containing the combined training dataset and validation dataset.</p> Source code in <code>src/capfinder/data_loader.py</code> <pre><code>def load_datasets(\n    train_x_path: str,\n    train_y_path: str,\n    val_x_path: str,\n    val_y_path: str,\n    batch_size: int,\n    num_timesteps: int,\n) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]:\n    \"\"\"Load and combine train and validation datasets.\n\n    Parameters:\n    -----------\n    train_x_path : str\n        Path to the CSV file containing training features.\n    train_y_path : str\n        Path to the CSV file containing training labels.\n    val_x_path : str\n        Path to the CSV file containing validation features.\n    val_y_path : str\n        Path to the CSV file containing validation labels.\n    batch_size : int\n        The size of each batch.\n    num_timesteps : int\n        The number of time steps in each time series.\n\n    Returns:\n    --------\n    Tuple[tf.data.Dataset, tf.data.Dataset]\n        A tuple containing the combined training dataset and validation dataset.\n    \"\"\"\n    train_features_dataset = load_feature_dataset(train_x_path, num_timesteps)\n    train_labels_dataset = load_label_dataset(train_y_path)\n    val_features_dataset = load_feature_dataset(val_x_path, num_timesteps)\n    val_labels_dataset = load_label_dataset(val_y_path)\n\n    train_dataset = combine_datasets(\n        train_features_dataset, train_labels_dataset, batch_size, num_timesteps\n    )\n    val_dataset = combine_datasets(\n        val_features_dataset, val_labels_dataset, batch_size, num_timesteps\n    )\n\n    return train_dataset, val_dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.data_loader.load_feature_dataset","title":"<code>load_feature_dataset(file_path: str, num_timesteps: int) -&gt; tf.data.Dataset</code>","text":"<p>Load feature dataset from a CSV file.</p>"},{"location":"api_docs/#src.capfinder.data_loader.load_feature_dataset--parameters","title":"Parameters:","text":"<p>file_path : str     The path to the CSV file containing features. num_timesteps : int     The number of time steps in each time series.</p>"},{"location":"api_docs/#src.capfinder.data_loader.load_feature_dataset--returns","title":"Returns:","text":"<p>tf.data.Dataset     A TensorFlow dataset containing the parsed features.</p> Source code in <code>src/capfinder/data_loader.py</code> <pre><code>def load_feature_dataset(file_path: str, num_timesteps: int) -&gt; tf.data.Dataset:\n    \"\"\"Load feature dataset from a CSV file.\n\n    Parameters:\n    -----------\n    file_path : str\n        The path to the CSV file containing features.\n    num_timesteps : int\n        The number of time steps in each time series.\n\n    Returns:\n    --------\n    tf.data.Dataset\n        A TensorFlow dataset containing the parsed features.\n    \"\"\"\n    dataset = tf.data.TextLineDataset(file_path)\n    dataset = dataset.skip(1)  # Skip header row\n    dataset = dataset.map(\n        lambda x: parse_features(x, num_timesteps), num_parallel_calls=tf.data.AUTOTUNE\n    )\n    return dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.data_loader.load_label_dataset","title":"<code>load_label_dataset(file_path: str) -&gt; tf.data.Dataset</code>","text":"<p>Load label dataset from a CSV file.</p>"},{"location":"api_docs/#src.capfinder.data_loader.load_label_dataset--parameters","title":"Parameters:","text":"<p>file_path : str     The path to the CSV file containing labels.</p>"},{"location":"api_docs/#src.capfinder.data_loader.load_label_dataset--returns","title":"Returns:","text":"<p>tf.data.Dataset     A TensorFlow dataset containing the parsed labels.</p> Source code in <code>src/capfinder/data_loader.py</code> <pre><code>def load_label_dataset(file_path: str) -&gt; tf.data.Dataset:\n    \"\"\"Load label dataset from a CSV file.\n\n    Parameters:\n    -----------\n    file_path : str\n        The path to the CSV file containing labels.\n\n    Returns:\n    --------\n    tf.data.Dataset\n        A TensorFlow dataset containing the parsed labels.\n    \"\"\"\n    dataset = tf.data.TextLineDataset(file_path)\n    dataset = dataset.skip(1)  # Skip header row\n    dataset = dataset.map(parse_labels, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.data_loader.parse_features","title":"<code>parse_features(line: tf.Tensor, num_timesteps: int) -&gt; tf.Tensor</code>","text":"<p>Parse features from a CSV line and reshape them.</p>"},{"location":"api_docs/#src.capfinder.data_loader.parse_features--parameters","title":"Parameters:","text":"<p>line : tf.Tensor     A tensor representing a single line from the CSV file. num_timesteps : int     The number of time steps in each time series.</p>"},{"location":"api_docs/#src.capfinder.data_loader.parse_features--returns","title":"Returns:","text":"<p>tf.Tensor     A tensor of shape (num_timesteps, 1) containing the parsed features.</p> Source code in <code>src/capfinder/data_loader.py</code> <pre><code>def parse_features(line: tf.Tensor, num_timesteps: int) -&gt; tf.Tensor:\n    \"\"\"Parse features from a CSV line and reshape them.\n\n    Parameters:\n    -----------\n    line : tf.Tensor\n        A tensor representing a single line from the CSV file.\n    num_timesteps : int\n        The number of time steps in each time series.\n\n    Returns:\n    --------\n    tf.Tensor\n        A tensor of shape (num_timesteps, 1) containing the parsed features.\n    \"\"\"\n    column_defaults = [[0.0]] * num_timesteps\n    fields = tf.io.decode_csv(line, record_defaults=column_defaults)\n    features = tf.reshape(fields, (num_timesteps, 1))  # Reshape to (timesteps, 1)\n    return features\n</code></pre>"},{"location":"api_docs/#src.capfinder.data_loader.parse_labels","title":"<code>parse_labels(line: tf.Tensor) -&gt; tf.Tensor</code>","text":"<p>Parse labels from a CSV line.</p>"},{"location":"api_docs/#src.capfinder.data_loader.parse_labels--parameters","title":"Parameters:","text":"<p>line : tf.Tensor     A tensor representing a single line from the CSV file.</p>"},{"location":"api_docs/#src.capfinder.data_loader.parse_labels--returns","title":"Returns:","text":"<p>tf.Tensor     A tensor containing the parsed label.</p> Source code in <code>src/capfinder/data_loader.py</code> <pre><code>def parse_labels(line: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"Parse labels from a CSV line.\n\n    Parameters:\n    -----------\n    line : tf.Tensor\n        A tensor representing a single line from the CSV file.\n\n    Returns:\n    --------\n    tf.Tensor\n        A tensor containing the parsed label.\n    \"\"\"\n    label = tf.io.decode_csv(line, record_defaults=[[0]])\n    return label[0]\n</code></pre>"},{"location":"api_docs/#src.capfinder.download_model","title":"<code>download_model</code>","text":""},{"location":"api_docs/#src.capfinder.download_model.create_version_info_file","title":"<code>create_version_info_file(output_dir: str, version: str) -&gt; None</code>","text":"<p>Create a file to store the version information. If any file with a name starting with \"v\" already exists in the output directory, delete it before creating a new one.</p> <p>Parameters: output_dir (str): The directory where the version file will be created. version (str): The version string to be written to the file.</p> <p>Returns: None</p> Source code in <code>src/capfinder/download_model.py</code> <pre><code>def create_version_info_file(output_dir: str, version: str) -&gt; None:\n    \"\"\"\n    Create a file to store the version information. If any file with a name starting\n    with \"v\" already exists in the output directory, delete it before creating a new one.\n\n    Parameters:\n    output_dir (str): The directory where the version file will be created.\n    version (str): The version string to be written to the file.\n\n    Returns:\n    None\n    \"\"\"\n    version_file = os.path.join(output_dir, f\"v{version}\")\n\n    # Find and delete any existing version file\n    existing_files = glob.glob(os.path.join(output_dir, \"v*\"))\n    for file in existing_files:\n        os.remove(file)\n\n    # Create a new version file\n    with open(version_file, \"w\") as f:\n        f.write(version)\n</code></pre>"},{"location":"api_docs/#src.capfinder.download_model.download_comet_model","title":"<code>download_comet_model(workspace: str, model_name: str, version: str, output_dir: str = './', force_download: bool = False) -&gt; None</code>","text":"<p>Download a model from Comet ML using the official API.</p> <p>Parameters: workspace (str): The Comet ML workspace name model_name (str): The name of the model version (str): The version of the model to download (use \"latest\" for the most recent version) output_dir (str): The local directory to save the downloaded model (default is current directory) force_download (bool): If True, download the model even if it already exists locally</p> <p>Returns: str: The path to the model file (either existing or newly downloaded), or None if download failed</p> Source code in <code>src/capfinder/download_model.py</code> <pre><code>def download_comet_model(\n    workspace: str,\n    model_name: str,\n    version: str,\n    output_dir: str = \"./\",\n    force_download: bool = False,\n) -&gt; None:\n    \"\"\"\n    Download a model from Comet ML using the official API.\n\n    Parameters:\n    workspace (str): The Comet ML workspace name\n    model_name (str): The name of the model\n    version (str): The version of the model to download (use \"latest\" for the most recent version)\n    output_dir (str): The local directory to save the downloaded model (default is current directory)\n    force_download (bool): If True, download the model even if it already exists locally\n\n    Returns:\n    str: The path to the model file (either existing or newly downloaded), or None if download failed\n    \"\"\"\n\n    os.makedirs(output_dir, exist_ok=True)\n    api = API()\n    model = api.get_model(workspace, model_name)\n    model.download(version, output_dir, expand=True)\n    orig_model_name = model._get_assets(version)[0][\"fileName\"]\n    rename_downloaded_model(output_dir, orig_model_name, f\"{model_name}.keras\")\n    create_version_info_file(output_dir, version)\n</code></pre>"},{"location":"api_docs/#src.capfinder.download_model.rename_downloaded_model","title":"<code>rename_downloaded_model(output_dir: str, orig_model_name: str, new_model_name: str) -&gt; None</code>","text":"<p>Renames the downloaded model file to a new name.</p> <p>Parameters: output_dir (str): The directory where the model file is located. orig_model_name (str): The original name of the model file. new_model_name (str): The new name to rename the model file to.</p> <p>Returns: None</p> Source code in <code>src/capfinder/download_model.py</code> <pre><code>def rename_downloaded_model(\n    output_dir: str, orig_model_name: str, new_model_name: str\n) -&gt; None:\n    \"\"\"\n    Renames the downloaded model file to a new name.\n\n    Parameters:\n    output_dir (str): The directory where the model file is located.\n    orig_model_name (str): The original name of the model file.\n    new_model_name (str): The new name to rename the model file to.\n\n    Returns:\n    None\n    \"\"\"\n    # Construct the new full path\n    orig_path = os.path.join(output_dir, orig_model_name)\n    new_path = os.path.join(output_dir, new_model_name)\n    os.rename(orig_path, new_path)\n</code></pre>"},{"location":"api_docs/#src.capfinder.encoder_model","title":"<code>encoder_model</code>","text":""},{"location":"api_docs/#src.capfinder.encoder_model.CapfinderHyperModel","title":"<code>CapfinderHyperModel</code>","text":"<p>               Bases: <code>HyperModel</code></p> <p>Custom HyperModel class to wrap the model building function for Capfinder.</p> <p>This class defines the hyperparameter search space and builds the model based on the selected hyperparameters, including a variable number of MLP layers.</p>"},{"location":"api_docs/#src.capfinder.encoder_model.CapfinderHyperModel--attributes","title":"Attributes:","text":"<p>input_shape : Tuple[int, int]     The shape of the input data. n_classes : int     The number of output classes for the classification task. encoder_model : Optional[keras.Model]     Stores the encoder part of the model, initialized during the build process.</p> Source code in <code>src/capfinder/encoder_model.py</code> <pre><code>class CapfinderHyperModel(HyperModel):\n    \"\"\"\n    Custom HyperModel class to wrap the model building function for Capfinder.\n\n    This class defines the hyperparameter search space and builds the model\n    based on the selected hyperparameters, including a variable number of MLP layers.\n\n    Attributes:\n    ----------\n    input_shape : Tuple[int, int]\n        The shape of the input data.\n    n_classes : int\n        The number of output classes for the classification task.\n    encoder_model : Optional[keras.Model]\n        Stores the encoder part of the model, initialized during the build process.\n    \"\"\"\n\n    def __init__(self, input_shape: Tuple[int, int], n_classes: int):\n        self.input_shape = input_shape\n        self.n_classes = n_classes\n        self.encoder_model: Optional[keras.Model] = None\n\n    def build(self, hp: HyperParameters) -&gt; Model:\n        \"\"\"\n        Build and compile the model based on the hyperparameters.\n\n        Parameters:\n        ----------\n        hp : HyperParameters\n            The hyperparameters to use for building the model.\n\n        Returns:\n        -------\n        Model\n            The compiled Keras model.\n        \"\"\"\n        # Hyperparameter for number of MLP layers\n        num_mlp_layers = hp.Int(\"num_mlp_layers\", min_value=1, max_value=3, step=1)\n\n        # Create a list of MLP units for each layer\n        mlp_units = [\n            hp.Int(f\"mlp_units_{i}\", min_value=32, max_value=256, step=32)\n            for i in range(num_mlp_layers)\n        ]\n\n        # Call the model builder function and obtain the full model and encoder model\n        model, encoder_model = build_model(\n            input_shape=self.input_shape,\n            head_size=hp.Int(\"head_size\", min_value=32, max_value=256, step=32),\n            num_heads=hp.Int(\"num_heads\", min_value=1, max_value=8, step=1),\n            ff_dim=hp.Int(\"ff_dim\", min_value=32, max_value=512, step=32),\n            num_transformer_blocks=hp.Int(\n                \"num_transformer_blocks\", min_value=1, max_value=8, step=1\n            ),\n            mlp_units=mlp_units,  # Now passing the list of MLP units\n            n_classes=self.n_classes,\n            mlp_dropout=hp.Float(\"mlp_dropout\", min_value=0.1, max_value=0.5, step=0.1),\n            dropout=hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1),\n        )\n\n        # Store the encoder model as an instance attribute for later access\n        self.encoder_model = encoder_model\n\n        # Define learning rate as a hyperparameter\n        learning_rate = hp.Float(\n            \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n        )\n\n        # Compile the model\n        model.compile(\n            loss=\"sparse_categorical_crossentropy\",\n            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n            metrics=[\"sparse_categorical_accuracy\"],\n        )\n\n        # Return only the full model to Keras Tuner\n        return model\n</code></pre>"},{"location":"api_docs/#src.capfinder.encoder_model.CapfinderHyperModel.build","title":"<code>build(hp: HyperParameters) -&gt; Model</code>","text":"<p>Build and compile the model based on the hyperparameters.</p>"},{"location":"api_docs/#src.capfinder.encoder_model.CapfinderHyperModel.build--parameters","title":"Parameters:","text":"<p>hp : HyperParameters     The hyperparameters to use for building the model.</p>"},{"location":"api_docs/#src.capfinder.encoder_model.CapfinderHyperModel.build--returns","title":"Returns:","text":"<p>Model     The compiled Keras model.</p> Source code in <code>src/capfinder/encoder_model.py</code> <pre><code>def build(self, hp: HyperParameters) -&gt; Model:\n    \"\"\"\n    Build and compile the model based on the hyperparameters.\n\n    Parameters:\n    ----------\n    hp : HyperParameters\n        The hyperparameters to use for building the model.\n\n    Returns:\n    -------\n    Model\n        The compiled Keras model.\n    \"\"\"\n    # Hyperparameter for number of MLP layers\n    num_mlp_layers = hp.Int(\"num_mlp_layers\", min_value=1, max_value=3, step=1)\n\n    # Create a list of MLP units for each layer\n    mlp_units = [\n        hp.Int(f\"mlp_units_{i}\", min_value=32, max_value=256, step=32)\n        for i in range(num_mlp_layers)\n    ]\n\n    # Call the model builder function and obtain the full model and encoder model\n    model, encoder_model = build_model(\n        input_shape=self.input_shape,\n        head_size=hp.Int(\"head_size\", min_value=32, max_value=256, step=32),\n        num_heads=hp.Int(\"num_heads\", min_value=1, max_value=8, step=1),\n        ff_dim=hp.Int(\"ff_dim\", min_value=32, max_value=512, step=32),\n        num_transformer_blocks=hp.Int(\n            \"num_transformer_blocks\", min_value=1, max_value=8, step=1\n        ),\n        mlp_units=mlp_units,  # Now passing the list of MLP units\n        n_classes=self.n_classes,\n        mlp_dropout=hp.Float(\"mlp_dropout\", min_value=0.1, max_value=0.5, step=0.1),\n        dropout=hp.Float(\"dropout\", min_value=0.1, max_value=0.5, step=0.1),\n    )\n\n    # Store the encoder model as an instance attribute for later access\n    self.encoder_model = encoder_model\n\n    # Define learning rate as a hyperparameter\n    learning_rate = hp.Float(\n        \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n    )\n\n    # Compile the model\n    model.compile(\n        loss=\"sparse_categorical_crossentropy\",\n        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n        metrics=[\"sparse_categorical_accuracy\"],\n    )\n\n    # Return only the full model to Keras Tuner\n    return model\n</code></pre>"},{"location":"api_docs/#src.capfinder.encoder_model.build_model","title":"<code>build_model(input_shape: Tuple[int, int], head_size: int, num_heads: int, ff_dim: int, num_transformer_blocks: int, mlp_units: List[int], n_classes: int, dropout: float = 0.0, mlp_dropout: float = 0.0) -&gt; Tuple[keras.Model, keras.Model]</code>","text":"<p>Build a transformer-based neural network model and return the encoder output.</p> <p>input_shape : Tuple[int, int]     The shape of the input data. head_size : int     The size of the attention heads in the transformer encoder. num_heads : int     The number of attention heads in the transformer encoder. ff_dim : int     The dimensionality of the feed-forward network in the transformer encoder. num_transformer_blocks : int     The number of transformer encoder blocks in the model. mlp_units : List[int]     A list containing the number of units for each layer in the MLP. n_classes : int     The number of output classes (for classification tasks). dropout : float, optional     The dropout rate applied in the transformer encoder. mlp_dropout : float, optional     The dropout rate applied in the MLP.</p> <p>Tuple[keras.Model, keras.Model]:     A tuple containing the full model and the encoder model.</p> Source code in <code>src/capfinder/encoder_model.py</code> <pre><code>def build_model(\n    input_shape: Tuple[int, int],\n    head_size: int,\n    num_heads: int,\n    ff_dim: int,\n    num_transformer_blocks: int,\n    mlp_units: List[int],\n    n_classes: int,\n    dropout: float = 0.0,\n    mlp_dropout: float = 0.0,\n) -&gt; Tuple[keras.Model, keras.Model]:\n    \"\"\"\n    Build a transformer-based neural network model and return the encoder output.\n\n    Parameters:\n    input_shape : Tuple[int, int]\n        The shape of the input data.\n    head_size : int\n        The size of the attention heads in the transformer encoder.\n    num_heads : int\n        The number of attention heads in the transformer encoder.\n    ff_dim : int\n        The dimensionality of the feed-forward network in the transformer encoder.\n    num_transformer_blocks : int\n        The number of transformer encoder blocks in the model.\n    mlp_units : List[int]\n        A list containing the number of units for each layer in the MLP.\n    n_classes : int\n        The number of output classes (for classification tasks).\n    dropout : float, optional\n        The dropout rate applied in the transformer encoder.\n    mlp_dropout : float, optional\n        The dropout rate applied in the MLP.\n\n    Returns:\n    Tuple[keras.Model, keras.Model]:\n        A tuple containing the full model and the encoder model.\n    \"\"\"\n    # Input layer\n    inputs = keras.Input(shape=input_shape)\n\n    # Apply transformer encoder blocks and save the output of the encoder\n    x = inputs\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n\n    # Apply global average pooling\n    x = layers.GlobalAveragePooling1D(data_format=\"channels_last\")(x)\n\n    # Save the encoder output\n    encoder_output = x\n\n    # Add multi-layer perceptron (MLP) layers\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"relu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n\n    # Add softmax output layer\n    outputs = layers.Dense(n_classes, activation=\"softmax\")(x)\n\n    # Construct the full model\n    model = keras.Model(inputs, outputs)\n\n    # Create a model that produces only the encoder output\n    encoder_model = keras.Model(inputs, encoder_output)\n\n    # Return the full model and the encoder model\n    return model, encoder_model\n</code></pre>"},{"location":"api_docs/#src.capfinder.encoder_model.transformer_encoder","title":"<code>transformer_encoder(inputs: keras.layers.Layer, head_size: int, num_heads: int, ff_dim: int, dropout: Optional[float] = 0.0) -&gt; keras.layers.Layer</code>","text":"<p>Create a transformer encoder block.</p> <p>The transformer encoder block consists of a multi-head attention layer followed by layer normalization and a feed-forward network.</p>"},{"location":"api_docs/#src.capfinder.encoder_model.transformer_encoder--parameters","title":"Parameters:","text":"<p>inputs : keras.layers.Layer     The input layer or tensor for the encoder block. head_size : int     The size of the attention heads. num_heads : int     The number of attention heads. ff_dim : int     The dimensionality of the feed-forward network. dropout : float, optional     The dropout rate applied after the attention layer and within the feed-forward network. Default is 0.0.</p>"},{"location":"api_docs/#src.capfinder.encoder_model.transformer_encoder--returns","title":"Returns:","text":"<p>keras.layers.Layer     The output layer of the encoder block, which can be used as input for the next layer in a neural network.</p> Source code in <code>src/capfinder/encoder_model.py</code> <pre><code>def transformer_encoder(\n    inputs: keras.layers.Layer,\n    head_size: int,\n    num_heads: int,\n    ff_dim: int,\n    dropout: Optional[float] = 0.0,\n) -&gt; keras.layers.Layer:\n    \"\"\"\n    Create a transformer encoder block.\n\n    The transformer encoder block consists of a multi-head attention layer\n    followed by layer normalization and a feed-forward network.\n\n    Parameters:\n    ----------\n    inputs : keras.layers.Layer\n        The input layer or tensor for the encoder block.\n    head_size : int\n        The size of the attention heads.\n    num_heads : int\n        The number of attention heads.\n    ff_dim : int\n        The dimensionality of the feed-forward network.\n    dropout : float, optional\n        The dropout rate applied after the attention layer and within the feed-forward network. Default is 0.0.\n\n    Returns:\n    -------\n    keras.layers.Layer\n        The output layer of the encoder block, which can be used as input for the next layer in a neural network.\n    \"\"\"\n    # Multi-head attention layer with dropout and layer normalization\n    x = layers.MultiHeadAttention(\n        key_dim=head_size, num_heads=num_heads, dropout=dropout\n    )(inputs, inputs)\n    x = layers.Dropout(dropout)(x)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n    res = x + inputs\n\n    # Feed-forward network with convolutional layers, dropout, and layer normalization\n    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    x = layers.LayerNormalization(epsilon=1e-6)(x)\n\n    # Masking for zero padding\n    mask = layers.Lambda(\n        lambda x: keras.ops.cast(keras.ops.equal(x, 0.0), dtype=\"float32\"),\n        output_shape=(inputs.shape[1], 1),  # Specify output shape\n    )(inputs)\n    x = (x + res) * mask  # Apply mask to the summed output (including residual)\n\n    return x\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test","title":"<code>find_ote_test</code>","text":"<p>The module contains the code to find OTE sequence in test data -- where we only know the context to the left of the NNNNNN region -- and its location with high-confidence. The modules can process one read at a time or all reads in a FASTQ file or folder of FASTQ files.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.find_ote_test.cnt_match_mismatch_gaps","title":"<code>cnt_match_mismatch_gaps(aln_str: str) -&gt; Tuple[int, int, int]</code>","text":"<p>Takes an alignment string and counts the number of matches, mismatches, and gaps.</p> <p>Parameters:</p> Name Type Description Default <code>aln_str</code> <code>str</code> <p>The alignment string.</p> required <p>Returns:</p> Name Type Description <code>match_cnt</code> <code>int</code> <p>The number of matches in the alignment string.</p> <code>mismatch_cnt</code> <code>int</code> <p>The number of mismatches in the alignment string.</p> <code>gap_cnt</code> <code>int</code> <p>The number of gaps in the alignment string.</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def cnt_match_mismatch_gaps(aln_str: str) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Takes an alignment string and counts the number of matches, mismatches, and gaps.\n\n    Args:\n        aln_str (str): The alignment string.\n\n    Returns:\n        match_cnt (int): The number of matches in the alignment string.\n        mismatch_cnt (int): The number of mismatches in the alignment string.\n        gap_cnt (int): The number of gaps in the alignment string.\n    \"\"\"\n    match_cnt = 0\n    mismatch_cnt = 0\n    gap_cnt = 0\n    for aln_chr in aln_str:\n        if aln_chr == \"|\":\n            match_cnt += 1\n        elif aln_chr == \"/\":\n            mismatch_cnt += 1\n        elif aln_chr == \" \":\n            gap_cnt += 1\n    return match_cnt, mismatch_cnt, gap_cnt\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.dispatcher","title":"<code>dispatcher(input_path: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Check if the input path is a file or folder, and call the appropriate function to process the input.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The path to the FASTQ file or folder.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def dispatcher(\n    input_path: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Check if the input path is a file or folder, and call the appropriate function to process the input.\n\n    Args:\n        input_path (str): The path to the FASTQ file or folder.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    if os.path.isfile(input_path):\n        process_fastq_file(\n            input_path, reference, cap0_pos, num_processes, output_folder\n        )\n    elif os.path.isdir(input_path):\n        process_fastq_folder(\n            input_path, reference, cap0_pos, num_processes, output_folder\n        )\n    else:\n        raise ValueError(\"Error! Invalid path type. Path must be a file or folder.\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.find_ote_test","title":"<code>find_ote_test(input_path: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Main function to process a FASTQ file or folder of FASTQ files to find OTEs in the reads. The function is suitable only for testing data where only the OTE sequence is known and the N1N2 cap bases and any bases 3' of them are unknown.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The path to the FASTQ file or folder.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:     None</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def find_ote_test(\n    input_path: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Main function to process a FASTQ file or folder of FASTQ files to find OTEs\n    in the reads. The function is suitable only for testing data where only the OTE\n    sequence is known and the N1N2 cap bases and any bases 3' of them are unknown.\n\n    Args:\n        input_path (str): The path to the FASTQ file or folder.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n    Returns:\n        None\n    \"\"\"\n    dispatcher(input_path, reference, cap0_pos, num_processes, output_folder)\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.has_good_aln_in_5prime_flanking_region","title":"<code>has_good_aln_in_5prime_flanking_region(match_cnt: int, mismatch_cnt: int, gap_cnt: int) -&gt; bool</code>","text":"<p>Checks if the alignment in the flanking region before the cap is good.</p> <p>Parameters:</p> Name Type Description Default <code>match_cnt</code> <code>int</code> <p>The number of matches in the flanking region.</p> required <code>mismatch_cnt</code> <code>int</code> <p>The number of mismatches in the flanking region.</p> required <code>gap_cnt</code> <code>int</code> <p>The number of gaps in the flanking region.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the alignment in the flanking region is good, False otherwise.</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def has_good_aln_in_5prime_flanking_region(\n    match_cnt: int, mismatch_cnt: int, gap_cnt: int\n) -&gt; bool:\n    \"\"\"\n    Checks if the alignment in the flanking region before the cap is good.\n\n    Args:\n        match_cnt (int): The number of matches in the flanking region.\n        mismatch_cnt (int): The number of mismatches in the flanking region.\n        gap_cnt (int): The number of gaps in the flanking region.\n\n    Returns:\n        bool: True if the alignment in the flanking region is good, False otherwise.\n    \"\"\"\n    if (mismatch_cnt &gt; match_cnt) or (gap_cnt &gt; match_cnt):\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.has_good_aln_in_n_region","title":"<code>has_good_aln_in_n_region(match_cnt: int, mismatch_cnt: int, gap_cnt: int) -&gt; bool</code>","text":"<p>Checks if the alignment in the NNNNNN region is good.</p> <p>Parameters:</p> Name Type Description Default <code>match_cnt</code> <code>int</code> <p>The number of matches in the NNNNNN region.</p> required <code>mismatch_cnt</code> <code>int</code> <p>The number of mismatches in the NNNNNN region.</p> required <code>gap_cnt</code> <code>int</code> <p>The number of gaps in the NNNNNN region.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the alignment in the NNNNNN region is good, False otherwise.</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def has_good_aln_in_n_region(match_cnt: int, mismatch_cnt: int, gap_cnt: int) -&gt; bool:\n    \"\"\"\n    Checks if the alignment in the NNNNNN region is good.\n\n    Args:\n        match_cnt (int): The number of matches in the NNNNNN region.\n        mismatch_cnt (int): The number of mismatches in the NNNNNN region.\n        gap_cnt (int): The number of gaps in the NNNNNN region.\n\n    Returns:\n        bool: True if the alignment in the NNNNNN region is good, False otherwise.\n    \"\"\"\n    # For a good alignment in NNNNNN region, the number of mismatches should be\n    # greater than the number of gaps\n    if mismatch_cnt &gt;= gap_cnt:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.make_coordinates","title":"<code>make_coordinates(aln_str: str, ref_str: str) -&gt; List[int]</code>","text":"<p>Walk along the alignment string and make an incrementing index where there is a match, mismatch, and deletions. For gaps in the alignment string, it output a -1 in the index list.</p> <p>Parameters:</p> Name Type Description Default <code>aln_str</code> <code>str</code> <p>The alignment string.</p> required <code>ref_str</code> <code>str</code> <p>The reference string.</p> required <p>Returns:</p> Name Type Description <code>coord_list</code> <code>list</code> <p>A list of indices corresponding to the alignment string.</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def make_coordinates(aln_str: str, ref_str: str) -&gt; List[int]:\n    \"\"\"\n    Walk along the alignment string and make an incrementing index\n    where there is a match, mismatch, and deletions. For gaps in\n    the alignment string, it output a -1 in the index list.\n\n    Args:\n        aln_str (str): The alignment string.\n        ref_str (str): The reference string.\n\n    Returns:\n        coord_list (list): A list of indices corresponding to the alignment string.\n    \"\"\"\n    # Make index coordinates along the alignment string\n    coord_list = []\n    cnt = 0\n    for idx, aln_chr in enumerate(aln_str):\n        if aln_chr != \" \":\n            coord_list.append(cnt)\n            cnt += 1\n        else:\n            if ref_str[idx] != \"-\":  # handle deletions\n                coord_list.append(cnt)\n                cnt += 1\n            else:\n                coord_list.append(-1)\n\n    # Go in reverse in the coord_list, and put -1 of all the places\n    # where there is a gap in the alignment string. Break out when the\n    # first non-gap character is encountered.\n    for idx in range(len(coord_list) - 1, -1, -1):\n        if aln_str[idx] == \" \":\n            coord_list[idx] = -1\n        else:\n            break\n    return coord_list\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.process_fastq_file","title":"<code>process_fastq_file(fastq_filepath: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process a single FASTQ file. The function reads the FASTQ file, and processes each read in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>fastq_filepath</code> <code>str</code> <p>The path to the FASTQ file.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def process_fastq_file(\n    fastq_filepath: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Process a single FASTQ file. The function reads the FASTQ file, and processes each read in parallel.\n\n    Args:\n        fastq_filepath (str): The path to the FASTQ file.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Make output file name\n    directory, filename = os.path.split(fastq_filepath)\n    filename_no_extension, extension = os.path.splitext(filename)\n    os.path.join(output_folder, f\"{filename_no_extension}.txt\")\n\n    with file_opener(fastq_filepath) as fastq_file:\n        records = list(SeqIO.parse(fastq_file, \"fastq\"))\n        total_records = len(records)\n\n        with WorkerPool(n_jobs=num_processes) as pool:\n            results = pool.map(\n                process_read,\n                [(record, reference, cap0_pos) for record in records],\n                iterable_len=total_records,\n                progress_bar=True,\n            )\n            write_csv(\n                results,\n                output_filepath=os.path.join(\n                    output_folder,\n                    filename_no_extension + \"_test_ote_search_results.csv\",\n                ),\n            )\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.process_fastq_folder","title":"<code>process_fastq_folder(folder_path: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process all FASTQ files in a folder. The function reads all FASTQ files in a folder, and feeds one FASTQ at a time which to a prcessing function that processes reads in this FASTQ file in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The path to the folder containing FASTQ files.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>cap0_pos</code> <code>int</code> <p>The position of the first cap base (N1) in the reference sequence (0-indexed).</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def process_fastq_folder(\n    folder_path: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Process all FASTQ files in a folder. The function reads all FASTQ files in a folder,\n    and feeds one FASTQ at a time which to a prcessing function that processes reads in this\n    FASTQ file in parallel.\n\n    Args:\n        folder_path (str): The path to the folder containing FASTQ files.\n        reference (str): The reference sequence to align the read to.\n        cap0_pos (int): The position of the first cap base (N1) in the reference sequence (0-indexed).\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    # List all files in the folder\n    for root, _, files in os.walk(folder_path):\n        for file_name in files:\n            if file_name.endswith((\".fastq\", \".fastq.gz\")):\n                file_path = os.path.join(root, file_name)\n                process_fastq_file(\n                    file_path, reference, cap0_pos, num_processes, output_folder\n                )\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.process_read","title":"<code>process_read(record: Any, reference: str, cap0_pos: int) -&gt; Dict[str, Any]</code>","text":"<p>Process a single read from a FASTQ file. The function alnigns the read to the reference, and checks if the alignment in the NNNNNN region and the flanking regions is good. If the alignment is good, then the function returns the read ID, alignment score, and the positions of the left flanking region, cap0 base, and the right flanking region in the read's FASTQ sequence. If the alignment is bad, then the function returns the read ID, alignment score, and the reason why the alignment is bad.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SeqRecord</code> <p>A single read from a FASTQ file.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>cap0_pos</code> <code>int</code> <p>The position of the first cap base in the reference sequence (0-indexed).</p> required <p>Returns:</p> Name Type Description <code>out_ds</code> <code>dict</code> <p>A dictionary containing the following keys: read_id (str): The identifier of the sequence read. read_type (str): The type of the read, which can be 'good' or 'bad' reason (str or None): The reason for the failed alignment, if available. alignment_score (float): The alignment score for the read. left_flanking_region_start_fastq_pos (int or None): The starting position of the left flanking region in the FASTQ file, if available. cap_n1_minus_1_read_fastq_pos (int or None): The position of the caps N1 base in the FASTQ file (0-indexed), if available. right_flanking_region_start_fastq_pos (int or None): The starting position of the right flanking region in the FASTQ file, if available.</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def process_read(record: Any, reference: str, cap0_pos: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a single read from a FASTQ file. The function alnigns the read to the reference,\n    and checks if the alignment in the NNNNNN region and the flanking regions is good. If the\n    alignment is good, then the function returns the read ID, alignment score, and the\n    positions of the left flanking region, cap0 base, and the right flanking region in the\n    read's FASTQ sequence. If the alignment is bad, then the function returns the read ID,\n    alignment score, and the reason why the alignment is bad.\n\n    Args:\n        record (SeqRecord): A single read from a FASTQ file.\n        reference (str): The reference sequence to align the read to.\n        cap0_pos (int): The position of the first cap base in the reference sequence (0-indexed).\n\n    Returns:\n        out_ds (dict): A dictionary containing the following keys:\n            read_id (str): The identifier of the sequence read.\n            read_type (str): The type of the read, which can be 'good' or 'bad'\n            reason (str or None): The reason for the failed alignment, if available.\n            alignment_score (float): The alignment score for the read.\n            left_flanking_region_start_fastq_pos (int or None): The starting position of the left flanking region\n            in the FASTQ file, if available.\n            cap_n1_minus_1_read_fastq_pos (int or None): The position of the caps N1 base in the FASTQ file (0-indexed), if available.\n            right_flanking_region_start_fastq_pos (int or None): The starting position of the right flanking region\n            in the FASTQ file, if available.\n    \"\"\"\n    # Get alignment\n    sequence = str(record.seq)\n    fasta_length = len(sequence)\n\n    with contextlib.redirect_stdout(None):\n        qry_str, aln_str, ref_str, aln_score = align(\n            query_seq=sequence, target_seq=reference, pretty_print_alns=False\n        )\n\n    # define a data structure to return when the read OTE is not found\n    out_ds_failed = {\n        \"read_id\": record.id,\n        \"read_type\": \"bad\",\n        \"reason\": None,\n        \"alignment_score\": aln_score,\n        \"left_flanking_region_start_fastq_pos\": None,\n        \"cap_n1_minus_1_read_fastq_pos\": None,\n        \"right_flanking_region_start_fastq_pos\": None,\n        \"roi_fasta\": None,\n        \"fasta_length\": fasta_length,\n    }\n\n    # For low quality alignments, return None\n    if aln_score &lt; 20:\n        out_ds_failed[\"reason\"] = \"low_aln_score\"\n        return out_ds_failed\n\n    # Make index coordinates along the reference\n    coord_list = make_coordinates(aln_str, ref_str)\n\n    # Check if the the first base 5 prime of the cap N1 base is in\n    # the coordinates list. If not then the alignment did not\n    # even reach the cap, so it is a bad read then.\n    try:\n        cap_n1_minus_1_idx = coord_list.index(cap0_pos - 1)\n    except Exception:\n        out_ds_failed[\"reason\"] = \"aln_does_not_reach_the_cap_base\"\n        return out_ds_failed\n\n    # 2. Define regions in which to check for good alignment\n    before_nnn_region = (\n        cap_n1_minus_1_idx - BEFORE_N_REGION_WINDOW_LEN,\n        cap_n1_minus_1_idx + 1,\n    )\n\n    # 3. Extract alignment strings for each region\n    aln_str_before_nnn_region = aln_str[before_nnn_region[0] : before_nnn_region[1]]\n\n    # 4. Count matches, mismatches, and gaps in each region\n    bn_match_cnt, bn_mismatch_cnt, bn_gap_cnt = cnt_match_mismatch_gaps(\n        aln_str_before_nnn_region\n    )\n\n    # 5. Is there a good alignment region flanking 5' of the cap?\n    has_good_aln_before_n_region = has_good_aln_in_5prime_flanking_region(\n        bn_match_cnt, bn_mismatch_cnt, bn_gap_cnt\n    )\n    if not (has_good_aln_before_n_region):\n        out_ds_failed[\"reason\"] = \"bad_alignment_before_the_cap\"\n        return out_ds_failed\n\n    # Find the position of cap N1-1 base in read's sequence (0-based indexing)\n    cap_n1_minus_1_read_fastq_pos = (\n        qry_str[:cap_n1_minus_1_idx].replace(\"-\", \"\").count(\"\")\n    )\n\n    # To reach the 5' end of the left flanking region, we need to find\n    # to over the alignment string on count the matches and mismatches\n    # but not the gaps\n    idx = cap_n1_minus_1_idx\n    cnt = 0\n    while True:\n        if cnt == NUM_CAP_FLANKING_BASES:\n            break\n        else:\n            if aln_str[idx] != \" \":\n                cnt += 1\n                left_flanking_region_start_idx = idx\n        idx -= 1\n\n    left_flanking_region_start_fastq_pos = (\n        qry_str[:left_flanking_region_start_idx].replace(\"-\", \"\").count(\"\") - 1\n    )\n    right_flanking_region_start_fastq_pos = (\n        cap_n1_minus_1_read_fastq_pos + NUM_CAP_FLANKING_BASES + 1\n    )\n\n    roi_fasta = sequence[\n        left_flanking_region_start_fastq_pos:right_flanking_region_start_fastq_pos\n    ]\n\n    out_ds_passed = {\n        \"read_id\": record.id,\n        \"read_type\": \"good\",\n        \"reason\": \"good_alignment_in_cap-flanking_regions\",\n        \"alignment_score\": aln_score,\n        \"left_flanking_region_start_fastq_pos\": left_flanking_region_start_fastq_pos,\n        \"cap_n1_minus_1_read_fastq_pos\": cap_n1_minus_1_read_fastq_pos,\n        \"right_flanking_region_start_fastq_pos\": right_flanking_region_start_fastq_pos,\n        \"roi_fasta\": roi_fasta,\n        \"fasta_length\": fasta_length,\n    }\n\n    # A fix to avoid outputting blank ROI when the read is shorter than\n    # computed ROI coordinates\n    sl = len(sequence)\n    if (left_flanking_region_start_fastq_pos) &gt; sl or (\n        right_flanking_region_start_fastq_pos\n    ) &gt; sl:\n        output = out_ds_failed\n    else:\n        output = out_ds_passed\n\n    return output\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_test.write_csv","title":"<code>write_csv(resutls_list: List[dict], output_filepath: str) -&gt; None</code>","text":"<p>Take a list of dictionaries and write them to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>resutls_list</code> <code>list</code> <p>A list of dictionaries.</p> required <code>output_filepath</code> <code>str</code> <p>The path to the output CSV file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_test.py</code> <pre><code>def write_csv(resutls_list: List[dict], output_filepath: str) -&gt; None:\n    \"\"\"\n    Take a list of dictionaries and write them to a CSV file.\n\n    Args:\n        resutls_list (list): A list of dictionaries.\n        output_filepath (str): The path to the output CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # Specify the CSV column headers based on the dictionary keys\n    fieldnames = resutls_list[0].keys()\n\n    # Create and write to the CSV file\n    with open(output_filepath, \"w\", newline=\"\") as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n        # Write the header row\n        writer.writeheader()\n\n        # Write the data rows\n        writer.writerows(resutls_list)\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train","title":"<code>find_ote_train</code>","text":"<p>The module contains the code to find OTE sequence in training data -- where we know both the left and right context to the NNNNNN region -- and its location with high-confidence. The modules can process one read at a time or all reads in a FASTQ file or folder of FASTQ files.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.find_ote_train.cnt_match_mismatch_gaps","title":"<code>cnt_match_mismatch_gaps(aln_str: str) -&gt; Tuple[int, int, int]</code>","text":"<p>Takes an alignment string and counts the number of matches, mismatches, and gaps.</p> <p>Parameters:</p> Name Type Description Default <code>aln_str</code> <code>str</code> <p>The alignment string.</p> required <p>Returns:</p> Name Type Description <code>match_cnt</code> <code>int</code> <p>The number of matches in the alignment string.</p> <code>mismatch_cnt</code> <code>int</code> <p>The number of mismatches in the alignment string.</p> <code>gap_cnt</code> <code>int</code> <p>The number of gaps in the alignment string.</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def cnt_match_mismatch_gaps(aln_str: str) -&gt; Tuple[int, int, int]:\n    \"\"\"\n    Takes an alignment string and counts the number of matches, mismatches, and gaps.\n\n    Args:\n        aln_str (str): The alignment string.\n\n    Returns:\n        match_cnt (int): The number of matches in the alignment string.\n        mismatch_cnt (int): The number of mismatches in the alignment string.\n        gap_cnt (int): The number of gaps in the alignment string.\n    \"\"\"\n    match_cnt = 0\n    mismatch_cnt = 0\n    gap_cnt = 0\n    for aln_chr in aln_str:\n        if aln_chr == \"|\":\n            match_cnt += 1\n        elif aln_chr == \"/\":\n            mismatch_cnt += 1\n        elif aln_chr == \" \":\n            gap_cnt += 1\n    return match_cnt, mismatch_cnt, gap_cnt\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.dispatcher","title":"<code>dispatcher(input_path: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Check if the input path is a file or folder, and call the appropriate function to process the input.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The path to the FASTQ file or folder.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def dispatcher(\n    input_path: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Check if the input path is a file or folder, and call the appropriate function to process the input.\n\n    Args:\n        input_path (str): The path to the FASTQ file or folder.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    if os.path.isfile(input_path):\n        process_fastq_file(\n            input_path, reference, cap0_pos, num_processes, output_folder\n        )\n    elif os.path.isdir(input_path):\n        process_fastq_folder(\n            input_path, reference, cap0_pos, num_processes, output_folder\n        )\n    else:\n        raise ValueError(\"Error! Invalid path type. Path must be a file or folder.\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.find_ote_train","title":"<code>find_ote_train(input_path: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Main function to process a FASTQ file or folder of FASTQ files ot find OTEs in the reads.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>The path to the FASTQ file or folder.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:     None</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def find_ote_train(\n    input_path: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Main function to process a FASTQ file or folder of FASTQ files ot find OTEs\n    in the reads.\n\n    Args:\n        input_path (str): The path to the FASTQ file or folder.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n    Returns:\n        None\n    \"\"\"\n    dispatcher(input_path, reference, cap0_pos, num_processes, output_folder)\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.has_good_aln_in_n_region","title":"<code>has_good_aln_in_n_region(match_cnt: int, mismatch_cnt: int, gap_cnt: int) -&gt; bool</code>","text":"<p>Checks if the alignment in the NNNNNN region is good.</p> <p>Parameters:</p> Name Type Description Default <code>match_cnt</code> <code>int</code> <p>The number of matches in the NNNNNN region.</p> required <code>mismatch_cnt</code> <code>int</code> <p>The number of mismatches in the NNNNNN region.</p> required <code>gap_cnt</code> <code>int</code> <p>The number of gaps in the NNNNNN region.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the alignment in the NNNNNN region is good, False otherwise.</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def has_good_aln_in_n_region(match_cnt: int, mismatch_cnt: int, gap_cnt: int) -&gt; bool:\n    \"\"\"\n    Checks if the alignment in the NNNNNN region is good.\n\n    Args:\n        match_cnt (int): The number of matches in the NNNNNN region.\n        mismatch_cnt (int): The number of mismatches in the NNNNNN region.\n        gap_cnt (int): The number of gaps in the NNNNNN region.\n\n    Returns:\n        bool: True if the alignment in the NNNNNN region is good, False otherwise.\n    \"\"\"\n    # For a good alignment in NNNNNN region, the number of mismatches should be\n    # greater than the number of gaps\n    if mismatch_cnt &gt;= gap_cnt:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.has_good_aln_ns_flanking_region","title":"<code>has_good_aln_ns_flanking_region(match_cnt: int, mismatch_cnt: int, gap_cnt: int) -&gt; bool</code>","text":"<p>Checks if the alignment in the flanking region before or after the NNNNNN region is good.</p> <p>Parameters:</p> Name Type Description Default <code>match_cnt</code> <code>int</code> <p>The number of matches in the flanking region.</p> required <code>mismatch_cnt</code> <code>int</code> <p>The number of mismatches in the flanking region.</p> required <code>gap_cnt</code> <code>int</code> <p>The number of gaps in the flanking region.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the alignment in the flanking region is good, False otherwise.</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def has_good_aln_ns_flanking_region(\n    match_cnt: int, mismatch_cnt: int, gap_cnt: int\n) -&gt; bool:\n    \"\"\"\n    Checks if the alignment in the flanking region before or after the NNNNNN region is good.\n\n    Args:\n        match_cnt (int): The number of matches in the flanking region.\n        mismatch_cnt (int): The number of mismatches in the flanking region.\n        gap_cnt (int): The number of gaps in the flanking region.\n\n    Returns:\n        bool: True if the alignment in the flanking region is good, False otherwise.\n    \"\"\"\n    if (mismatch_cnt &gt; match_cnt) or (gap_cnt &gt; match_cnt):\n        return False\n    else:\n        return True\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.make_coordinates","title":"<code>make_coordinates(aln_str: str, ref_str: str) -&gt; List[int]</code>","text":"<p>Walk along the alignment string and make an incrementing index where there is a match, mismatch, and deletions. For gaps in the alignment string, it output a -1 in the index list.</p> <p>Parameters:</p> Name Type Description Default <code>aln_str</code> <code>str</code> <p>The alignment string.</p> required <code>ref_str</code> <code>str</code> <p>The reference string.</p> required <p>Returns:</p> Name Type Description <code>coord_list</code> <code>list</code> <p>A list of indices corresponding to the alignment string.</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def make_coordinates(aln_str: str, ref_str: str) -&gt; List[int]:\n    \"\"\"\n    Walk along the alignment string and make an incrementing index\n    where there is a match, mismatch, and deletions. For gaps in\n    the alignment string, it output a -1 in the index list.\n\n    Args:\n        aln_str (str): The alignment string.\n        ref_str (str): The reference string.\n\n    Returns:\n        coord_list (list): A list of indices corresponding to the alignment string.\n    \"\"\"\n    # Make index coordinates along the alignment string\n    coord_list = []\n    cnt = 0\n    for idx, aln_chr in enumerate(aln_str):\n        if aln_chr != \" \":\n            coord_list.append(cnt)\n            cnt += 1\n        else:\n            if ref_str[idx] != \"-\":  # handle deletions\n                coord_list.append(cnt)\n                cnt += 1\n            else:\n                coord_list.append(-1)\n\n    # Go in reverse in the coord_list, and put -1 of all the places\n    # where there is a gap in the alignment string. Break out when the\n    # first non-gap character is encountered.\n    for idx in range(len(coord_list) - 1, -1, -1):\n        if aln_str[idx] == \" \":\n            coord_list[idx] = -1\n        else:\n            break\n    return coord_list\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.process_fastq_file","title":"<code>process_fastq_file(fastq_filepath: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process a single FASTQ file. The function reads the FASTQ file, and processes each read in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>fastq_filepath</code> <code>str</code> <p>The path to the FASTQ file.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def process_fastq_file(\n    fastq_filepath: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Process a single FASTQ file. The function reads the FASTQ file, and processes each read in parallel.\n\n    Args:\n        fastq_filepath (str): The path to the FASTQ file.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Make output file name\n    directory, filename = os.path.split(fastq_filepath)\n    filename_no_extension, extension = os.path.splitext(filename)\n    os.path.join(output_folder, f\"{filename_no_extension}.txt\")\n\n    with file_opener(fastq_filepath) as fastq_file:\n        records = list(SeqIO.parse(fastq_file, \"fastq\"))\n        total_records = len(records)\n\n        with WorkerPool(n_jobs=num_processes) as pool:\n            results = pool.map(\n                process_read,\n                [(record, reference, cap0_pos) for record in records],\n                iterable_len=total_records,\n                progress_bar=True,\n            )\n            write_csv(\n                results,\n                output_filepath=os.path.join(\n                    output_folder,\n                    filename_no_extension + \"_train_ote_search_results.csv\",\n                ),\n            )\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.process_fastq_folder","title":"<code>process_fastq_folder(folder_path: str, reference: str, cap0_pos: int, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process all FASTQ files in a folder. The function reads all FASTQ files in a folder, and feeds one FASTQ at a time which to a prcessing function that processes reads in this FASTQ file in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The path to the folder containing FASTQ files.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>cap0_pos</code> <code>int</code> <p>The position of the first cap base (N1) in the reference sequence (0-indexed).</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where worker output files will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def process_fastq_folder(\n    folder_path: str,\n    reference: str,\n    cap0_pos: int,\n    num_processes: int,\n    output_folder: str,\n) -&gt; None:\n    \"\"\"\n    Process all FASTQ files in a folder. The function reads all FASTQ files in a folder,\n    and feeds one FASTQ at a time which to a prcessing function that processes reads in this\n    FASTQ file in parallel.\n\n    Args:\n        folder_path (str): The path to the folder containing FASTQ files.\n        reference (str): The reference sequence to align the read to.\n        cap0_pos (int): The position of the first cap base (N1) in the reference sequence (0-indexed).\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where worker output files will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    # List all files in the folder\n    for root, _, files in os.walk(folder_path):\n        for file_name in files:\n            if file_name.endswith((\".fastq\", \".fastq.gz\")):\n                file_path = os.path.join(root, file_name)\n                process_fastq_file(\n                    file_path, reference, cap0_pos, num_processes, output_folder\n                )\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.process_read","title":"<code>process_read(record: Any, reference: str, cap0_pos: int) -&gt; Dict[str, Any]</code>","text":"<p>Process a single read from a FASTQ file. The function alnigns the read to the reference, and checks if the alignment in the NNNNNN region and the flanking regions is good. If the alignment is good, then the function returns the read ID, alignment score, and the positions of the left flanking region, cap0 base, and the right flanking region in the read's FASTQ sequence. If the alignment is bad, then the function returns the read ID, alignment score, and the reason why the alignment is bad.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>SeqRecord</code> <p>A single read from a FASTQ file.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>cap0_pos</code> <code>int</code> <p>The position of the first cap base in the reference sequence (0-indexed).</p> required <p>Returns:</p> Name Type Description <code>out_ds</code> <code>dict</code> <p>A dictionary containing the following keys: read_id (str): The identifier of the sequence read. read_type (str): The type of the read, which can be 'good' or 'bad' reason (str or None): The reason for the failed alignment, if available. alignment_score (float): The alignment score for the read. left_flanking_region_start_fastq_pos (int or None): The starting position of the left flanking region in the FASTQ file, if available. cap0_read_fastq_pos (int or None): The position of the caps N1 base in the FASTQ file (0-indexed), if available. right_flanking_region_start_fastq_pos (int or None): The starting position of the right flanking region in the FASTQ file, if available.</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def process_read(record: Any, reference: str, cap0_pos: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a single read from a FASTQ file. The function alnigns the read to the reference,\n    and checks if the alignment in the NNNNNN region and the flanking regions is good. If the\n    alignment is good, then the function returns the read ID, alignment score, and the\n    positions of the left flanking region, cap0 base, and the right flanking region in the\n    read's FASTQ sequence. If the alignment is bad, then the function returns the read ID,\n    alignment score, and the reason why the alignment is bad.\n\n    Args:\n        record (SeqRecord): A single read from a FASTQ file.\n        reference (str): The reference sequence to align the read to.\n        cap0_pos (int): The position of the first cap base in the reference sequence (0-indexed).\n\n    Returns:\n        out_ds (dict): A dictionary containing the following keys:\n            read_id (str): The identifier of the sequence read.\n            read_type (str): The type of the read, which can be 'good' or 'bad'\n            reason (str or None): The reason for the failed alignment, if available.\n            alignment_score (float): The alignment score for the read.\n            left_flanking_region_start_fastq_pos (int or None): The starting position of the left flanking region\n            in the FASTQ file, if available.\n            cap0_read_fastq_pos (int or None): The position of the caps N1 base in the FASTQ file (0-indexed), if available.\n            right_flanking_region_start_fastq_pos (int or None): The starting position of the right flanking region\n            in the FASTQ file, if available.\n    \"\"\"\n    # Get alignment\n    sequence = str(record.seq)\n    with contextlib.redirect_stdout(None):\n        qry_str, aln_str, ref_str, aln_score = align(\n            query_seq=sequence, target_seq=reference, pretty_print_alns=False\n        )\n\n    # define a data structure to return when the read OTE is not found\n    out_ds_failed = {\n        \"read_id\": record.id,\n        \"read_type\": \"bad\",\n        \"reason\": None,\n        \"alignment_score\": aln_score,\n        \"left_flanking_region_start_fastq_pos\": None,\n        \"cap0_read_fastq_pos\": None,\n        \"right_flanking_region_start_fastq_pos\": None,\n        \"roi_fasta\": None,\n    }\n\n    # For low quality alignments, return None\n    if aln_score &lt; 20:\n        out_ds_failed[\"reason\"] = \"low_aln_score\"\n        return out_ds_failed\n\n    # Make index coordinates along the reference\n    coord_list = make_coordinates(aln_str, ref_str)\n\n    # Check if the first cap base is in the coordinates list. If not then\n    # the alignment did not even reach the cap, so it is a bad read then.\n    try:\n        cap0_idx = coord_list.index(\n            cap0_pos\n        )  # cap0 position in the reference with gaps\n    except Exception:\n        out_ds_failed[\"reason\"] = \"aln_does_not_reach_the_cap_base\"\n        return out_ds_failed\n\n    # Check if the NNNNNN region in the reference has matches in it\n    # 1. First find the end index of the NNNNNN region\n    try:\n        n_region_end_idx = coord_list.index(cap0_pos + N_REGION_LEN - 1)\n    except Exception:\n        out_ds_failed[\"reason\"] = \"aln_does_not_reach_nnnnnn_region\"\n        return out_ds_failed\n\n    # 2. Define regions in which to check for good alignment\n    nnn_region = (cap0_idx, n_region_end_idx + 1)\n    before_nnn_region = (cap0_idx - BEFORE_N_REGION_WINDOW_LEN, cap0_idx)\n    after_nnn_region = (\n        n_region_end_idx + 1,\n        n_region_end_idx + 1 + AFTER_N_REGION_WINDOW_LEN,\n    )\n\n    # 3. Extract alignment strings for each region\n    aln_str_nnn_region = aln_str[nnn_region[0] : nnn_region[1]]\n    aln_str_before_nnn_region = aln_str[before_nnn_region[0] : before_nnn_region[1]]\n    aln_str_after_nnn_region = aln_str[after_nnn_region[0] : after_nnn_region[1]]\n\n    # 4. Count matches, mismatches, and gaps in each region\n    n_match_cnt, n_mismatch_cnt, n_gap_cnt = cnt_match_mismatch_gaps(aln_str_nnn_region)\n    bn_match_cnt, bn_mismatch_cnt, bn_gap_cnt = cnt_match_mismatch_gaps(\n        aln_str_before_nnn_region\n    )\n    an_match_cnt, an_mismatch_cnt, an_gap_cnt = cnt_match_mismatch_gaps(\n        aln_str_after_nnn_region\n    )\n\n    # 5. Are there good alignments in the the NNN region and the regions flanking it?\n    has_good_aln_in_nnn_region = has_good_aln_in_n_region(\n        n_match_cnt, n_mismatch_cnt, n_gap_cnt\n    )\n    has_good_aln_before_n_region = has_good_aln_ns_flanking_region(\n        bn_match_cnt, bn_mismatch_cnt, bn_gap_cnt\n    )\n    has_good_aln_after_n_region = has_good_aln_ns_flanking_region(\n        an_match_cnt, an_mismatch_cnt, an_gap_cnt\n    )\n\n    # 6. If all three alignment are good then a read has good and reliable OTE\n    if not (\n        has_good_aln_before_n_region\n        and has_good_aln_in_nnn_region\n        and has_good_aln_after_n_region\n    ):\n        out_ds_failed[\"reason\"] = \"111\"  # 111 means all three regions are good\n        if not (has_good_aln_before_n_region):\n            reason_list = list(out_ds_failed[\"reason\"])\n            reason_list[0] = \"0\"\n            out_ds_failed[\"reason\"] = \"\".join(reason_list)\n        if not (has_good_aln_in_nnn_region):\n            reason_list = list(out_ds_failed[\"reason\"])\n            reason_list[1] = \"0\"\n            out_ds_failed[\"reason\"] = \"\".join(reason_list)\n        if not (has_good_aln_after_n_region):\n            reason_list = list(out_ds_failed[\"reason\"])\n            reason_list[2] = \"0\"\n            out_ds_failed[\"reason\"] = \"\".join(reason_list)\n        return out_ds_failed\n\n    # Find the position of cap N1 base in read's sequence (0-based indexing)\n    cap0_read_fastq_pos = qry_str[:cap0_idx].replace(\"-\", \"\").count(\"\") - 1\n\n    # Find the index of first base of the left flanking region\n    left_flanking_region_start_ref_idx = cap0_idx - NUM_CAP_FLANKING_BASES\n    left_flanking_region_start_fastq_pos = (\n        qry_str[:left_flanking_region_start_ref_idx].replace(\"-\", \"\").count(\"\") - 1\n    )\n    right_flanking_region_end_ref_idx = cap0_idx + 1 + NUM_CAP_FLANKING_BASES\n    right_flanking_region_start_fastq_pos = (\n        qry_str[:right_flanking_region_end_ref_idx].replace(\"-\", \"\").count(\"\") - 1\n    )\n    roi_fasta = sequence[\n        left_flanking_region_start_fastq_pos:right_flanking_region_start_fastq_pos\n    ]\n\n    out_ds_passed = {\n        \"read_id\": record.id,\n        \"read_type\": \"good\",\n        \"reason\": \"111\",\n        \"alignment_score\": aln_score,\n        \"left_flanking_region_start_fastq_pos\": left_flanking_region_start_fastq_pos,\n        \"cap0_read_fastq_pos\": cap0_read_fastq_pos,\n        \"right_flanking_region_start_fastq_pos\": right_flanking_region_start_fastq_pos,\n        \"roi_fasta\": roi_fasta,\n    }\n\n    return out_ds_passed\n</code></pre>"},{"location":"api_docs/#src.capfinder.find_ote_train.write_csv","title":"<code>write_csv(resutls_list: List[dict], output_filepath: str) -&gt; None</code>","text":"<p>Take a list of dictionaries and write them to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>resutls_list</code> <code>list</code> <p>A list of dictionaries.</p> required <code>output_filepath</code> <code>str</code> <p>The path to the output CSV file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/find_ote_train.py</code> <pre><code>def write_csv(resutls_list: List[dict], output_filepath: str) -&gt; None:\n    \"\"\"\n    Take a list of dictionaries and write them to a CSV file.\n\n    Args:\n        resutls_list (list): A list of dictionaries.\n        output_filepath (str): The path to the output CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # Specify the CSV column headers based on the dictionary keys\n    fieldnames = resutls_list[0].keys()\n\n    # Create and write to the CSV file\n    with open(output_filepath, \"w\", newline=\"\") as csv_file:\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n\n        # Write the header row\n        writer.writeheader()\n\n        # Write the data rows\n        writer.writerows(resutls_list)\n</code></pre>"},{"location":"api_docs/#src.capfinder.index","title":"<code>index</code>","text":"<p>We cannot random access a record in a BAM file, we can only iterate through it. That is our starting point. For each record in BAM file, we need to find the corresponding record in POD5 file. For that we need a mapping between POD5 file and read_ids. This is why we need to build an index of POD5 files. This module helps us to build an index of POD5 files and stores it in a SQLite database.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.index.fetch_filepath_using_filename","title":"<code>fetch_filepath_using_filename(conn: sqlite3.Connection, cursor: sqlite3.Cursor, pod5_filename: str) -&gt; Any</code>","text":"<p>Retrieve the pod5_filepath based on pod5_filename from the database.</p> <p>Parameters:</p> Name Type Description Default <code>conn</code> <code>Connection</code> <p>Connection object for the database.</p> required <code>cursor</code> <code>Cursor</code> <p>Cursor object for the database.</p> required <code>pod5_filename</code> <code>str</code> <p>The pod5_filename to be searched for.</p> required <p>Returns:</p> Name Type Description <code>pod5_filepath</code> <code>Any</code> <p>The corresponding pod5_filepath if found, else None.</p> Source code in <code>src/capfinder/index.py</code> <pre><code>def fetch_filepath_using_filename(\n    conn: sqlite3.Connection, cursor: sqlite3.Cursor, pod5_filename: str\n) -&gt; Any:\n    \"\"\"\n    Retrieve the pod5_filepath based on pod5_filename from the database.\n\n    Params:\n        conn (sqlite3.Connection): Connection object for the database.\n        cursor (sqlite3.Cursor): Cursor object for the database.\n        pod5_filename (str): The pod5_filename to be searched for.\n\n    Returns:\n        pod5_filepath (Any): The corresponding pod5_filepath if found, else None.\n    \"\"\"\n    try:\n        # Execute the SQL query to retrieve the pod5_filepath based on pod5_filename\n        cursor.execute(\n            \"SELECT pod5_filepath FROM pod5_index WHERE pod5_filename = ?\",\n            (pod5_filename,),\n        )\n        result = cursor.fetchone()\n\n        # Return the result (pod5_filepath) or None if not found\n        return result[0] if result else None\n\n    except sqlite3.Error as e:\n        logger.error(f\"Error: {e}\")\n        return None\n</code></pre>"},{"location":"api_docs/#src.capfinder.index.find_database_size","title":"<code>find_database_size(database_path: str) -&gt; Any</code>","text":"<p>Find the number of records in the database.</p> <p>Parameters:</p> Name Type Description Default <code>database_path</code> <code>str</code> <p>Path to the database.</p> required <p>Returns:</p> Name Type Description <code>size</code> <code>Any</code> <p>Number of records in the database.</p> Source code in <code>src/capfinder/index.py</code> <pre><code>def find_database_size(database_path: str) -&gt; Any:\n    \"\"\"\n    Find the number of records in the database.\n\n    Params:\n        database_path (str): Path to the database.\n\n    Returns:\n        size (Any): Number of records in the database.\n    \"\"\"\n    conn = sqlite3.connect(database_path)\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM pod5_index\")\n    result = cursor.fetchone()\n    size = result[0] if result is not None else 0\n    return size\n</code></pre>"},{"location":"api_docs/#src.capfinder.index.generate_pod5_path_and_name","title":"<code>generate_pod5_path_and_name(pod5_path: str) -&gt; Generator[Tuple[str, str], None, None]</code>","text":"<p>Traverse the directory and yield all the names+extension and fullpaths of the pod5 files.</p> <p>Parameters:</p> Name Type Description Default <code>pod5_path</code> <code>str</code> <p>Path to a POD5 file/directory of POD5 files.</p> required <p>Yields:</p> Type Description <code>str</code> <p>Tuple[str, str]: Tuple containing the name+extension and full path of a POD5 file.</p> Source code in <code>src/capfinder/index.py</code> <pre><code>def generate_pod5_path_and_name(\n    pod5_path: str,\n) -&gt; Generator[Tuple[str, str], None, None]:\n    \"\"\"Traverse the directory and yield all the names+extension and\n    fullpaths of the pod5 files.\n\n    Params:\n        pod5_path (str): Path to a POD5 file/directory of POD5 files.\n\n    Yields:\n        Tuple[str, str]: Tuple containing the name+extension and full path of a POD5 file.\n    \"\"\"\n\n    if os.path.isdir(pod5_path):\n        for root, _dirs, files in os.walk(pod5_path):\n            for file in files:\n                if file.endswith(\".pod5\"):\n                    yield (file, os.path.join(root, file))\n    elif os.path.isfile(pod5_path) and pod5_path.endswith(\".pod5\"):\n        root = os.path.basename(pod5_path)\n        file = os.path.dirname(pod5_path)\n        yield (file, os.path.join(root, file))\n</code></pre>"},{"location":"api_docs/#src.capfinder.index.index","title":"<code>index(pod5_path: str, output_dir: str) -&gt; None</code>","text":"<p>Builds an index mapping read_ids to POD5 file paths.</p> <p>Parameters:</p> Name Type Description Default <code>pod5_path</code> <code>str</code> <p>Path to a POD5 file or directory of POD5 files.</p> required <code>output_dir</code> <code>str</code> <p>Path where database.db file is written to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/index.py</code> <pre><code>def index(pod5_path: str, output_dir: str) -&gt; None:\n    \"\"\"\n    Builds an index mapping read_ids to POD5 file paths.\n\n    Params:\n        pod5_path (str): Path to a POD5 file or directory of POD5 files.\n        output_dir (str): Path where database.db file is written to.\n\n    Returns:\n        None\n    \"\"\"\n\n    database_path = os.path.join(output_dir, \"database.db\")\n    cursor, conn = initialize_database(database_path)\n    total_files = sum(1 for _ in generate_pod5_path_and_name(pod5_path))\n    logger.info(f\"Indexing {total_files} POD5 files\")\n    for data in tqdm(\n        generate_pod5_path_and_name(pod5_path),\n        total=total_files,\n        desc=\"\",\n        unit=\"files\",\n    ):\n        write_database(data, cursor, conn)\n    logger.info(\"Indexing complete\")\n    conn.close()\n</code></pre>"},{"location":"api_docs/#src.capfinder.index.initialize_database","title":"<code>initialize_database(database_path: str) -&gt; Tuple[sqlite3.Cursor, sqlite3.Connection]</code>","text":"<p>Intializes the database connection based on the database path.</p> <p>Parameters:</p> Name Type Description Default <code>database_path</code> <code>str</code> <p>Path to the database.</p> required <p>Returns:</p> Name Type Description <code>cursor</code> <code>Cursor</code> <p>Cursor object for the database.</p> <code>conn</code> <code>Connection</code> <p>Connection object for the database.</p> Source code in <code>src/capfinder/index.py</code> <pre><code>def initialize_database(\n    database_path: str,\n) -&gt; Tuple[sqlite3.Cursor, sqlite3.Connection]:\n    \"\"\"\n    Intializes the database connection based on the database path.\n\n    Params:\n        database_path (str): Path to the database.\n\n    Returns:\n        cursor (sqlite3.Cursor): Cursor object for the database.\n        conn (sqlite3.Connection): Connection object for the database.\n    \"\"\"\n    conn = sqlite3.connect(database_path)\n    cursor = conn.cursor()\n    cursor.execute(\n        \"\"\"CREATE TABLE IF NOT EXISTS pod5_index (pod5_filename TEXT PRIMARY KEY, pod5_filepath TEXT)\"\"\"\n    )\n    return cursor, conn\n</code></pre>"},{"location":"api_docs/#src.capfinder.index.write_database","title":"<code>write_database(data: Tuple[str, str], cursor: sqlite3.Cursor, conn: sqlite3.Connection) -&gt; None</code>","text":"<p>Write the index to a database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tuple[str, str]</code> <p>Tuples of fileroot and file</p> required <code>cursor</code> <code>Cursor</code> <p>Cursor object for the database.</p> required <code>conn</code> <code>Connection</code> <p>Connection object for the database.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/index.py</code> <pre><code>def write_database(\n    data: Tuple[str, str], cursor: sqlite3.Cursor, conn: sqlite3.Connection\n) -&gt; None:\n    \"\"\"\n    Write the index to a database.\n\n    Params:\n        data Tuple[str, str]): Tuples of fileroot and file\n        cursor (sqlite3.Cursor): Cursor object for the database.\n        conn (sqlite3.Connection): Connection object for the database.\n\n    Returns:\n        None\n    \"\"\"\n    cursor.execute(\"INSERT or REPLACE INTO pod5_index VALUES (?, ?)\", data)\n    conn.commit()\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference","title":"<code>inference</code>","text":""},{"location":"api_docs/#src.capfinder.inference.batched_inference","title":"<code>batched_inference(dataset: tf.data.Dataset, model: keras.Model, output_dir: str, csv_file_path: str) -&gt; str</code>","text":"<p>Perform batched inference on a dataset using a given model and save predictions to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The input dataset to perform inference on.</p> required <code>model</code> <code>Model</code> <p>The Keras model to use for making predictions.</p> required <code>output_dir</code> <code>str</code> <p>The directory where the output CSV file will be saved.</p> required <code>csv_file_path</code> <code>str</code> <p>Path to the original CSV file used to create the dataset.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the output CSV file containing the predictions.</p> Source code in <code>src/capfinder/inference.py</code> <pre><code>@task(cache_key_fn=task_input_hash)\ndef batched_inference(\n    dataset: tf.data.Dataset,\n    model: keras.Model,\n    output_dir: str,\n    csv_file_path: str,\n) -&gt; str:\n    \"\"\"\n    Perform batched inference on a dataset using a given model and save predictions to a CSV file.\n\n    Args:\n        dataset (tf.data.Dataset): The input dataset to perform inference on.\n        model (keras.Model): The Keras model to use for making predictions.\n        output_dir (str): The directory where the output CSV file will be saved.\n        csv_file_path (str): Path to the original CSV file used to create the dataset.\n\n    Returns:\n        str: The path to the output CSV file containing the predictions.\n    \"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    output_csv_path = os.path.join(output_dir, \"predictions.csv\")\n\n    total_reads = count_csv_rows(csv_file_path)\n    logger.info(f\"Total reads to perform cap predictions on: {total_reads}\")\n\n    with open(output_csv_path, \"w\", newline=\"\") as csvfile:\n        csvwriter = csv.writer(csvfile)\n        csvwriter.writerow([\"read_id\", \"predicted_cap\"])\n\n        pbar = tqdm(total=total_reads, unit=\"reads\")\n\n        processed_reads = 0\n        try:\n            for batch in dataset:\n                x, _, read_id = batch\n                preds = model.predict(x, verbose=0)\n                batch_pred_classes = tf.argmax(preds, axis=1).numpy()\n\n                rows_to_write = [\n                    [rid.decode(\"utf-8\"), map_cap_int_to_name(pred_class)]\n                    for rid, pred_class in zip(read_id.numpy(), batch_pred_classes)\n                ]\n\n                csvwriter.writerows(rows_to_write)\n\n                batch_size = len(read_id)\n                processed_reads += batch_size\n                pbar.update(batch_size)\n\n                if processed_reads &gt;= total_reads:\n                    break\n        except tf.errors.OutOfRangeError:\n            logger.warning(\n                \"Dataset iterator exhausted before processing all expected reads.\"\n            )\n        finally:\n            pbar.close()\n\n    logger.info(f\"Batched inference completed! Processed {processed_reads} reads.\")\n    if processed_reads != total_reads:\n        logger.warning(\n            f\"Number of processed samples ({processed_reads}) \"\n            f\"differs from expected total ({total_reads}).\"\n        )\n\n    return output_csv_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.collate_bam_pod5_wrapper","title":"<code>collate_bam_pod5_wrapper(bam_filepath: str, pod5_dir: str, num_cpus: int, reference: str, cap_class: int, cap0_pos: int, train_or_test: str, plot_signal: bool, output_dir: str) -&gt; tuple[str, str]</code>","text":"<p>Wrapper for collating BAM and POD5 files.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <code>pod5_dir</code> <code>str</code> <p>Directory containing POD5 files.</p> required <code>num_cpus</code> <code>int</code> <p>Number of CPUs to use for processing.</p> required <code>reference</code> <code>str</code> <p>Reference sequence.</p> required <code>cap_class</code> <code>int</code> <p>CAP class identifier.</p> required <code>cap0_pos</code> <code>int</code> <p>Position of CAP0.</p> required <code>train_or_test</code> <code>str</code> <p>Indicates whether data is for training or testing.</p> required <code>plot_signal</code> <code>bool</code> <p>Flag to plot the signal.</p> required <code>output_dir</code> <code>str</code> <p>Directory where output files will be saved.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: Paths to the data and metadata files.</p> Source code in <code>src/capfinder/inference.py</code> <pre><code>@task(cache_key_fn=task_input_hash)\ndef collate_bam_pod5_wrapper(\n    bam_filepath: str,\n    pod5_dir: str,\n    num_cpus: int,\n    reference: str,\n    cap_class: int,\n    cap0_pos: int,\n    train_or_test: str,\n    plot_signal: bool,\n    output_dir: str,\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Wrapper for collating BAM and POD5 files.\n\n    Args:\n        bam_filepath (str): Path to the BAM file.\n        pod5_dir (str): Directory containing POD5 files.\n        num_cpus (int): Number of CPUs to use for processing.\n        reference (str): Reference sequence.\n        cap_class (int): CAP class identifier.\n        cap0_pos (int): Position of CAP0.\n        train_or_test (str): Indicates whether data is for training or testing.\n        plot_signal (bool): Flag to plot the signal.\n        output_dir (str): Directory where output files will be saved.\n\n    Returns:\n        tuple[str, str]: Paths to the data and metadata files.\n    \"\"\"\n    data_path, metadata_path = collate_bam_pod5(\n        bam_filepath=bam_filepath,\n        pod5_dir=pod5_dir,\n        num_processes=num_cpus,\n        reference=reference,\n        cap_class=cap_class,\n        cap0_pos=cap0_pos,\n        train_or_test=train_or_test,\n        plot_signal=plot_signal,\n        output_dir=output_dir,\n    )\n    return data_path, metadata_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.count_csv_rows","title":"<code>count_csv_rows(file_path: str) -&gt; int</code>","text":"<p>Quickly count the number of rows in a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of rows in the CSV file (excluding the header).</p> Source code in <code>src/capfinder/inference.py</code> <pre><code>def count_csv_rows(file_path: str) -&gt; int:\n    \"\"\"\n    Quickly count the number of rows in a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file.\n\n    Returns:\n        int: Number of rows in the CSV file (excluding the header).\n    \"\"\"\n    with Path(file_path).open() as f:\n        return sum(1 for _ in f) - 1  # Subtract 1 to exclude the header\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.custom_cache_key_fn","title":"<code>custom_cache_key_fn(context: TaskRunContext, parameters: dict) -&gt; str</code>","text":"<p>Generate a custom cache key based on input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>TaskRunContext</code> <p>Prefect context (unused in this function).</p> required <code>parameters</code> <code>dict</code> <p>Dictionary of parameters used for cache key generation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated cache key.</p> Source code in <code>src/capfinder/inference.py</code> <pre><code>def custom_cache_key_fn(context: TaskRunContext, parameters: dict) -&gt; str:\n    \"\"\"\n    Generate a custom cache key based on input parameters.\n\n    Args:\n        context (TaskRunContext): Prefect context (unused in this function).\n        parameters (dict): Dictionary of parameters used for cache key generation.\n\n    Returns:\n        str: The generated cache key.\n    \"\"\"\n    dataset_hash = hashlib.md5(str(parameters[\"dataset\"]).encode()).hexdigest()\n    model_hash = hashlib.md5(str(parameters[\"model\"]).encode()).hexdigest()\n    output_dir_hash = hashlib.md5(parameters[\"output_dir\"].encode()).hexdigest()\n    combined_hash = hashlib.md5(\n        f\"{dataset_hash}_{model_hash}_{output_dir_hash}\".encode()\n    ).hexdigest()\n    return combined_hash\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.generate_report_wrapper","title":"<code>generate_report_wrapper(metadata_file: str, predictions_file: str, output_csv: str, output_html: str) -&gt; None</code>","text":"<p>Wrapper for generating the report.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_file</code> <code>str</code> <p>Path to the metadata file.</p> required <code>predictions_file</code> <code>str</code> <p>Path to the predictions file.</p> required <code>output_csv</code> <code>str</code> <p>Path to save the output CSV.</p> required <code>output_html</code> <code>str</code> <p>Path to save the output HTML.</p> required Source code in <code>src/capfinder/inference.py</code> <pre><code>@task(cache_key_fn=task_input_hash)\ndef generate_report_wrapper(\n    metadata_file: str, predictions_file: str, output_csv: str, output_html: str\n) -&gt; None:\n    \"\"\"\n    Wrapper for generating the report.\n\n    Args:\n        metadata_file (str): Path to the metadata file.\n        predictions_file (str): Path to the predictions file.\n        output_csv (str): Path to save the output CSV.\n        output_html (str): Path to save the output HTML.\n    \"\"\"\n    generate_report(\n        metadata_file,\n        predictions_file,\n        output_csv,\n        output_html,\n    )\n    os.remove(predictions_file)\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.get_model","title":"<code>get_model(model_path: Optional[str] = None, load_optimizer: bool = False) -&gt; keras.Model</code>","text":"<p>Load and return a model from the given model path or use the default model.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Optional[str]</code> <p>Path to the custom model file. If None, use the default model.</p> <code>None</code> <code>load_optimizer</code> <code>bool</code> <p>Whether to load the optimizer with the model.</p> <code>False</code> <p>Returns:</p> Type Description <code>Model</code> <p>keras.Model: The loaded Keras model.</p> Source code in <code>src/capfinder/inference.py</code> <pre><code>def get_model(\n    model_path: Optional[str] = None, load_optimizer: bool = False\n) -&gt; keras.Model:\n    \"\"\"\n    Load and return a model from the given model path or use the default model.\n\n    Args:\n        model_path (Optional[str]): Path to the custom model file. If None, use the default model.\n        load_optimizer (bool): Whether to load the optimizer with the model.\n\n    Returns:\n        keras.Model: The loaded Keras model.\n    \"\"\"\n    if model_path is None:\n        model_file = resources.files(model_module).joinpath(\"cnn_lstm-classifier.keras\")\n        with resources.as_file(model_file) as default_model_path:\n            model = keras.models.load_model(default_model_path, compile=False)\n    else:\n        model = keras.models.load_model(model_path, compile=False)\n\n    logger.info(\n        f\"Model loaded successfully from {'default path' if model_path is None else model_path}\"\n    )\n    return model\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.predict_cap_types","title":"<code>predict_cap_types(bam_filepath: str, pod5_dir: str, num_cpus: int, output_dir: str, dtype: DtypeLiteral, reference: str = 'GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT', cap0_pos: int = 52, train_or_test: str = 'test', plot_signal: bool = False, cap_class: int = -99, target_length: int = 500, batch_size: int = 32, custom_model_path: Optional[str] = None, debug_code: bool = False, refresh_cache: bool = False, formatted_command: Optional[str] = None) -&gt; None</code>","text":"<p>Predict CAP types by preparing the inference data and running the prediction workflow.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <code>pod5_dir</code> <code>str</code> <p>Directory containing POD5 files.</p> required <code>num_cpus</code> <code>int</code> <p>Number of CPUs to use for processing.</p> required <code>output_dir</code> <code>str</code> <p>Directory where output files will be saved.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>Data type for the features.</p> required <code>reference</code> <code>str</code> <p>Reference sequence.</p> <code>'GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT'</code> <code>cap0_pos</code> <code>int</code> <p>Position of CAP0.</p> <code>52</code> <code>train_or_test</code> <code>str</code> <p>Indicates whether data is for training or testing.</p> <code>'test'</code> <code>plot_signal</code> <code>bool</code> <p>Flag to plot the signal.</p> <code>False</code> <code>cap_class</code> <code>int</code> <p>CAP class identifier.</p> <code>-99</code> <code>target_length</code> <code>int</code> <p>Length of the target sequence.</p> <code>500</code> <code>batch_size</code> <code>int</code> <p>Size of the data batches.</p> <code>32</code> <code>custom_model_path</code> <code>Optional[str]</code> <p>Path to a custom model file. If None, use the default model.</p> <code>None</code> <code>debug_code</code> <code>bool</code> <p>Flag to enable debugging information in logs.</p> <code>False</code> <code>refresh_cache</code> <code>bool</code> <p>Flag to refresh cached data.</p> <code>False</code> <code>formatted_command</code> <code>Optional[str]</code> <p>The formatted command string to be logged.</p> <code>None</code> Source code in <code>src/capfinder/inference.py</code> <pre><code>def predict_cap_types(\n    bam_filepath: str,\n    pod5_dir: str,\n    num_cpus: int,\n    output_dir: str,\n    dtype: DtypeLiteral,\n    reference: str = \"GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT\",\n    cap0_pos: int = 52,\n    train_or_test: str = \"test\",\n    plot_signal: bool = False,\n    cap_class: int = -99,\n    target_length: int = 500,\n    batch_size: int = 32,\n    custom_model_path: Optional[str] = None,\n    debug_code: bool = False,\n    refresh_cache: bool = False,\n    formatted_command: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Predict CAP types by preparing the inference data and running the prediction workflow.\n\n    Args:\n        bam_filepath (str): Path to the BAM file.\n        pod5_dir (str): Directory containing POD5 files.\n        num_cpus (int): Number of CPUs to use for processing.\n        output_dir (str): Directory where output files will be saved.\n        dtype (DtypeLiteral): Data type for the features.\n        reference (str): Reference sequence.\n        cap0_pos (int): Position of CAP0.\n        train_or_test (str): Indicates whether data is for training or testing.\n        plot_signal (bool): Flag to plot the signal.\n        cap_class (int): CAP class identifier.\n        target_length (int): Length of the target sequence.\n        batch_size (int): Size of the data batches.\n        custom_model_path (Optional[str]): Path to a custom model file. If None, use the default model.\n        debug_code (bool): Flag to enable debugging information in logs.\n        refresh_cache (bool): Flag to refresh cached data.\n        formatted_command (Optional[str]): The formatted command string to be logged.\n    \"\"\"\n    log_filepath = configure_logger(\n        os.path.join(output_dir, \"logs\"), show_location=debug_code\n    )\n    configure_prefect_logging(show_location=debug_code)\n    version_info = version(\"capfinder\")\n    log_header(f\"Using Capfinder v{version_info}\")\n    logger.info(formatted_command)\n    output_csv_path, output_html_path = prepare_inference_data(\n        bam_filepath,\n        pod5_dir,\n        num_cpus,\n        output_dir,\n        dtype,\n        reference,\n        cap0_pos,\n        train_or_test,\n        plot_signal,\n        cap_class,\n        target_length,\n        batch_size,\n        custom_model_path,\n        debug_code,\n        refresh_cache,\n    )\n    grey = \"\\033[90m\"\n    reset = \"\\033[0m\"\n    log_output(\n        f\"Cap predictions have been saved to the following path:\\n {grey}{output_csv_path}{reset}\\nThe log file has been saved to:\\n {grey}{log_filepath}{reset}\\nThe analysis report has been saved to:\\n {grey}{output_html_path}{reset}\"\n    )\n    log_header(\"Processing finished!\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.prepare_inference_data","title":"<code>prepare_inference_data(bam_filepath: str, pod5_dir: str, num_cpus: int, output_dir: str, dtype: DtypeLiteral, reference: str = 'GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT', cap0_pos: int = 52, train_or_test: str = 'test', plot_signal: bool = False, cap_class: int = -99, target_length: int = 500, batch_size: int = 32, custom_model_path: Optional[str] = None, debug_code: bool = False, refresh_cache: bool = False) -&gt; tuple[str, str]</code>","text":"<p>Prepare inference data by processing BAM and POD5 files, and generate features for the model.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <code>pod5_dir</code> <code>str</code> <p>Directory containing POD5 files.</p> required <code>num_cpus</code> <code>int</code> <p>Number of CPUs to use for processing.</p> required <code>output_dir</code> <code>str</code> <p>Directory where output files will be saved.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>Data type for the features.</p> required <code>reference</code> <code>str</code> <p>Reference sequence.</p> <code>'GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT'</code> <code>cap0_pos</code> <code>int</code> <p>Position of CAP0.</p> <code>52</code> <code>train_or_test</code> <code>str</code> <p>Indicates whether data is for training or testing.</p> <code>'test'</code> <code>plot_signal</code> <code>bool</code> <p>Flag to plot the signal.</p> <code>False</code> <code>cap_class</code> <code>int</code> <p>CAP class identifier.</p> <code>-99</code> <code>target_length</code> <code>int</code> <p>Length of the target sequence.</p> <code>500</code> <code>batch_size</code> <code>int</code> <p>Size of the data batches.</p> <code>32</code> <code>custom_model_path</code> <code>Optional[str]</code> <p>Path to a custom model file. If None, use the default model.</p> <code>None</code> <code>debug_code</code> <code>bool</code> <p>Flag to enable debugging information in logs.</p> <code>False</code> <code>refresh_cache</code> <code>bool</code> <p>Flag to refresh cached data.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>tuple[str, str]: Paths to the output CSV and HTML files.</p> Source code in <code>src/capfinder/inference.py</code> <pre><code>@flow(name=\"prepare-inference-data\")\ndef prepare_inference_data(\n    bam_filepath: str,\n    pod5_dir: str,\n    num_cpus: int,\n    output_dir: str,\n    dtype: DtypeLiteral,\n    reference: str = \"GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT\",\n    cap0_pos: int = 52,\n    train_or_test: str = \"test\",\n    plot_signal: bool = False,\n    cap_class: int = -99,\n    target_length: int = 500,\n    batch_size: int = 32,\n    custom_model_path: Optional[str] = None,\n    debug_code: bool = False,\n    refresh_cache: bool = False,\n) -&gt; tuple[str, str]:\n    \"\"\"\n    Prepare inference data by processing BAM and POD5 files, and generate features for the model.\n\n    Args:\n        bam_filepath (str): Path to the BAM file.\n        pod5_dir (str): Directory containing POD5 files.\n        num_cpus (int): Number of CPUs to use for processing.\n        output_dir (str): Directory where output files will be saved.\n        dtype (DtypeLiteral): Data type for the features.\n        reference (str): Reference sequence.\n        cap0_pos (int): Position of CAP0.\n        train_or_test (str): Indicates whether data is for training or testing.\n        plot_signal (bool): Flag to plot the signal.\n        cap_class (int): CAP class identifier.\n        target_length (int): Length of the target sequence.\n        batch_size (int): Size of the data batches.\n        custom_model_path (Optional[str]): Path to a custom model file. If None, use the default model.\n        debug_code (bool): Flag to enable debugging information in logs.\n        refresh_cache (bool): Flag to refresh cached data.\n\n    Returns:\n        tuple[str, str]: Paths to the output CSV and HTML files.\n    \"\"\"\n    configure_prefect_logging(show_location=debug_code)\n    os.makedirs(output_dir, exist_ok=True)\n\n    log_step(1, 5, \"Extracting Cap Signal by collating BAM and POD5 files\")\n    data_path, metadata_path = collate_bam_pod5_wrapper.with_options(\n        refresh_cache=refresh_cache\n    )(\n        bam_filepath=bam_filepath,\n        pod5_dir=pod5_dir,\n        num_cpus=num_cpus,\n        reference=reference,\n        cap_class=cap_class,\n        cap0_pos=cap0_pos,\n        train_or_test=train_or_test,\n        plot_signal=plot_signal,\n        output_dir=os.path.join(output_dir, \"0_raw_cap_signal_data\"),\n    )\n\n    log_step(2, 5, \"Creating TensorFlow dataset\")\n    dataset = create_dataset(data_path, target_length, batch_size, dtype)\n\n    log_step(\n        3, 5, f\"Loading the {'custom' if custom_model_path else 'pre-trained'} model\"\n    )\n    model = get_model(custom_model_path)\n\n    log_step(4, 5, \"Performing batch inference for cap type prediction\")\n    predictions_csv_path = batched_inference.with_options(refresh_cache=refresh_cache)(\n        dataset,\n        model,\n        output_dir=os.path.join(output_dir, \"1_cap_predictions\"),\n        csv_file_path=data_path,\n    )\n\n    log_step(5, 5, \"Generating report\")\n    output_csv_path = os.path.join(\n        output_dir, \"1_cap_predictions\", \"cap_predictions.csv\"\n    )\n    output_html_path = os.path.join(\n        output_dir, \"1_cap_predictions\", \"cap_analysis_report.html\"\n    )\n    generate_report_wrapper.with_options(refresh_cache=refresh_cache)(\n        metadata_file=metadata_path,\n        predictions_file=predictions_csv_path,\n        output_csv=output_csv_path,\n        output_html=output_html_path,\n    )\n    return output_csv_path, output_html_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference.reconfigure_logging_task","title":"<code>reconfigure_logging_task(output_dir: str, debug_code: bool) -&gt; None</code>","text":"<p>Reconfigure logging settings for both application and Prefect.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Directory where logs will be saved.</p> required <code>debug_code</code> <code>bool</code> <p>Flag to determine if code locations should be shown in logs.</p> required Source code in <code>src/capfinder/inference.py</code> <pre><code>@task(cache_key_fn=custom_cache_key_fn)\ndef reconfigure_logging_task(output_dir: str, debug_code: bool) -&gt; None:\n    \"\"\"\n    Reconfigure logging settings for both application and Prefect.\n\n    Args:\n        output_dir (str): Directory where logs will be saved.\n        debug_code (bool): Flag to determine if code locations should be shown in logs.\n    \"\"\"\n    configure_logger(output_dir, show_location=debug_code)\n    configure_prefect_logging(show_location=debug_code)\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference_data_loader","title":"<code>inference_data_loader</code>","text":""},{"location":"api_docs/#src.capfinder.inference_data_loader.create_dataset","title":"<code>create_dataset(file_path: str, target_length: int, batch_size: int, dtype: DtypeLiteral) -&gt; tf.data.Dataset</code>","text":"<p>Create a TensorFlow dataset from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required <code>target_length</code> <code>int</code> <p>The desired length of the timeseries tensor.</p> required <code>batch_size</code> <code>int</code> <p>The number of samples per batch.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>The desired data type for the timeseries tensor as a string.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>tf.data.Dataset: A TensorFlow dataset that yields batches of parsed and formatted data.</p> Source code in <code>src/capfinder/inference_data_loader.py</code> <pre><code>def create_dataset(\n    file_path: str, target_length: int, batch_size: int, dtype: DtypeLiteral\n) -&gt; tf.data.Dataset:\n    \"\"\"\n    Create a TensorFlow dataset from a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file.\n        target_length (int): The desired length of the timeseries tensor.\n        batch_size (int): The number of samples per batch.\n        dtype (DtypeLiteral): The desired data type for the timeseries tensor as a string.\n\n    Returns:\n        tf.data.Dataset: A TensorFlow dataset that yields batches of parsed and formatted data.\n    \"\"\"\n    tf_dtype = get_dtype(dtype)\n\n    dataset = tf.data.Dataset.from_generator(\n        lambda: csv_generator(file_path),\n        output_signature=(\n            tf.TensorSpec(shape=(), dtype=tf.string),\n            tf.TensorSpec(shape=(), dtype=tf.string),\n            tf.TensorSpec(shape=(), dtype=tf.string),\n        ),\n    )\n\n    dataset = dataset.map(\n        lambda x, y, z: parse_row((x, y, z), target_length, tf_dtype),\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\n    logger.info(\"Dataset created successfully.\")\n    return dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference_data_loader.csv_generator","title":"<code>csv_generator(file_path: str, chunk_size: int = 10000) -&gt; Generator[Tuple[str, str, str], None, None]</code>","text":"<p>Generate rows from a CSV file in chunks.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required <code>chunk_size</code> <code>int</code> <p>Number of rows to process in each chunk. Defaults to 10000.</p> <code>10000</code> <p>Yields:</p> Type Description <code>str</code> <p>Tuple[str, str, str]: A tuple containing read_id, cap_class, and timeseries as strings.</p> Source code in <code>src/capfinder/inference_data_loader.py</code> <pre><code>def csv_generator(\n    file_path: str, chunk_size: int = 10000\n) -&gt; Generator[Tuple[str, str, str], None, None]:\n    \"\"\"\n    Generate rows from a CSV file in chunks.\n\n    Args:\n        file_path (str): Path to the CSV file.\n        chunk_size (int, optional): Number of rows to process in each chunk. Defaults to 10000.\n\n    Yields:\n        Tuple[str, str, str]: A tuple containing read_id, cap_class, and timeseries as strings.\n    \"\"\"\n    df = pl.scan_csv(file_path)\n    total_rows = df.select(pl.count()).collect().item()\n\n    for start in range(0, total_rows, chunk_size):\n        min(start + chunk_size, total_rows)\n        chunk = df.slice(start, chunk_size).collect()\n        for row in chunk.iter_rows():\n            yield (str(row[0]), str(row[1]), str(row[2]))\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference_data_loader.get_dtype","title":"<code>get_dtype(dtype: str) -&gt; tf.DType</code>","text":"<p>Convert a string dtype to its corresponding TensorFlow data type.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>str</code> <p>A string representing the desired data type.</p> required <p>Returns:</p> Type Description <code>DType</code> <p>tf.DType: The corresponding TensorFlow data type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid dtype string is provided.</p> Source code in <code>src/capfinder/inference_data_loader.py</code> <pre><code>def get_dtype(dtype: str) -&gt; tf.DType:\n    \"\"\"\n    Convert a string dtype to its corresponding TensorFlow data type.\n\n    Args:\n        dtype (str): A string representing the desired data type.\n\n    Returns:\n        tf.DType: The corresponding TensorFlow data type.\n\n    Raises:\n        ValueError: If an invalid dtype string is provided.\n    \"\"\"\n    valid_dtypes = {\n        \"float16\": float16,\n        \"float32\": float32,\n        \"float64\": float64,\n    }\n\n    if dtype in valid_dtypes:\n        return valid_dtypes[dtype]\n    else:\n        logger.warning('You provided an invalid dtype. Using \"float32\" as default.')\n        return float32\n</code></pre>"},{"location":"api_docs/#src.capfinder.inference_data_loader.parse_row","title":"<code>parse_row(row: Tuple[str, str, str], target_length: int, dtype: tf.DType) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor]</code>","text":"<p>Parse a row of data and convert it to the appropriate tensor format. Padding and truncation are performed equally on both sides of the time series.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Tuple[str, str, str]</code> <p>A tuple containing read_id, cap_class, and timeseries as strings.</p> required <code>target_length</code> <code>int</code> <p>The desired length of the timeseries tensor.</p> required <code>dtype</code> <code>DType</code> <p>The desired data type for the timeseries tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: A tuple containing the parsed and formatted tensors for</p> <code>Tensor</code> <p>timeseries, cap_class, and read_id.</p> Source code in <code>src/capfinder/inference_data_loader.py</code> <pre><code>def parse_row(\n    row: Tuple[str, str, str], target_length: int, dtype: tf.DType\n) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n    \"\"\"\n    Parse a row of data and convert it to the appropriate tensor format.\n    Padding and truncation are performed equally on both sides of the time series.\n\n    Args:\n        row (Tuple[str, str, str]): A tuple containing read_id, cap_class, and timeseries as strings.\n        target_length (int): The desired length of the timeseries tensor.\n        dtype (tf.DType): The desired data type for the timeseries tensor.\n\n    Returns:\n        Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: A tuple containing the parsed and formatted tensors for\n        timeseries, cap_class, and read_id.\n    \"\"\"\n    read_id, cap_class, timeseries = row\n    cap_class = tf.strings.to_number(cap_class, out_type=tf.int32)\n\n    # Split the timeseries string and convert to float\n    timeseries = tf.strings.split(timeseries, sep=\",\")\n    timeseries = tf.strings.to_number(timeseries, out_type=tf.float32)\n\n    # Get the current length of the timeseries\n    current_length = tf.shape(timeseries)[0]\n\n    # Function to pad the timeseries\n    def pad_timeseries() -&gt; tf.Tensor:\n        pad_amount = target_length - current_length\n        pad_left = pad_amount // 2\n        pad_right = pad_amount - pad_left\n        return tf.pad(\n            timeseries,\n            [[pad_left, pad_right]],\n            constant_values=0.0,\n        )\n\n    # Function to truncate the timeseries\n    def truncate_timeseries() -&gt; tf.Tensor:\n        truncate_amount = current_length - target_length\n        truncate_left = truncate_amount // 2\n        truncate_right = current_length - (truncate_amount - truncate_left)\n        return timeseries[truncate_left:truncate_right]\n\n    # Pad or truncate the timeseries to the target length\n    padded = tf.cond(\n        current_length &gt;= target_length, truncate_timeseries, pad_timeseries\n    )\n\n    padded = tf.reshape(padded, (target_length, 1))\n\n    # Cast to the desired dtype\n    if dtype != tf.float32:\n        padded = tf.cast(padded, dtype)\n\n    return padded, cap_class, read_id\n</code></pre>"},{"location":"api_docs/#src.capfinder.logger_config","title":"<code>logger_config</code>","text":""},{"location":"api_docs/#src.capfinder.logger_config.PrefectHandler","title":"<code>PrefectHandler</code>","text":"<p>               Bases: <code>Handler</code></p> <p>A custom logging handler for Prefect that filters and formats log messages.</p> <p>This handler integrates with Loguru, applies custom formatting, and prevents duplicate log messages.</p> Source code in <code>src/capfinder/logger_config.py</code> <pre><code>class PrefectHandler(logging.Handler):\n    \"\"\"\n    A custom logging handler for Prefect that filters and formats log messages.\n\n    This handler integrates with Loguru, applies custom formatting, and prevents duplicate log messages.\n    \"\"\"\n\n    def __init__(self, loguru_logger: loguru.Logger, show_location: bool) -&gt; None:\n        \"\"\"\n        Initialize the PrefectHandler.\n\n        Args:\n            loguru_logger (Logger): The Loguru logger instance to use for logging.\n            show_location (bool): Whether to show the source location in log messages.\n        \"\"\"\n        super().__init__()\n        self.loguru_logger = loguru_logger\n        self.show_location = show_location\n        self.logged_messages: set[str] = set()\n        self.prefix_pattern: re.Pattern = re.compile(\n            r\"(logging:handle:\\d+ - )(\\w+\\.\\w+)\"\n        )\n\n    def emit(self, record: logging.LogRecord) -&gt; None:\n        \"\"\"\n        Emit a log record.\n\n        This method formats the log record, applies custom styling, and logs it using Loguru.\n        It also filters out duplicate messages and HTTP status messages.\n\n        Args:\n            record (logging.LogRecord): The log record to emit.\n        \"\"\"\n        try:\n            # Filter out HTTP status messages\n            if \"HTTP Request:\" in record.getMessage():\n                return\n\n            level: str = record.levelname\n            message: str = self.format(record)\n            name: str = record.name\n            function: str = record.funcName\n            line: int = record.lineno\n\n            # Color the part after \"logging:handle:XXXX - \" cyan\n            colored_message: str = self.prefix_pattern.sub(\n                r\"\\1&lt;cyan&gt;\\2&lt;/cyan&gt;\", message\n            )\n\n            # Handle progress bar messages\n            if \"|\" in colored_message and (\n                \"%\" in colored_message or \"it/s\" in colored_message\n            ):\n                formatted_message: str = f\"Progress: {colored_message}\"\n            else:\n                if self.show_location:\n                    formatted_message = f\"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - {colored_message}\"\n                else:\n                    formatted_message = colored_message\n\n            # Create a unique identifier for this log message\n            message_id: str = message\n\n            # Only log if we haven't seen this message before\n            if message_id not in self.logged_messages:\n                self.logged_messages.add(message_id)\n                self.loguru_logger.opt(depth=1, colors=True).log(\n                    level, formatted_message\n                )\n        except Exception:\n            self.handleError(record)\n</code></pre>"},{"location":"api_docs/#src.capfinder.logger_config.PrefectHandler.__init__","title":"<code>__init__(loguru_logger: loguru.Logger, show_location: bool) -&gt; None</code>","text":"<p>Initialize the PrefectHandler.</p> <p>Parameters:</p> Name Type Description Default <code>loguru_logger</code> <code>Logger</code> <p>The Loguru logger instance to use for logging.</p> required <code>show_location</code> <code>bool</code> <p>Whether to show the source location in log messages.</p> required Source code in <code>src/capfinder/logger_config.py</code> <pre><code>def __init__(self, loguru_logger: loguru.Logger, show_location: bool) -&gt; None:\n    \"\"\"\n    Initialize the PrefectHandler.\n\n    Args:\n        loguru_logger (Logger): The Loguru logger instance to use for logging.\n        show_location (bool): Whether to show the source location in log messages.\n    \"\"\"\n    super().__init__()\n    self.loguru_logger = loguru_logger\n    self.show_location = show_location\n    self.logged_messages: set[str] = set()\n    self.prefix_pattern: re.Pattern = re.compile(\n        r\"(logging:handle:\\d+ - )(\\w+\\.\\w+)\"\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.logger_config.PrefectHandler.emit","title":"<code>emit(record: logging.LogRecord) -&gt; None</code>","text":"<p>Emit a log record.</p> <p>This method formats the log record, applies custom styling, and logs it using Loguru. It also filters out duplicate messages and HTTP status messages.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>LogRecord</code> <p>The log record to emit.</p> required Source code in <code>src/capfinder/logger_config.py</code> <pre><code>def emit(self, record: logging.LogRecord) -&gt; None:\n    \"\"\"\n    Emit a log record.\n\n    This method formats the log record, applies custom styling, and logs it using Loguru.\n    It also filters out duplicate messages and HTTP status messages.\n\n    Args:\n        record (logging.LogRecord): The log record to emit.\n    \"\"\"\n    try:\n        # Filter out HTTP status messages\n        if \"HTTP Request:\" in record.getMessage():\n            return\n\n        level: str = record.levelname\n        message: str = self.format(record)\n        name: str = record.name\n        function: str = record.funcName\n        line: int = record.lineno\n\n        # Color the part after \"logging:handle:XXXX - \" cyan\n        colored_message: str = self.prefix_pattern.sub(\n            r\"\\1&lt;cyan&gt;\\2&lt;/cyan&gt;\", message\n        )\n\n        # Handle progress bar messages\n        if \"|\" in colored_message and (\n            \"%\" in colored_message or \"it/s\" in colored_message\n        ):\n            formatted_message: str = f\"Progress: {colored_message}\"\n        else:\n            if self.show_location:\n                formatted_message = f\"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - {colored_message}\"\n            else:\n                formatted_message = colored_message\n\n        # Create a unique identifier for this log message\n        message_id: str = message\n\n        # Only log if we haven't seen this message before\n        if message_id not in self.logged_messages:\n            self.logged_messages.add(message_id)\n            self.loguru_logger.opt(depth=1, colors=True).log(\n                level, formatted_message\n            )\n    except Exception:\n        self.handleError(record)\n</code></pre>"},{"location":"api_docs/#src.capfinder.logger_config.configure_logger","title":"<code>configure_logger(new_log_directory: str = '', show_location: bool = True) -&gt; str</code>","text":"<p>Configure the logger to log to a file in the specified directory.</p> Source code in <code>src/capfinder/logger_config.py</code> <pre><code>def configure_logger(new_log_directory: str = \"\", show_location: bool = True) -&gt; str:\n    \"\"\"Configure the logger to log to a file in the specified directory.\"\"\"\n    global log_directory\n    log_directory = new_log_directory if new_log_directory else log_directory\n\n    # Ensure log directory exists\n    os.makedirs(log_directory, exist_ok=True)\n\n    # Get current date and time\n    now: datetime = datetime.now()\n    timestamp: str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n    app_version: str = get_version()\n\n    # Use the timestamp in the log file name\n    log_filename: str = f\"capfinder_v{app_version}_{timestamp}.log\"\n    log_filepath: str = os.path.join(log_directory, log_filename)\n\n    # Remove default handler\n    logger.remove()\n\n    # Configure logger to log to both file and console with the same format\n    log_format: str = (\n        \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss.SSS}&lt;/green&gt; | &lt;level&gt;{level:&lt;8}&lt;/level&gt; | \"\n    )\n    if show_location:\n        log_format += (\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n        )\n    log_format += \"&lt;level&gt;{message}&lt;/level&gt;\"\n\n    logger.add(log_filepath, format=log_format, colorize=True)\n    logger.add(sys.stdout, format=log_format, colorize=True)\n\n    return log_filepath\n</code></pre>"},{"location":"api_docs/#src.capfinder.logger_config.configure_prefect_logging","title":"<code>configure_prefect_logging(show_location: bool = True) -&gt; None</code>","text":"<p>Configure Prefect logging with custom handler and settings.</p> <p>This function sets up a custom PrefectHandler for all Prefect loggers, configures the root logger, and adjusts logging levels.</p> <p>Parameters:</p> Name Type Description Default <code>show_location</code> <code>bool</code> <p>Whether to show source location in log messages. Defaults to True.</p> <code>True</code> Source code in <code>src/capfinder/logger_config.py</code> <pre><code>def configure_prefect_logging(show_location: bool = True) -&gt; None:\n    \"\"\"\n    Configure Prefect logging with custom handler and settings.\n\n    This function sets up a custom PrefectHandler for all Prefect loggers,\n    configures the root logger, and adjusts logging levels.\n\n    Args:\n        show_location (bool, optional): Whether to show source location in log messages. Defaults to True.\n    \"\"\"\n    # Create a single PrefectHandler instance\n    handler = PrefectHandler(logger, show_location)\n\n    # Configure the root logger\n    root_logger = logging.getLogger()\n    for h in root_logger.handlers[:]:\n        root_logger.removeHandler(h)\n    root_logger.addHandler(handler)\n    root_logger.setLevel(logging.INFO)\n\n    # Configure all Prefect loggers\n    prefect_loggers = [\n        logging.getLogger(name)\n        for name in logging.root.manager.loggerDict\n        if name.startswith(\"prefect\")\n    ]\n    for prefect_logger in prefect_loggers:\n        prefect_logger.handlers = [handler]\n        prefect_logger.propagate = False\n        prefect_logger.setLevel(logging.INFO)\n\n    # Disable httpx logging\n    logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n</code></pre>"},{"location":"api_docs/#src.capfinder.logger_config.get_version","title":"<code>get_version() -&gt; str</code>","text":"<p>Get the version of the app from pyproject.toml.</p> Source code in <code>src/capfinder/logger_config.py</code> <pre><code>def get_version() -&gt; str:\n    \"\"\"Get the version of the app from pyproject.toml.\"\"\"\n    app_version: str = version(\"capfinder\")\n    return app_version\n</code></pre>"},{"location":"api_docs/#src.capfinder.plot","title":"<code>plot</code>","text":"<p>The modules helps in plotting the entire read signal, signal for ROI, and the base annotations. It also prints alignments. All this information is useful in understanding if the OTE-finding algorthim is homing-in on the correct region of interest (ROI).</p> <p>The plot is saved as an HTML file.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.plot.append_dummy_sequence","title":"<code>append_dummy_sequence(fasta_sequence: str, num_left_clipped_bases: int, num_right_clipped_bases: int) -&gt; str</code>","text":"<p>Append/prepend 'H' to the left/right of the FASTA sequence based on soft-clipping counts</p> <p>Parameters:</p> Name Type Description Default <code>fasta_sequence</code> <code>str</code> <p>FASTA sequence</p> required <code>num_left_clipped_bases</code> <code>int</code> <p>Number of bases soft-clipped from the left</p> required <code>num_right_clipped_bases</code> <code>int</code> <p>Number of bases soft-clipped from the right</p> required <p>Returns:</p> Name Type Description <code>modified_sequence</code> <code>str</code> <p>FASTA sequence with 'H' appended/prepended to the left/right</p> Source code in <code>src/capfinder/plot.py</code> <pre><code>def append_dummy_sequence(\n    fasta_sequence: str, num_left_clipped_bases: int, num_right_clipped_bases: int\n) -&gt; str:\n    \"\"\"Append/prepend 'H' to the left/right of the FASTA sequence based on soft-clipping counts\n\n    Args:\n        fasta_sequence (str): FASTA sequence\n        num_left_clipped_bases (int): Number of bases soft-clipped from the left\n        num_right_clipped_bases (int): Number of bases soft-clipped from the right\n\n    Returns:\n        modified_sequence (str): FASTA sequence with 'H' appended/prepended to the left/right\n\n    \"\"\"\n    modified_sequence = (\n        \"H\" * num_left_clipped_bases + fasta_sequence + \"H\" * num_right_clipped_bases\n    )\n    return modified_sequence\n</code></pre>"},{"location":"api_docs/#src.capfinder.process_pod5","title":"<code>process_pod5</code>","text":"<p>Given read_id and pod5 filepath, this file preprocesses the signal data, and extracts the signal data for a region of interest (ROI).</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.process_pod5.clip_extreme_values","title":"<code>clip_extreme_values(z_normalized_data: npt.NDArray[np.float64], num_std_dev: float = 4.0) -&gt; npt.NDArray[np.float64]</code>","text":"<p>Clip extreme values in the Z-score normalized data.</p> <p>Clips values outside the specified number of standard deviations from the mean. This function takes Z-score normalized data as input, along with an optional parameter to set the number of standard deviations.</p> <p>Parameters:</p> Name Type Description Default <code>z_normalized_data</code> <code>NDArray[float64]</code> <p>Z-score normalized data.</p> required <code>num_std_dev</code> <code>float</code> <p>Number of standard deviations to use as the limit. Defaults to 4.0.</p> <code>4.0</code> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>npt.NDArray[np.float64]: Clipped data within the specified range.</p> Example <p>z_normalized_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0]) clipped_data = clip_extreme_values(z_normalized_data, num_std_dev=3.0)</p> Source code in <code>src/capfinder/process_pod5.py</code> <pre><code>def clip_extreme_values(\n    z_normalized_data: npt.NDArray[np.float64], num_std_dev: float = 4.0\n) -&gt; npt.NDArray[np.float64]:\n    \"\"\"Clip extreme values in the Z-score normalized data.\n\n    Clips values outside the specified number of standard deviations from\n    the mean. This function takes Z-score normalized data as input, along\n    with an optional parameter to set the number of standard deviations.\n\n    Params:\n        z_normalized_data (npt.NDArray[np.float64]): Z-score normalized data.\n        num_std_dev (float, optional): Number of standard deviations to use\n            as the limit. Defaults to 4.0.\n\n    Returns:\n        npt.NDArray[np.float64]: Clipped data within the specified range.\n\n    Example:\n        &gt;&gt;&gt; z_normalized_data = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n        &gt;&gt;&gt; clipped_data = clip_extreme_values(z_normalized_data, num_std_dev=3.0)\n    \"\"\"\n\n    lower_limit = -num_std_dev\n    upper_limit = num_std_dev\n    clipped_data: npt.NDArray[np.float64] = np.clip(\n        z_normalized_data, lower_limit, upper_limit\n    )\n    return clipped_data\n</code></pre>"},{"location":"api_docs/#src.capfinder.process_pod5.extract_roi_signal","title":"<code>extract_roi_signal(signal: np.ndarray, base_locs_in_signal: npt.NDArray[np.int32], fasta: str, experiment_type: str, start_base_idx_in_fasta: int, end_base_idx_in_fasta: int, num_left_clipped_bases: int) -&gt; ROIData</code>","text":"<p>Extracts the signal data for a region of interest (ROI).</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>ndarray</code> <p>Signal data.</p> required <code>base_locs_in_signal</code> <code>NDArray[int32]</code> <p>Array of locations of each new base in the signal.</p> required <code>fasta</code> <code>str</code> <p>Fasta sequence of the read.</p> required <code>experiment_type</code> <code>str</code> <p>Type of experiment (rna or dna).</p> required <code>start_base_idx_in_fasta</code> <code>int</code> <p>Index of the first base in the ROI.</p> required <code>end_base_idx_in_fasta</code> <code>int</code> <p>Index of the last base in the ROI.</p> required <code>num_left_clipped_bases</code> <code>int</code> <p>Number of bases clipped from the left.</p> required <p>Returns:</p> Name Type Description <code>ROIData</code> <code>ROIData</code> <p>Dictionary containing the ROI signal and fasta sequence.</p> Source code in <code>src/capfinder/process_pod5.py</code> <pre><code>def extract_roi_signal(\n    signal: np.ndarray,\n    base_locs_in_signal: npt.NDArray[np.int32],\n    fasta: str,\n    experiment_type: str,\n    start_base_idx_in_fasta: int,\n    end_base_idx_in_fasta: int,\n    num_left_clipped_bases: int,\n) -&gt; ROIData:\n    \"\"\"\n    Extracts the signal data for a region of interest (ROI).\n\n    Params:\n        signal (np.ndarray): Signal data.\n        base_locs_in_signal (npt.NDArray[np.int32]): Array of locations of each new base in the signal.\n        fasta (str): Fasta sequence of the read.\n        experiment_type (str): Type of experiment (rna or dna).\n        start_base_idx_in_fasta (int): Index of the first base in the ROI.\n        end_base_idx_in_fasta (int): Index of the last base in the ROI.\n        num_left_clipped_bases (int): Number of bases clipped from the left.\n\n    Returns:\n        ROIData: Dictionary containing the ROI signal and fasta sequence.\n    \"\"\"\n    signal = preprocess_signal_data(signal)\n    roi_data: ROIData = {\n        \"roi_fasta\": None,\n        \"roi_signal\": np.array([], dtype=np.float64),\n        \"signal_start\": None,\n        \"signal_end\": None,\n        \"plot_signal\": signal,  # Assuming signal is defined somewhere\n        \"roi_signal_for_plot\": None,\n        \"base_locs_in_signal\": base_locs_in_signal,  # Assuming base_locs_in_signal is defined somewhere\n        \"start_base_idx_in_fasta\": None,\n        \"end_base_idx_in_fasta\": None,\n        \"read_id\": None,\n    }\n\n    # Check for valid inputs\n    if end_base_idx_in_fasta is None and start_base_idx_in_fasta is None:\n        return roi_data\n\n    if end_base_idx_in_fasta &gt; len(fasta) or start_base_idx_in_fasta &lt; 0:\n        return roi_data\n\n    if experiment_type not in (\"rna\", \"dna\"):\n        return roi_data\n\n    start_base_idx_in_fasta += num_left_clipped_bases\n    end_base_idx_in_fasta += num_left_clipped_bases\n    if experiment_type == \"rna\":\n        rev_base_locs_in_signal = base_locs_in_signal[::-1]\n        signal_end = rev_base_locs_in_signal[start_base_idx_in_fasta - 1]\n        signal_start = rev_base_locs_in_signal[end_base_idx_in_fasta - 1]\n        roi_data[\"roi_fasta\"] = fasta[\n            start_base_idx_in_fasta\n            - num_left_clipped_bases : end_base_idx_in_fasta\n            - num_left_clipped_bases\n        ]\n    else:\n        # TODO: THE LOGIC IS NOT TESTED\n        signal_start = base_locs_in_signal[\n            start_base_idx_in_fasta - 1\n        ]  # TODO: Confirm -1\n        signal_end = base_locs_in_signal[end_base_idx_in_fasta - 1]  # TODO: Confirm -1\n        roi_data[\"roi_fasta\"] = fasta[start_base_idx_in_fasta:end_base_idx_in_fasta]\n\n    # Signal data is 3'-&gt; 5' for RNA 5' -&gt; 3' for DNA\n    # The ROI FASTA is always 5' -&gt; 3' irrespective of the experiment type\n    roi_data[\"roi_signal\"] = signal[signal_start:signal_end]\n\n    # Make roi signal for plot and pad it with NaNs outside the ROI\n    plot_signal = np.copy(signal)\n    plot_signal[:signal_start] = np.nan\n    plot_signal[signal_end:] = np.nan\n    roi_data[\"signal_start\"] = signal_start\n    roi_data[\"signal_end\"] = signal_end\n    roi_data[\"roi_signal_for_plot\"] = plot_signal\n    roi_data[\"base_locs_in_signal\"] = base_locs_in_signal\n\n    return roi_data\n</code></pre>"},{"location":"api_docs/#src.capfinder.process_pod5.find_base_locs_in_signal","title":"<code>find_base_locs_in_signal(bam_data: dict) -&gt; npt.NDArray[np.int32]</code>","text":"<p>Finds the locations of each new base in the signal.</p> <p>Parameters:</p> Name Type Description Default <code>bam_data</code> <code>dict</code> <p>Dictionary containing information from the BAM file.</p> required <p>Returns:</p> Type Description <code>NDArray[int32]</code> <p>npt.NDArray[np.int32]: Array of locations of each new base in the signal.</p> Source code in <code>src/capfinder/process_pod5.py</code> <pre><code>def find_base_locs_in_signal(bam_data: dict) -&gt; npt.NDArray[np.int32]:\n    \"\"\"\n    Finds the locations of each new base in the signal.\n\n    Params:\n        bam_data (dict): Dictionary containing information from the BAM file.\n\n    Returns:\n        npt.NDArray[np.int32]: Array of locations of each new base in the signal.\n    \"\"\"\n    start_sample = bam_data[\"start_sample\"]\n    split_point = bam_data[\"split_point\"]\n\n    # we map the moves from 3' to 5' to the signal\n    # and start from the start sample or its sum with the split point\n    # if the read is split\n    start_sample = start_sample + split_point\n\n    moves_step = bam_data[\"moves_step\"]\n    moves_table = np.array(bam_data[\"moves_table\"])\n\n    # Where do moves occur in the signal coordinates?\n    moves_indices = np.arange(\n        start_sample, start_sample + moves_step * len(moves_table), moves_step\n    )\n\n    # We only need locs where a move of 1 occurs\n    base_locs_in_signal: npt.NDArray[np.int32] = moves_indices[moves_table != 0]\n\n    return base_locs_in_signal\n</code></pre>"},{"location":"api_docs/#src.capfinder.process_pod5.preprocess_signal_data","title":"<code>preprocess_signal_data(signal: np.ndarray) -&gt; npt.NDArray[np.float64]</code>","text":"<p>Preprocesses the signal data.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>ndarray</code> <p>Signal data.</p> required <p>Returns:</p> Name Type Description <code>signal</code> <code>NDArray[float64]</code> <p>Preprocessed signal data.</p> Source code in <code>src/capfinder/process_pod5.py</code> <pre><code>def preprocess_signal_data(signal: np.ndarray) -&gt; npt.NDArray[np.float64]:\n    \"\"\"\n    Preprocesses the signal data.\n\n    Params:\n        signal (np.ndarray): Signal data.\n\n    Returns:\n        signal (npt.NDArray[np.float64]): Preprocessed signal data.\n    \"\"\"\n    signal = z_normalize(signal)\n    signal = clip_extreme_values(signal)\n    return signal\n</code></pre>"},{"location":"api_docs/#src.capfinder.process_pod5.pull_read_from_pod5","title":"<code>pull_read_from_pod5(read_id: str, pod5_filepath: str) -&gt; Dict[str, Any]</code>","text":"<p>Returns a single read from a pod5 file.</p> <p>Parameters:</p> Name Type Description Default <code>read_id</code> <code>str</code> <p>str The read_id of the read to be extracted.</p> required <code>pod5_filepath</code> <code>str</code> <p>str Path to the pod5 file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>Dictionary containing information about the extracted read. - 'sample_rate': Sample rate of the read. - 'sequencing_kit': Sequencing kit used. - 'experiment_type': Experiment type. - 'local_basecalling': Local basecalling information. - 'signal': Signal data. - 'signal_pa': Signal data for the positive strand. - 'end_reason': Reason for the end of the read. - 'sample_count': Number of samples in the read. - 'channel': Pore channel information. - 'well': Pore well information. - 'pore_type': Pore type. - 'writing_software': Software used for writing. - 'scale': Scaling factor for the signal. - 'shift': Shift factor for the signal.</p> Source code in <code>src/capfinder/process_pod5.py</code> <pre><code>def pull_read_from_pod5(read_id: str, pod5_filepath: str) -&gt; Dict[str, Any]:\n    \"\"\"Returns a single read from a pod5 file.\n\n    Params:\n        read_id: str\n            The read_id of the read to be extracted.\n        pod5_filepath: str\n            Path to the pod5 file.\n\n    Returns:\n        dict: Dictionary containing information about the extracted read.\n            - 'sample_rate': Sample rate of the read.\n            - 'sequencing_kit': Sequencing kit used.\n            - 'experiment_type': Experiment type.\n            - 'local_basecalling': Local basecalling information.\n            - 'signal': Signal data.\n            - 'signal_pa': Signal data for the positive strand.\n            - 'end_reason': Reason for the end of the read.\n            - 'sample_count': Number of samples in the read.\n            - 'channel': Pore channel information.\n            - 'well': Pore well information.\n            - 'pore_type': Pore type.\n            - 'writing_software': Software used for writing.\n            - 'scale': Scaling factor for the signal.\n            - 'shift': Shift factor for the signal.\n\n    \"\"\"\n    signal_dict = {}\n    with p5.Reader(pod5_filepath) as reader:\n        read = next(reader.reads(selection=[read_id]))\n        # Get the signal data and sample rate\n        signal_dict[\"sample_rate\"] = read.run_info.sample_rate\n        signal_dict[\"sequencing_kit\"] = read.run_info.sequencing_kit\n        signal_dict[\"experiment_type\"] = read.run_info.context_tags[\"experiment_type\"]\n        signal_dict[\"local_basecalling\"] = read.run_info.context_tags[\n            \"local_basecalling\"\n        ]\n        signal_dict[\"signal\"] = read.signal\n        signal_dict[\"signal_pa\"] = read.signal_pa\n        signal_dict[\"end_reason\"] = read.end_reason.reason.name\n        signal_dict[\"sample_count\"] = read.sample_count\n        signal_dict[\"channel\"] = read.pore.channel\n        signal_dict[\"well\"] = read.pore.well\n        signal_dict[\"pore_type\"] = read.pore.pore_type\n        signal_dict[\"writing_software\"] = reader.writing_software\n        signal_dict[\"scale\"] = read.tracked_scaling.scale\n        signal_dict[\"shift\"] = read.tracked_scaling.shift\n    return signal_dict\n</code></pre>"},{"location":"api_docs/#src.capfinder.process_pod5.z_normalize","title":"<code>z_normalize(data: np.ndarray) -&gt; npt.NDArray[np.float64]</code>","text":"<p>Normalize the input data using Z-score normalization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Input data to be Z-score normalized.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>npt.NDArray[np.float64]: Z-score normalized data.</p> Note <p>Z-score normalization (or Z normalization) transforms the data to have a mean of 0 and a standard deviation of 1.</p> Source code in <code>src/capfinder/process_pod5.py</code> <pre><code>def z_normalize(data: np.ndarray) -&gt; npt.NDArray[np.float64]:\n    \"\"\"Normalize the input data using Z-score normalization.\n\n    Params:\n        data (np.ndarray): Input data to be Z-score normalized.\n\n    Returns:\n        npt.NDArray[np.float64]: Z-score normalized data.\n\n    Note:\n        Z-score normalization (or Z normalization) transforms the data\n        to have a mean of 0 and a standard deviation of 1.\n    \"\"\"\n    mean = np.mean(data)\n    std_dev = np.std(data)\n    z_normalized_data: npt.NDArray[np.float64] = (data - mean) / std_dev\n    return z_normalized_data\n</code></pre>"},{"location":"api_docs/#src.capfinder.report","title":"<code>report</code>","text":""},{"location":"api_docs/#src.capfinder.report.count_csv_rows","title":"<code>count_csv_rows(csv_file: str) -&gt; int</code>","text":"<p>Count the number of rows in a CSV file.</p> Source code in <code>src/capfinder/report.py</code> <pre><code>def count_csv_rows(csv_file: str) -&gt; int:\n    \"\"\"Count the number of rows in a CSV file.\"\"\"\n    with open(csv_file) as f:\n        return sum(1 for _ in f) - 1  # Subtract 1 to account for header\n</code></pre>"},{"location":"api_docs/#src.capfinder.report.create_database","title":"<code>create_database(db_path: str) -&gt; sqlite3.Connection</code>","text":"<p>Create a new SQLite database and return the connection.</p> Source code in <code>src/capfinder/report.py</code> <pre><code>def create_database(db_path: str) -&gt; sqlite3.Connection:\n    \"\"\"Create a new SQLite database and return the connection.\"\"\"\n    conn = sqlite3.connect(db_path)\n    return conn\n</code></pre>"},{"location":"api_docs/#src.capfinder.report.create_table","title":"<code>create_table(conn: sqlite3.Connection, table_name: str, columns: List[str]) -&gt; None</code>","text":"<p>Create a table in the SQLite database.</p> Source code in <code>src/capfinder/report.py</code> <pre><code>def create_table(conn: sqlite3.Connection, table_name: str, columns: List[str]) -&gt; None:\n    \"\"\"Create a table in the SQLite database.\"\"\"\n    cursor = conn.cursor()\n    columns_str = \", \".join(columns)\n    cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns_str})\")\n    conn.commit()\n</code></pre>"},{"location":"api_docs/#src.capfinder.report.csv_to_sqlite","title":"<code>csv_to_sqlite(csv_file: str, db_conn: sqlite3.Connection, table_name: str, chunk_size: int = 100000) -&gt; None</code>","text":"<p>Import CSV data into SQLite database in chunks with progress bar.</p> Source code in <code>src/capfinder/report.py</code> <pre><code>def csv_to_sqlite(\n    csv_file: str,\n    db_conn: sqlite3.Connection,\n    table_name: str,\n    chunk_size: int = 100000,\n) -&gt; None:\n    \"\"\"Import CSV data into SQLite database in chunks with progress bar.\"\"\"\n    create_table(\n        db_conn,\n        table_name,\n        [\"read_id TEXT PRIMARY KEY\", \"pod5_file TEXT\", \"predicted_cap TEXT\"],\n    )\n\n    cursor = db_conn.cursor()\n    total_rows = count_csv_rows(csv_file)\n\n    with open(csv_file) as f:\n        csv_reader = csv.DictReader(f)\n        chunk = []\n        with tqdm(total=total_rows, unit=\"reads\") as pbar:\n            for rw in csv_reader:\n                chunk.append(\n                    (\n                        rw[\"read_id\"],\n                        rw.get(\"pod5_file\", \"\"),\n                        rw.get(\"predicted_cap\", \"\"),\n                    )\n                )\n                if len(chunk) &gt;= chunk_size:\n                    cursor.executemany(\n                        f\"INSERT OR REPLACE INTO {table_name} (read_id, pod5_file, predicted_cap) VALUES (?, ?, ?)\",\n                        chunk,\n                    )\n                    db_conn.commit()\n                    pbar.update(len(chunk))\n                    chunk = []\n\n            if chunk:  # Insert any remaining rows\n                cursor.executemany(\n                    f\"INSERT OR REPLACE INTO {table_name} (read_id, pod5_file, predicted_cap) VALUES (?, ?, ?)\",\n                    chunk,\n                )\n                db_conn.commit()\n                pbar.update(len(chunk))\n</code></pre>"},{"location":"api_docs/#src.capfinder.report.get_cap_type_counts","title":"<code>get_cap_type_counts(conn: sqlite3.Connection) -&gt; DefaultDict[str, int]</code>","text":"<p>Get cap type counts from the joined data.</p> Source code in <code>src/capfinder/report.py</code> <pre><code>def get_cap_type_counts(conn: sqlite3.Connection) -&gt; DefaultDict[str, int]:\n    \"\"\"Get cap type counts from the joined data.\"\"\"\n    query = \"\"\"\n    SELECT COALESCE(predicted_cap, 'OTE_not_found') as cap, COUNT(*) as count\n    FROM (\n        SELECT m.read_id, COALESCE(p.predicted_cap, 'OTE_not_found') as predicted_cap\n        FROM metadata m\n        LEFT JOIN predictions p ON m.read_id = p.read_id\n    )\n    GROUP BY cap\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(query)\n\n    results = defaultdict(int)\n    for cap, count in cursor:\n        results[cap] = count\n\n    return results\n</code></pre>"},{"location":"api_docs/#src.capfinder.report.join_tables","title":"<code>join_tables(conn: sqlite3.Connection, output_csv: str, chunk_size: int = 100000) -&gt; None</code>","text":"<p>Join metadata and predictions tables and save to CSV in chunks with progress bar.</p> Source code in <code>src/capfinder/report.py</code> <pre><code>def join_tables(\n    conn: sqlite3.Connection, output_csv: str, chunk_size: int = 100000\n) -&gt; None:\n    \"\"\"Join metadata and predictions tables and save to CSV in chunks with progress bar.\"\"\"\n    query = \"\"\"\n    SELECT m.read_id, m.pod5_file, COALESCE(p.predicted_cap, 'OTE_not_found') as predicted_cap\n    FROM metadata m\n    LEFT JOIN predictions p ON m.read_id = p.read_id\n    \"\"\"\n\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM metadata\")\n    total_rows = cursor.fetchone()[0]\n\n    cursor.execute(query)\n\n    with open(output_csv, \"w\", newline=\"\") as f:\n        csv_writer = csv.writer(f)\n        csv_writer.writerow([\"read_id\", \"pod5_file\", \"predicted_cap\"])  # Write header\n\n        with tqdm(total=total_rows, unit=\"reads\") as pbar:\n            while True:\n                results = cursor.fetchmany(chunk_size)\n                if not results:\n                    break\n                csv_writer.writerows(results)\n                pbar.update(len(results))\n</code></pre>"},{"location":"api_docs/#src.capfinder.resnet_model","title":"<code>resnet_model</code>","text":""},{"location":"api_docs/#src.capfinder.resnet_model.ResNetTimeSeriesHyper","title":"<code>ResNetTimeSeriesHyper</code>","text":"<p>               Bases: <code>HyperModel</code></p> <p>A HyperModel class for building a ResNet-style neural network for time series classification.</p> <p>This class defines a tunable ResNet architecture that can be optimized using Keras Tuner. It creates a model with an initial convolutional layer, followed by a variable number of ResNet blocks, and ends with global average pooling and dense layers.</p> <p>Attributes:</p> Name Type Description <code>input_shape</code> <code>Tuple[int, int]</code> <p>The shape of the input data (timesteps, features).</p> <code>n_classes</code> <code>int</code> <p>The number of classes for classification.</p> <p>Methods:</p> Name Description <code>build</code> <p>Builds and returns a compiled Keras model based on the provided hyperparameters.</p> Source code in <code>src/capfinder/resnet_model.py</code> <pre><code>class ResNetTimeSeriesHyper(HyperModel):\n    \"\"\"\n    A HyperModel class for building a ResNet-style neural network for time series classification.\n\n    This class defines a tunable ResNet architecture that can be optimized using Keras Tuner.\n    It creates a model with an initial convolutional layer, followed by a variable number of\n    ResNet blocks, and ends with global average pooling and dense layers.\n\n    Attributes:\n        input_shape (Tuple[int, int]): The shape of the input data (timesteps, features).\n        n_classes (int): The number of classes for classification.\n\n    Methods:\n        build(hp): Builds and returns a compiled Keras model based on the provided hyperparameters.\n    \"\"\"\n\n    def __init__(self, input_shape: Tuple[int, int], n_classes: int):\n        self.input_shape = input_shape\n        self.n_classes = n_classes\n        self.encoder_model = None\n\n    def build(self, hp: HyperParameters) -&gt; Model:\n        \"\"\"\n        Build and compile a ResNet model based on the provided hyperparameters.\n\n        This method constructs a ResNet architecture with tunable hyperparameters including\n        the number of filters, kernel sizes, number of ResNet blocks, dense layer units,\n        dropout rate, and learning rate.\n\n        Args:\n            hp (hp.HyperParameters): A HyperParameters object used to define the search space.\n\n        Returns:\n            Model: A compiled Keras model ready for training.\n        \"\"\"\n        inputs = keras.Input(shape=self.input_shape)\n\n        # Initial convolution\n        initial_filters = hp.Int(\n            \"initial_filters\", min_value=32, max_value=128, step=32\n        )\n        x = layers.Conv1D(\n            initial_filters,\n            kernel_size=hp.Choice(\"initial_kernel\", values=[3, 5, 7]),\n            padding=\"same\",\n        )(inputs)\n        x = layers.BatchNormalization()(x)\n        x = keras.activations.relu(x)\n        x = layers.MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n\n        # ResNet blocks\n        num_blocks_per_stage = hp.Int(\"num_blocks_per_stage\", min_value=2, max_value=4)\n        num_stages = hp.Int(\"num_stages\", min_value=2, max_value=4)\n\n        for stage in range(num_stages):\n            filters = hp.Int(\n                f\"filters_stage_{stage}\", min_value=64, max_value=256, step=64\n            )\n            for block in range(num_blocks_per_stage):\n                kernel_size = hp.Choice(\n                    f\"kernel_stage_{stage}_block_{block}\", values=[3, 5, 7]\n                )\n                strides = 2 if block == 0 and stage &gt; 0 else 1\n                x = ResNetBlockHyper(filters, kernel_size, strides)(x)\n\n        # Global pooling and output\n        x = layers.GlobalAveragePooling1D()(x)\n        x = layers.Dense(\n            hp.Int(\"dense_units\", min_value=32, max_value=256, step=32),\n            activation=\"relu\",\n        )(x)\n        x = layers.Dropout(hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1))(\n            x\n        )\n        outputs = layers.Dense(self.n_classes, activation=\"softmax\")(x)\n\n        model = Model(inputs, outputs)\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(\n                hp.Float(\n                    \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n                )\n            ),\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"sparse_categorical_accuracy\"],\n        )\n\n        return model\n</code></pre>"},{"location":"api_docs/#src.capfinder.resnet_model.ResNetTimeSeriesHyper.build","title":"<code>build(hp: HyperParameters) -&gt; Model</code>","text":"<p>Build and compile a ResNet model based on the provided hyperparameters.</p> <p>This method constructs a ResNet architecture with tunable hyperparameters including the number of filters, kernel sizes, number of ResNet blocks, dense layer units, dropout rate, and learning rate.</p> <p>Parameters:</p> Name Type Description Default <code>hp</code> <code>HyperParameters</code> <p>A HyperParameters object used to define the search space.</p> required <p>Returns:</p> Name Type Description <code>Model</code> <code>Model</code> <p>A compiled Keras model ready for training.</p> Source code in <code>src/capfinder/resnet_model.py</code> <pre><code>def build(self, hp: HyperParameters) -&gt; Model:\n    \"\"\"\n    Build and compile a ResNet model based on the provided hyperparameters.\n\n    This method constructs a ResNet architecture with tunable hyperparameters including\n    the number of filters, kernel sizes, number of ResNet blocks, dense layer units,\n    dropout rate, and learning rate.\n\n    Args:\n        hp (hp.HyperParameters): A HyperParameters object used to define the search space.\n\n    Returns:\n        Model: A compiled Keras model ready for training.\n    \"\"\"\n    inputs = keras.Input(shape=self.input_shape)\n\n    # Initial convolution\n    initial_filters = hp.Int(\n        \"initial_filters\", min_value=32, max_value=128, step=32\n    )\n    x = layers.Conv1D(\n        initial_filters,\n        kernel_size=hp.Choice(\"initial_kernel\", values=[3, 5, 7]),\n        padding=\"same\",\n    )(inputs)\n    x = layers.BatchNormalization()(x)\n    x = keras.activations.relu(x)\n    x = layers.MaxPooling1D(pool_size=3, strides=2, padding=\"same\")(x)\n\n    # ResNet blocks\n    num_blocks_per_stage = hp.Int(\"num_blocks_per_stage\", min_value=2, max_value=4)\n    num_stages = hp.Int(\"num_stages\", min_value=2, max_value=4)\n\n    for stage in range(num_stages):\n        filters = hp.Int(\n            f\"filters_stage_{stage}\", min_value=64, max_value=256, step=64\n        )\n        for block in range(num_blocks_per_stage):\n            kernel_size = hp.Choice(\n                f\"kernel_stage_{stage}_block_{block}\", values=[3, 5, 7]\n            )\n            strides = 2 if block == 0 and stage &gt; 0 else 1\n            x = ResNetBlockHyper(filters, kernel_size, strides)(x)\n\n    # Global pooling and output\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(\n        hp.Int(\"dense_units\", min_value=32, max_value=256, step=32),\n        activation=\"relu\",\n    )(x)\n    x = layers.Dropout(hp.Float(\"dropout\", min_value=0.0, max_value=0.5, step=0.1))(\n        x\n    )\n    outputs = layers.Dense(self.n_classes, activation=\"softmax\")(x)\n\n    model = Model(inputs, outputs)\n\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Float(\n                \"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n            )\n        ),\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"sparse_categorical_accuracy\"],\n    )\n\n    return model\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl","title":"<code>train_etl</code>","text":""},{"location":"api_docs/#src.capfinder.train_etl.augment_example","title":"<code>augment_example(x: tf.Tensor, y: tf.Tensor, dtype: tf.DType) -&gt; tf.data.Dataset</code>","text":"<p>Augment a single example by creating warped versions and combining them with the original.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be augmented.</p> required <code>y</code> <code>Tensor</code> <p>The corresponding label tensor.</p> required <code>dtype</code> <code>DType</code> <p>The desired data type for the augmented tensors.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>tf.data.Dataset: A dataset containing the original and augmented examples with their labels.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def augment_example(x: tf.Tensor, y: tf.Tensor, dtype: tf.DType) -&gt; tf.data.Dataset:\n    \"\"\"\n    Augment a single example by creating warped versions and combining them with the original.\n\n    Args:\n        x (tf.Tensor): The input tensor to be augmented.\n        y (tf.Tensor): The corresponding label tensor.\n        dtype (tf.DType): The desired data type for the augmented tensors.\n\n    Returns:\n        tf.data.Dataset: A dataset containing the original and augmented examples with their labels.\n    \"\"\"\n    # Apply augmentation to each example in the batch\n    squished, expanded = create_warped_examples(x, 0.2, dtype=dtype)\n\n    # Ensure all tensors have the same data type\n    x = tf.cast(x, dtype)\n    squished = tf.cast(squished, dtype)\n    expanded = tf.cast(expanded, dtype)\n\n    # Create a list of augmented examples\n    augmented_x = [x, squished, expanded]\n    augmented_y = [y, y, y]\n\n    return tf.data.Dataset.from_tensor_slices((augmented_x, augmented_y))\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.calculate_sizes","title":"<code>calculate_sizes(total_examples: int, train_fraction: float, batch_size: int) -&gt; Tuple[int, int]</code>","text":"<p>Compute the train and validation sizes based on the total number of examples.</p> <p>Parameters:</p> Name Type Description Default <code>total_examples</code> <code>int</code> <p>Total number of examples in the dataset.</p> required <code>train_fraction</code> <code>float</code> <p>Fraction of data to use for training.</p> required <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <p>Returns:</p> Type Description <code>Tuple[int, int]</code> <p>Tuple[int, int]: Train size and validation size, both divisible by batch_size.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def calculate_sizes(\n    total_examples: int, train_fraction: float, batch_size: int\n) -&gt; Tuple[int, int]:\n    \"\"\"\n    Compute the train and validation sizes based on the total number of examples.\n\n    Args:\n        total_examples (int): Total number of examples in the dataset.\n        train_fraction (float): Fraction of data to use for training.\n        batch_size (int): Size of each batch.\n\n    Returns:\n        Tuple[int, int]: Train size and validation size, both divisible by batch_size.\n    \"\"\"\n    train_size = int(total_examples * train_fraction)\n    val_size = total_examples - train_size\n\n    train_size = (train_size // batch_size) * batch_size\n    val_size = (val_size // batch_size) * batch_size\n\n    while train_size + val_size &gt; total_examples:\n        if train_size &gt; val_size:\n            train_size -= batch_size\n        else:\n            val_size -= batch_size\n\n    return train_size, val_size\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.combine_datasets","title":"<code>combine_datasets(datasets: List[tf.data.Dataset]) -&gt; tf.data.Dataset</code>","text":"<p>Combine datasets from different classes.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>List[Dataset]</code> <p>List of datasets to combine.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>tf.data.Dataset: A combined dataset.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def combine_datasets(datasets: List[tf.data.Dataset]) -&gt; tf.data.Dataset:\n    \"\"\"\n    Combine datasets from different classes.\n\n    Args:\n        datasets (List[tf.data.Dataset]): List of datasets to combine.\n\n    Returns:\n        tf.data.Dataset: A combined dataset.\n    \"\"\"\n    combined_dataset = datasets[0]\n    for dataset in datasets[1:]:\n        combined_dataset = combined_dataset.concatenate(dataset)\n    return combined_dataset.shuffle(buffer_size=10000)\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.count_examples_fast","title":"<code>count_examples_fast(file_path: str) -&gt; int</code>","text":"<p>Count lines in a file using fast bash utilities, falling back to Python if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file to count lines in.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of lines in the file (excluding header).</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def count_examples_fast(file_path: str) -&gt; int:\n    \"\"\"\n    Count lines in a file using fast bash utilities, falling back to Python if necessary.\n\n    Args:\n        file_path (str): Path to the file to count lines in.\n\n    Returns:\n        int: Number of lines in the file (excluding header).\n    \"\"\"\n    try:\n        # Try using wc -l command (fast)\n        result = subprocess.run([\"wc\", \"-l\", file_path], capture_output=True, text=True)\n        count = int(result.stdout.split()[0]) - 1  # Subtract 1 for header\n        return count\n    except (subprocess.SubprocessError, FileNotFoundError, ValueError):\n        try:\n            # Fallback to using sed and wc (slightly slower, but still fast)\n            result = subprocess.run(\n                f\"sed '1d' {file_path} | wc -l\",\n                shell=True,\n                capture_output=True,\n                text=True,\n            )\n            return int(result.stdout.strip())\n        except (subprocess.SubprocessError, ValueError):\n            # If bash methods fail, fall back to Python method\n            return count_examples_python(file_path)\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.count_examples_python","title":"<code>count_examples_python(file_path: str) -&gt; int</code>","text":"<p>Count lines in a file using Python (slower but portable).</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the file to count lines in.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of lines in the file (excluding header).</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def count_examples_python(file_path: str) -&gt; int:\n    \"\"\"\n    Count lines in a file using Python (slower but portable).\n\n    Args:\n        file_path (str): Path to the file to count lines in.\n\n    Returns:\n        int: Number of lines in the file (excluding header).\n    \"\"\"\n    with open(file_path) as f:\n        return sum(1 for _ in f) - 1  # Subtract 1 for header\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.create_class_dataset","title":"<code>create_class_dataset(file_paths: List[str], target_length: int, dtype: DtypeLiteral, examples_per_class: int, train_test_fraction: float) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]</code>","text":"<p>Create a dataset for a single class from multiple files.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>List[str]</code> <p>List of file paths for a single class.</p> required <code>target_length</code> <code>int</code> <p>The desired length of the timeseries tensor.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>The desired data type for the timeseries tensor as a string.</p> required <code>examples_per_class</code> <code>int</code> <p>Number of examples to take per class.</p> required <code>train_test_fraction</code> <code>float</code> <p>Fraction of data to use for training.</p> required <p>Returns:</p> Type Description <code>Tuple[Dataset, Dataset]</code> <p>Tuple[tf.data.Dataset, tf.data.Dataset]: Train and test datasets for the given class.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def create_class_dataset(\n    file_paths: List[str],\n    target_length: int,\n    dtype: DtypeLiteral,\n    examples_per_class: int,\n    train_test_fraction: float,\n) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]:\n    \"\"\"\n    Create a dataset for a single class from multiple files.\n\n    Args:\n        file_paths (List[str]): List of file paths for a single class.\n        target_length (int): The desired length of the timeseries tensor.\n        dtype (DtypeLiteral): The desired data type for the timeseries tensor as a string.\n        examples_per_class (int): Number of examples to take per class.\n        train_test_fraction (float): Fraction of data to use for training.\n\n    Returns:\n        Tuple[tf.data.Dataset, tf.data.Dataset]: Train and test datasets for the given class.\n    \"\"\"\n    class_dataset: Optional[tf.data.Dataset] = None\n\n    for file_path in file_paths:\n        dataset = create_dataset(file_path, target_length, dtype)\n\n        if class_dataset is None:\n            class_dataset = dataset\n        else:\n            class_dataset = class_dataset.concatenate(dataset)\n\n    if class_dataset is None:\n        raise ValueError(\"No valid datasets were created.\")\n\n    # Shuffle and take examples after concatenating all files\n    class_dataset = class_dataset.shuffle(buffer_size=10000).take(examples_per_class)\n\n    # Split into train and test\n    train_size = int(train_test_fraction * examples_per_class)\n    train_dataset = class_dataset.take(train_size)\n    test_dataset = class_dataset.skip(train_size)\n    return train_dataset, test_dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.create_dataset","title":"<code>create_dataset(file_path: str, target_length: int, dtype: DtypeLiteral) -&gt; tf.data.Dataset</code>","text":"<p>Create a TensorFlow dataset for a single class CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required <code>target_length</code> <code>int</code> <p>The desired length of the timeseries tensor.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>The desired data type for the timeseries tensor as a string.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>tf.data.Dataset: A dataset for the given class.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def create_dataset(\n    file_path: str,\n    target_length: int,\n    dtype: DtypeLiteral,\n) -&gt; tf.data.Dataset:\n    \"\"\"\n    Create a TensorFlow dataset for a single class CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file.\n        target_length (int): The desired length of the timeseries tensor.\n        dtype (DtypeLiteral): The desired data type for the timeseries tensor as a string.\n\n    Returns:\n        tf.data.Dataset: A dataset for the given class.\n    \"\"\"\n    tf_dtype = get_dtype(dtype)\n    dataset = tf.data.Dataset.from_generator(\n        lambda: csv_generator(file_path),\n        output_signature=(\n            tf.TensorSpec(shape=(), dtype=tf.string),\n            tf.TensorSpec(shape=(), dtype=tf.string),\n            tf.TensorSpec(shape=(), dtype=tf.string),\n        ),\n    )\n    dataset = dataset.map(\n        lambda x, y, z: parse_row((x, y, z), target_length, tf_dtype),\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    return dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.create_train_val_test_datasets_from_train_test_csvs","title":"<code>create_train_val_test_datasets_from_train_test_csvs(dataset_dir: str, batch_size: int, target_length: int, dtype: tf.DType, train_val_fraction: float, use_augmentation: bool = False) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int]</code>","text":"<p>Load ready-made train, validation, and test datasets from CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>Directory containing the CSV files.</p> required <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <code>target_length</code> <code>int</code> <p>Target length of each time series.</p> required <code>dtype</code> <code>DType</code> <p>Data type for the features.</p> required <code>train_val_fraction</code> <code>float</code> <p>Fraction of training data to use for validation.</p> required <code>use_augmentation</code> <code>bool</code> <p>Whether to augment original training examples with warped versions</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int]:</p> <code>Dataset</code> <p>Train dataset, validation dataset, test dataset, steps per epoch, and validation steps.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def create_train_val_test_datasets_from_train_test_csvs(\n    dataset_dir: str,\n    batch_size: int,\n    target_length: int,\n    dtype: tf.DType,\n    train_val_fraction: float,\n    use_augmentation: bool = False,\n) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int]:\n    \"\"\"\n    Load ready-made train, validation, and test datasets from CSV files.\n\n    Args:\n        dataset_dir (str): Directory containing the CSV files.\n        batch_size (int): Size of each batch.\n        target_length (int): Target length of each time series.\n        dtype (tf.DType): Data type for the features.\n        train_val_fraction (float): Fraction of training data to use for validation.\n        use_augmentation (bool): Whether to augment original training examples with warped versions\n\n    Returns:\n        Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int]:\n        Train dataset, validation dataset, test dataset, steps per epoch, and validation steps.\n    \"\"\"\n    logger.info(\"Loading train, val splits...\")\n\n    train_dataset, val_dataset, steps_per_epoch, validation_steps = (\n        load_train_dataset_from_csvs(\n            x_file_path=os.path.join(dataset_dir, \"train_x.csv\"),\n            y_file_path=os.path.join(dataset_dir, \"train_y.csv\"),\n            batch_size=batch_size,\n            target_length=target_length,\n            dtype=dtype,\n            train_val_fraction=train_val_fraction,\n            use_augmentation=use_augmentation,\n        )\n    )\n    logger.info(\"Loading test split ...\")\n\n    test_dataset = load_test_dataset_from_csvs(\n        x_file_path=os.path.join(dataset_dir, \"test_x.csv\"),\n        y_file_path=os.path.join(dataset_dir, \"test_y.csv\"),\n        batch_size=batch_size,\n        target_length=target_length,\n        dtype=dtype,\n    )\n    return train_dataset, val_dataset, test_dataset, steps_per_epoch, validation_steps\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.create_warped_examples","title":"<code>create_warped_examples(signal: tf.Tensor, max_warp_factor: float = 0.3, dtype: tf.DType = tf.float32) -&gt; Tuple[tf.Tensor, tf.Tensor]</code>","text":"<p>Create warped versions (squished and expanded) of the input signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>Tensor</code> <p>The input signal to be warped.</p> required <code>max_warp_factor</code> <code>float</code> <p>The maximum factor by which the signal can be warped. Defaults to 0.3.</p> <code>0.3</code> <code>dtype</code> <code>DType</code> <p>The desired data type for the output tensors. Defaults to tf.float32.</p> <code>float32</code> <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Tuple[tf.Tensor, tf.Tensor]: A tuple containing the squished and expanded versions of the input signal.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def create_warped_examples(\n    signal: tf.Tensor, max_warp_factor: float = 0.3, dtype: tf.DType = tf.float32\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n    \"\"\"\n    Create warped versions (squished and expanded) of the input signal.\n\n    Args:\n        signal (tf.Tensor): The input signal to be warped.\n        max_warp_factor (float): The maximum factor by which the signal can be warped. Defaults to 0.3.\n        dtype (tf.DType): The desired data type for the output tensors. Defaults to tf.float32.\n\n    Returns:\n        Tuple[tf.Tensor, tf.Tensor]: A tuple containing the squished and expanded versions of the input signal.\n    \"\"\"\n    original_dtype = signal.dtype\n    signal = tf.cast(signal, tf.float32)  # Convert to float32 for internal calculations\n\n    time_steps = tf.shape(signal)[0]\n\n    # Create squished version\n    squish_factor = 1 - tf.random.uniform((), 0, max_warp_factor, seed=43)\n    squished_length = tf.cast(tf.cast(time_steps, tf.float32) * squish_factor, tf.int32)\n    squished = tf.image.resize(tf.expand_dims(signal, -1), (squished_length, 1))[\n        :, :, 0\n    ]\n    pad_total = time_steps - squished_length\n    pad_left = pad_total // 2\n    pad_right = pad_total - pad_left\n    padding = [[pad_left, pad_right], [0, 0]]\n    squished = tf.pad(squished, padding)\n\n    # Create expanded version\n    expand_factor = 1 + tf.random.uniform((), 0, max_warp_factor, seed=43)\n    expanded_length = tf.cast(tf.cast(time_steps, tf.float32) * expand_factor, tf.int32)\n    expanded = tf.image.resize(tf.expand_dims(signal, -1), (expanded_length, 1))[\n        :, :, 0\n    ]\n    trim_total = expanded_length - time_steps\n    trim_left = trim_total // 2\n    trim_right = expanded_length - (trim_total - trim_left)\n    expanded = expanded[trim_left:trim_right]\n\n    # Cast back to original dtype\n    squished = tf.cast(squished, original_dtype)\n    expanded = tf.cast(expanded, original_dtype)\n\n    return squished, expanded\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.csv_generator","title":"<code>csv_generator(file_path: str) -&gt; Generator[Tuple[str, str, str], None, None]</code>","text":"<p>Generates rows from a CSV file one at a time.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Yields:</p> Type Description <code>str</code> <p>Tuple[str, str, str]: A tuple containing read_id, cap_class, and timeseries as strings.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def csv_generator(file_path: str) -&gt; Generator[Tuple[str, str, str], None, None]:\n    \"\"\"\n    Generates rows from a CSV file one at a time.\n\n    Args:\n        file_path (str): Path to the CSV file.\n\n    Yields:\n        Tuple[str, str, str]: A tuple containing read_id, cap_class, and timeseries as strings.\n    \"\"\"\n    with open(file_path) as csvfile:\n        reader = csv.reader(csvfile)\n        # Skip the header row\n        next(reader)\n        for row in reader:\n            yield (str(row[0]), str(row[1]), str(row[2]))\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.get_class_from_file","title":"<code>get_class_from_file(file_path: str) -&gt; int</code>","text":"<p>Read the first data row from a CSV file and return the class ID.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the CSV file.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Class ID from the first data row.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def get_class_from_file(file_path: str) -&gt; int:\n    \"\"\"\n    Read the first data row from a CSV file and return the class ID.\n\n    Args:\n        file_path (str): Path to the CSV file.\n\n    Returns:\n        int: Class ID from the first data row.\n    \"\"\"\n    with open(file_path) as f:\n        csv_reader = csv.reader(f)\n        next(csv_reader)  # Skip header\n        first_row = next(csv_reader)\n        return int(first_row[1])  # Assuming cap_class is the second column\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.get_local_dataset_version","title":"<code>get_local_dataset_version(dataset_dir: str) -&gt; Optional[str]</code>","text":"<p>Get the version of the local dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>The directory containing the dataset.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The version of the local dataset, or None if not found.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def get_local_dataset_version(dataset_dir: str) -&gt; Optional[str]:\n    \"\"\"\n    Get the version of the local dataset.\n\n    Args:\n        dataset_dir (str): The directory containing the dataset.\n\n    Returns:\n        Optional[str]: The version of the local dataset, or None if not found.\n    \"\"\"\n    stored_version = None\n    version_file = os.path.join(dataset_dir, \"artifact_version.txt\")\n    train_x_file = os.path.join(dataset_dir, \"train_x.csv\")\n    train_y_file = os.path.join(dataset_dir, \"train_y.csv\")\n    test_x_file = os.path.join(dataset_dir, \"test_x.csv\")\n    test_y_file = os.path.join(dataset_dir, \"test_y.csv\")\n    train_exists = os.path.exists(train_x_file) and os.path.exists(train_y_file)\n    test_exists = os.path.exists(test_x_file) and os.path.exists(test_y_file)\n    version_file_exists = os.path.exists(version_file)\n\n    if train_exists and test_exists and version_file_exists:\n        with open(version_file) as f:\n            stored_version = f.read().strip()\n    return stored_version\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.group_files_by_class","title":"<code>group_files_by_class(caps_data_dir: str) -&gt; Dict[int, List[str]]</code>","text":"<p>Group CSV files in the directory by their class ID.</p> <p>Parameters:</p> Name Type Description Default <code>caps_data_dir</code> <code>str</code> <p>Directory containing the CSV files.</p> required <p>Returns:</p> Type Description <code>Dict[int, List[str]]</code> <p>Dict[int, List[str]]: Dictionary mapping class IDs to lists of file paths.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def group_files_by_class(caps_data_dir: str) -&gt; Dict[int, List[str]]:\n    \"\"\"\n    Group CSV files in the directory by their class ID.\n\n    Args:\n        caps_data_dir (str): Directory containing the CSV files.\n\n    Returns:\n        Dict[int, List[str]]: Dictionary mapping class IDs to lists of file paths.\n    \"\"\"\n    class_files: Dict[int, List[str]] = defaultdict(list)\n    for file in os.listdir(caps_data_dir):\n        if file.endswith(\".csv\"):\n            file_path = os.path.join(caps_data_dir, file)\n            try:\n                class_id = get_class_from_file(file_path)\n                class_files[class_id].append(file_path)\n            except Exception as e:\n                logger.warning(\n                    f\"Couldn't determine class for file {file}. Error: {str(e)}\"\n                )\n    return class_files\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.interleave_class_datasets","title":"<code>interleave_class_datasets(class_datasets: List[tf.data.Dataset], num_classes: int) -&gt; tf.data.Dataset</code>","text":"<p>Interleave datasets from different classes to ensure class balance.</p> <p>Parameters:</p> Name Type Description Default <code>class_datasets</code> <code>List[Dataset]</code> <p>List of datasets, one for each class.</p> required <code>num_classes</code> <code>int</code> <p>The number of classes in the dataset.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>tf.data.Dataset: An interleaved dataset with balanced class representation.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def interleave_class_datasets(\n    class_datasets: List[tf.data.Dataset],\n    num_classes: int,\n) -&gt; tf.data.Dataset:\n    \"\"\"\n    Interleave datasets from different classes to ensure class balance.\n\n    Args:\n        class_datasets (List[tf.data.Dataset]): List of datasets, one for each class.\n        num_classes (int): The number of classes in the dataset.\n\n    Returns:\n        tf.data.Dataset: An interleaved dataset with balanced class representation.\n    \"\"\"\n    # Ensure we have the correct number of datasets\n    assert (\n        len(class_datasets) == num_classes\n    ), \"Number of datasets should match number of classes\"\n\n    def interleave_map_fn(dataset: tf.data.Dataset) -&gt; tf.data.Dataset:\n        return dataset.map(lambda x, y, z: (x, y, z))\n\n    # Use the interleave operation to balance the classes\n    interleaved_dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(\n        class_datasets\n    ).interleave(\n        interleave_map_fn,\n        cycle_length=num_classes,\n        block_length=1,\n        num_parallel_calls=tf.data.AUTOTUNE,\n    )\n    return interleaved_dataset\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.load_test_dataset_from_csvs","title":"<code>load_test_dataset_from_csvs(x_file_path: str, y_file_path: str, batch_size: int, target_length: int, dtype: DtypeLiteral) -&gt; tf.data.Dataset</code>","text":"<p>Load test dataset from CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>x_file_path</code> <code>str</code> <p>Path to the features CSV file.</p> required <code>y_file_path</code> <code>str</code> <p>Path to the labels CSV file.</p> required <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <code>target_length</code> <code>int</code> <p>Target length of each time series.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>Data type for the features as a string.</p> required <p>Returns:</p> Type Description <code>Dataset</code> <p>tf.data.Dataset: Test dataset.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def load_test_dataset_from_csvs(\n    x_file_path: str,\n    y_file_path: str,\n    batch_size: int,\n    target_length: int,\n    dtype: DtypeLiteral,\n) -&gt; tf.data.Dataset:\n    \"\"\"\n    Load test dataset from CSV files.\n\n    Args:\n        x_file_path (str): Path to the features CSV file.\n        y_file_path (str): Path to the labels CSV file.\n        batch_size (int): Size of each batch.\n        target_length (int): Target length of each time series.\n        dtype (DtypeLiteral): Data type for the features as a string.\n\n    Returns:\n        tf.data.Dataset: Test dataset.\n    \"\"\"\n    tf_dtype = get_dtype(dtype)\n\n    def parse_fn(x: tf.Tensor, y: tf.Tensor) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n        x = tf.io.decode_csv(x, record_defaults=[[0.0]] * target_length)\n        y = tf.io.decode_csv(y, record_defaults=[[0]])\n        return tf.reshape(tf.stack(x), (target_length, 1)), y[0]\n\n    dataset = tf.data.Dataset.zip(\n        (\n            tf.data.TextLineDataset(x_file_path).skip(1),\n            tf.data.TextLineDataset(y_file_path).skip(1),\n        )\n    )\n\n    return (\n        dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n        .batch(batch_size, drop_remainder=True)\n        .map(lambda x, y: (tf.cast(x, tf_dtype), y))\n        .prefetch(tf.data.AUTOTUNE)\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.load_train_dataset_from_csvs","title":"<code>load_train_dataset_from_csvs(x_file_path: str, y_file_path: str, batch_size: int, target_length: int, dtype: tf.DType, train_val_fraction: float = 0.8, use_augmentation: bool = False) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, int, int]</code>","text":"<p>Load training dataset from CSV files and split into train and validation sets.</p> <p>Parameters:</p> Name Type Description Default <code>x_file_path</code> <code>str</code> <p>Path to the features CSV file.</p> required <code>y_file_path</code> <code>str</code> <p>Path to the labels CSV file.</p> required <code>batch_size</code> <code>int</code> <p>Size of each batch.</p> required <code>target_length</code> <code>int</code> <p>Target length of each time series.</p> required <code>dtype</code> <code>DType</code> <p>Data type for the features.</p> required <code>train_val_fraction</code> <code>float</code> <p>Fraction of data to use for training. Defaults to 0.8.</p> <code>0.8</code> <code>use_augmentation</code> <code>bool</code> <p>Whether to augment original training examples with warped versions</p> <code>False</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>Tuple[tf.data.Dataset, tf.data.Dataset, int, int]: Train dataset, validation dataset,</p> <code>Dataset</code> <p>steps per epoch, and validation steps.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def load_train_dataset_from_csvs(\n    x_file_path: str,\n    y_file_path: str,\n    batch_size: int,\n    target_length: int,\n    dtype: tf.DType,\n    train_val_fraction: float = 0.8,\n    use_augmentation: bool = False,\n) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, int, int]:\n    \"\"\"\n    Load training dataset from CSV files and split into train and validation sets.\n\n    Args:\n        x_file_path (str): Path to the features CSV file.\n        y_file_path (str): Path to the labels CSV file.\n        batch_size (int): Size of each batch.\n        target_length (int): Target length of each time series.\n        dtype (tf.DType): Data type for the features.\n        train_val_fraction (float, optional): Fraction of data to use for training. Defaults to 0.8.\n        use_augmentation (bool): Whether to augment original training examples with warped versions\n\n    Returns:\n        Tuple[tf.data.Dataset, tf.data.Dataset, int, int]: Train dataset, validation dataset,\n        steps per epoch, and validation steps.\n    \"\"\"\n\n    def parse_fn(x: tf.Tensor, y: tf.Tensor) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n        x = tf.io.decode_csv(x, record_defaults=[[0.0]] * target_length)\n        y = tf.io.decode_csv(y, record_defaults=[[0]])\n        return tf.reshape(tf.stack(x), (target_length, 1)), y[0]\n\n    dataset = tf.data.Dataset.zip(\n        (\n            tf.data.TextLineDataset(x_file_path).skip(1),\n            tf.data.TextLineDataset(y_file_path).skip(1),\n        )\n    )\n\n    # Count total examples\n    total_examples = count_examples_fast(x_file_path)\n    # Calculate train and validation sizes\n    train_size, val_size = calculate_sizes(\n        total_examples, train_val_fraction, batch_size\n    )\n\n    # Split dataset into train and validation\n    train_dataset = dataset.take(train_size)\n    val_dataset = dataset.skip(train_size).take(val_size)\n\n    # Process and augment the training dataset\n    train_dataset = train_dataset.map(\n        parse_fn, num_parallel_calls=tf.data.AUTOTUNE\n    ).map(lambda x, y: (tf.cast(x, dtype), y))\n\n    if use_augmentation:\n        train_dataset = train_dataset.map(\n            lambda x, y: augment_example(x, y, dtype)\n        ).flat_map(\n            lambda x: x\n        )  # Flatten the dataset of datasets\n\n    train_dataset = train_dataset.batch(batch_size, drop_remainder=True).prefetch(\n        tf.data.AUTOTUNE\n    )\n\n    # Process the validation dataset (no augmentation)\n    val_dataset = (\n        val_dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n        .batch(batch_size, drop_remainder=True)\n        .map(lambda x, y: (tf.cast(x, dtype), y))\n        .prefetch(tf.data.AUTOTUNE)\n    )\n\n    # Recalculate steps per epoch\n    steps_per_epoch = (train_size * (3 if use_augmentation else 1)) // batch_size\n    validation_steps = val_size // batch_size\n\n    return (\n        train_dataset,\n        val_dataset,\n        steps_per_epoch,\n        validation_steps,\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.parse_row","title":"<code>parse_row(row: Tuple[str, str, str], target_length: int, dtype: tf.DType) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor]</code>","text":"<p>Parse a row of data and convert it to the appropriate tensor format. Padding and truncation are performed equally on both sides of the time series.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Tuple[str, str, str]</code> <p>A tuple containing read_id, cap_class, and timeseries as strings.</p> required <code>target_length</code> <code>int</code> <p>The desired length of the timeseries tensor.</p> required <code>dtype</code> <code>DType</code> <p>The desired data type for the timeseries tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: A tuple containing the parsed and formatted tensors for</p> <code>Tensor</code> <p>timeseries, cap_class, and read_id.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def parse_row(\n    row: Tuple[str, str, str], target_length: int, dtype: tf.DType\n) -&gt; Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:\n    \"\"\"\n    Parse a row of data and convert it to the appropriate tensor format.\n    Padding and truncation are performed equally on both sides of the time series.\n\n    Args:\n        row (Tuple[str, str, str]): A tuple containing read_id, cap_class, and timeseries as strings.\n        target_length (int): The desired length of the timeseries tensor.\n        dtype (tf.DType): The desired data type for the timeseries tensor.\n\n    Returns:\n        Tuple[tf.Tensor, tf.Tensor, tf.Tensor]: A tuple containing the parsed and formatted tensors for\n        timeseries, cap_class, and read_id.\n    \"\"\"\n    read_id, cap_class, timeseries = row\n    cap_class = tf.strings.to_number(cap_class, out_type=tf.int32)\n\n    # Split the timeseries string and convert to float\n    timeseries = tf.strings.split(timeseries, sep=\",\")\n    timeseries = tf.strings.to_number(timeseries, out_type=tf.float32)\n\n    # Get the current length of the timeseries\n    current_length = tf.shape(timeseries)[0]\n\n    # Function to pad the timeseries\n    def pad_timeseries() -&gt; tf.Tensor:\n        pad_amount = target_length - current_length\n        pad_left = pad_amount // 2\n        pad_right = pad_amount - pad_left\n        return tf.pad(\n            timeseries,\n            [[pad_left, pad_right]],\n            constant_values=0.0,\n        )\n\n    # Function to truncate the timeseries\n    def truncate_timeseries() -&gt; tf.Tensor:\n        truncate_amount = current_length - target_length\n        truncate_left = truncate_amount // 2\n        truncate_right = current_length - (truncate_amount - truncate_left)\n        return timeseries[truncate_left:truncate_right]\n\n    # Pad or truncate the timeseries to the target length\n    padded = tf.cond(\n        current_length &gt;= target_length, truncate_timeseries, pad_timeseries\n    )\n\n    padded = tf.reshape(padded, (target_length, 1))\n\n    # Cast to the desired dtype\n    if dtype != tf.float32:\n        padded = tf.cast(padded, dtype)\n\n    return padded, cap_class, read_id\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.read_dataset_version_info","title":"<code>read_dataset_version_info(dataset_dir: str) -&gt; Optional[str]</code>","text":"<p>Read the dataset version information from a file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>Directory containing the dataset version file.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The dataset version if found, None otherwise.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def read_dataset_version_info(dataset_dir: str) -&gt; Optional[str]:\n    \"\"\"\n    Read the dataset version information from a file.\n\n    Args:\n        dataset_dir (str): Directory containing the dataset version file.\n\n    Returns:\n        Optional[str]: The dataset version if found, None otherwise.\n    \"\"\"\n    version_file = os.path.join(dataset_dir, \"artifact_version.txt\")\n    if os.path.exists(version_file):\n        with open(version_file) as f:\n            return f.read().strip()\n    return None\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.train_etl","title":"<code>train_etl(caps_data_dir: str, dataset_dir: str, target_length: int, dtype: DtypeLiteral, examples_per_class: int, train_test_fraction: float, train_val_fraction: float, num_classes: int, batch_size: int, comet_project_name: str, use_remote_dataset_version: str = '', use_augmentation: bool = False) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int, str]</code>","text":"<p>Process the data from multiple class files, create balanced datasets, perform train-test split, and upload to Comet ML.</p> <p>Parameters:</p> Name Type Description Default <code>caps_data_dir</code> <code>str</code> <p>Directory containing the class CSV files.</p> required <code>dataset_dir</code> <code>str</code> <p>Directory to save the processed dataset.</p> required <code>target_length</code> <code>int</code> <p>The desired length of each time series.</p> required <code>dtype</code> <code>DtypeLiteral</code> <p>The desired data type for the timeseries tensor as a string.</p> required <code>examples_per_class</code> <code>int</code> <p>Number of samples to use per class.</p> required <code>train_test_fraction</code> <code>float</code> <p>Fraction of data to use for training.</p> required <code>train_val_fraction</code> <code>float</code> <p>Fraction of training data to use for validation.</p> required <code>num_classes</code> <code>int</code> <p>Number of classes in the dataset.</p> required <code>batch_size</code> <code>int</code> <p>The number of samples per batch.</p> required <code>comet_project_name</code> <code>str</code> <p>Name of the Comet ML project.</p> required <code>use_remote_dataset_version</code> <code>str</code> <p>Version of the remote dataset to use, if any.</p> <code>''</code> <code>use_augmentation</code> <code>bool</code> <p>Whether to augment original training examples with warped versions</p> <code>False</code> <p>Returns:     Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int, str]:     The train, validation, and test datasets, steps per epoch, validation steps, and the dataset version.</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def train_etl(\n    caps_data_dir: str,\n    dataset_dir: str,\n    target_length: int,\n    dtype: DtypeLiteral,\n    examples_per_class: int,\n    train_test_fraction: float,\n    train_val_fraction: float,\n    num_classes: int,\n    batch_size: int,\n    comet_project_name: str,\n    use_remote_dataset_version: str = \"\",\n    use_augmentation: bool = False,\n) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int, str]:\n    \"\"\"\n    Process the data from multiple class files, create balanced datasets,\n    perform train-test split, and upload to Comet ML.\n\n    Args:\n        caps_data_dir (str): Directory containing the class CSV files.\n        dataset_dir (str): Directory to save the processed dataset.\n        target_length (int): The desired length of each time series.\n        dtype (DtypeLiteral): The desired data type for the timeseries tensor as a string.\n        examples_per_class (int): Number of samples to use per class.\n        train_test_fraction (float): Fraction of data to use for training.\n        train_val_fraction (float): Fraction of training data to use for validation.\n        num_classes (int): Number of classes in the dataset.\n        batch_size (int): The number of samples per batch.\n        comet_project_name (str): Name of the Comet ML project.\n        use_remote_dataset_version (str): Version of the remote dataset to use, if any.\n        use_augmentation (bool): Whether to augment original training examples with warped versions\n    Returns:\n        Tuple[tf.data.Dataset, tf.data.Dataset, tf.data.Dataset, int, int, str]:\n        The train, validation, and test datasets, steps per epoch, validation steps, and the dataset version.\n    \"\"\"\n    comet_obj = CometArtifactManager(\n        project_name=comet_project_name, dataset_dir=dataset_dir\n    )\n    current_local_version = get_local_dataset_version(dataset_dir)\n    reprocess_dataset = False\n\n    # Check if the remote dataset version is different from the local version\n    # If yes download the remote dataset version and load it\n    # If no, then load the local dataset\n    if use_remote_dataset_version != \"\":\n        if use_remote_dataset_version != current_local_version:\n            logger.info(\n                f\"Downloading remote dataset version: v{use_remote_dataset_version}.. \"\n            )\n            comet_obj.download_remote_dataset(use_remote_dataset_version)\n        else:\n            logger.info(\n                \"Remote version is the same as the local version. Loading local dataset...\"\n            )\n        train_dataset, val_dataset, test_dataset, steps_per_epoch, validation_steps = (\n            create_train_val_test_datasets_from_train_test_csvs(\n                dataset_dir,\n                batch_size,\n                target_length,\n                dtype,\n                train_val_fraction,\n                use_augmentation,\n            )\n        )\n        write_dataset_version_info(dataset_dir, version=use_remote_dataset_version)\n        return (\n            train_dataset,\n            val_dataset,\n            test_dataset,\n            steps_per_epoch,\n            validation_steps,\n            use_remote_dataset_version,\n        )\n\n    if current_local_version and use_remote_dataset_version == \"\":\n        logger.info(\n            f\"A dataset v{current_local_version} was found locally in:\\n{dataset_dir}\"\n        )\n        reprocess = input(\n            \"Do you want to overwrite this local dataset by reprocess the data, and creating a new dataset version? (y/n): \"\n        )\n        if reprocess.lower() == \"y\" or reprocess.lower() == \"yes\":\n            logger.info(\"You chose to reprocess the data. Reprocessing data...\")\n            reprocess_dataset = True\n        else:\n            logger.info(\"You chose not to reprocess the data.Loading local dataset..\")\n            (\n                train_dataset,\n                val_dataset,\n                test_dataset,\n                steps_per_epoch,\n                validation_steps,\n            ) = create_train_val_test_datasets_from_train_test_csvs(\n                dataset_dir,\n                batch_size,\n                target_length,\n                dtype,\n                train_val_fraction,\n                use_augmentation,\n            )\n            logger.info(f\"Local dataset v{current_local_version} loaded successfully.\")\n            return (\n                train_dataset,\n                val_dataset,\n                test_dataset,\n                steps_per_epoch,\n                validation_steps,\n                current_local_version,\n            )\n\n    if reprocess_dataset or (\n        not current_local_version and use_remote_dataset_version == \"\"\n    ):\n        class_files = group_files_by_class(caps_data_dir)\n        min_class, min_rows = find_class_with_least_rows(class_files)\n        if examples_per_class is None:\n            examples_per_class = min_rows\n        else:\n            examples_per_class = min(examples_per_class, min_rows)\n        logger.info(\n            f\"Each class in the dataset will have {examples_per_class} examples\"\n        )\n\n        train_datasets = []\n        test_datasets = []\n        for class_id, file_paths in class_files.items():\n            train_ds, test_ds = create_class_dataset(\n                file_paths,\n                target_length,\n                dtype,\n                examples_per_class,\n                train_test_fraction,\n            )\n            train_datasets.append(train_ds)\n            test_datasets.append(test_ds)\n            logger.info(f\"Processed class {map_cap_int_to_name(class_id)}!\")\n\n        logger.info(\"Combining class datasets...\")\n        train_dataset = combine_datasets(train_datasets)\n        test_dataset = combine_datasets(test_datasets)\n\n        logger.info(\"Interleaving classes for ensuring class balance in each batch...\")\n        train_dataset = interleave_class_datasets(\n            train_datasets, num_classes=num_classes\n        )\n        test_dataset = interleave_class_datasets(test_datasets, num_classes=num_classes)\n\n        # Calculate total dataset size\n        logger.info(\"Calculating dataset size...\")\n        total_samples = examples_per_class * num_classes\n        train_samples = int(train_test_fraction * total_samples)\n        test_samples = total_samples - train_samples\n\n        # Batch the datasets\n        logger.info(\"Batching dataset...\")\n        train_dataset = train_dataset.batch(batch_size)\n        test_dataset = test_dataset.batch(batch_size)\n\n        # Prefetch for performance\n        logger.info(\"Prefetching dataset...\")\n        train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n        test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\n        logger.info(\"Saving train/test splits to CSV files...\")\n        write_dataset_to_csv(train_dataset, dataset_dir, \"train\")\n        write_dataset_to_csv(test_dataset, dataset_dir, \"test\")\n        logger.info(\n            f\"Train/test splits to CSV files in the following directory:\\n{dataset_dir}\"\n        )\n\n        # Log dataset information to Comet ML\n        comet_obj.experiment.log_parameter(\"target_length\", target_length)\n        comet_obj.experiment.log_parameter(\"dtype\", dtype)\n        comet_obj.experiment.log_parameter(\"examples_per_class\", examples_per_class)\n        comet_obj.experiment.log_parameter(\"train_test_fraction\", train_test_fraction)\n        comet_obj.experiment.log_parameter(\"train_val_fraction\", train_val_fraction)\n        comet_obj.experiment.log_parameter(\"batch_size\", batch_size)\n        comet_obj.experiment.log_parameter(\"num_classes\", len(class_files))\n        comet_obj.experiment.log_parameter(\"total_samples\", total_samples)\n        comet_obj.experiment.log_parameter(\"train_samples\", train_samples)\n        comet_obj.experiment.log_parameter(\"test_samples\", test_samples)\n\n        logger.info(\"Making Comet ML dataset artifacts for uploading...\")\n        version = upload_dataset_to_comet(dataset_dir, comet_project_name)\n\n        comet_obj.end_comet_experiment()\n        logger.info(\n            f\"Data processed and resulting dataset {version} uploaded to Comet ML successfully.\"\n        )\n\n        logger.info(\n            \"Creating train, validation, and test datasets from dataset CSV files...\"\n        )\n        train_dataset, val_dataset, test_dataset, steps_per_epoch, validation_steps = (\n            create_train_val_test_datasets_from_train_test_csvs(\n                dataset_dir,\n                batch_size,\n                target_length,\n                dtype,\n                train_val_fraction=train_val_fraction,\n                use_augmentation=use_augmentation,\n            )\n        )\n\n        return (\n            train_dataset,\n            val_dataset,\n            test_dataset,\n            steps_per_epoch,\n            validation_steps,\n            version,\n        )\n\n    raise RuntimeError(\"No valid dataset could be processed. Please check your inputs.\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.write_dataset_to_csv","title":"<code>write_dataset_to_csv(dataset: tf.data.Dataset, dataset_dir: str, train_test: str) -&gt; None</code>","text":"<p>Write a dataset to CSV files.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset to write.</p> required <code>dataset_dir</code> <code>str</code> <p>The directory to write the CSV files to.</p> required <code>train_test</code> <code>str</code> <p>Either 'train' or 'test' to indicate the dataset type.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def write_dataset_to_csv(\n    dataset: tf.data.Dataset, dataset_dir: str, train_test: str\n) -&gt; None:\n    \"\"\"\n    Write a dataset to CSV files.\n\n    Args:\n        dataset (tf.data.Dataset): The dataset to write.\n        dataset_dir (str): The directory to write the CSV files to.\n        train_test (str): Either 'train' or 'test' to indicate the dataset type.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(dataset_dir):\n        os.makedirs(dataset_dir)\n\n    x_filename = os.path.join(dataset_dir, f\"{train_test}_x.csv\")\n    y_filename = os.path.join(dataset_dir, f\"{train_test}_y.csv\")\n    read_id_filename = os.path.join(dataset_dir, f\"{train_test}_read_id.csv\")\n\n    with (\n        open(x_filename, \"w\", newline=\"\") as x_file,\n        open(y_filename, \"w\", newline=\"\") as y_file,\n        open(read_id_filename, \"w\", newline=\"\") as read_id_file,\n    ):\n        x_writer = csv.writer(x_file)\n        y_writer = csv.writer(y_file)\n        read_id_writer = csv.writer(read_id_file)\n\n        # Write headers\n        x_writer.writerow(\n            [f\"feature_{i}\" for i in range(dataset.element_spec[0].shape[1])]\n        )\n        y_writer.writerow([\"cap_class\"])\n        read_id_writer.writerow([\"read_id\"])\n\n        pbar = tqdm(dataset, desc=\"Processing batches\")\n\n        for batch_num, (x, y, read_id) in enumerate(pbar):\n            # Convert tensors to numpy arrays\n            x_numpy = x.numpy()\n            y_numpy = y.numpy()\n            read_id_numpy = read_id.numpy()\n\n            # Write x data (features)\n            x_writer.writerows(x_numpy.reshape(x_numpy.shape[0], -1))\n\n            # Write y data (labels)\n            y_writer.writerows(y_numpy.reshape(-1, 1))\n\n            # Write read_id data\n            read_id_writer.writerows(\n                [\n                    [rid.decode(\"utf-8\") if isinstance(rid, bytes) else rid]\n                    for rid in read_id_numpy\n                ]\n            )\n            pbar.set_description(f\"Processed {batch_num + 1} batches\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.train_etl.write_dataset_version_info","title":"<code>write_dataset_version_info(dataset_dir: str, version: str) -&gt; None</code>","text":"<p>Write the dataset version information to a file.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>Directory to write the version file.</p> required <code>version</code> <code>str</code> <p>Version information to write.</p> required Source code in <code>src/capfinder/train_etl.py</code> <pre><code>def write_dataset_version_info(dataset_dir: str, version: str) -&gt; None:\n    \"\"\"\n    Write the dataset version information to a file.\n\n    Args:\n        dataset_dir (str): Directory to write the version file.\n        version (str): Version information to write.\n    \"\"\"\n    version_file = os.path.join(dataset_dir, \"artifact_version.txt\")\n    with open(version_file, \"w\") as f:\n        f.write(version)\n</code></pre>"},{"location":"api_docs/#src.capfinder.training","title":"<code>training</code>","text":""},{"location":"api_docs/#src.capfinder.training.InterruptCallback","title":"<code>InterruptCallback</code>","text":"<p>               Bases: <code>Callback</code></p> <p>Callback to interrupt training based on a global flag.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>class InterruptCallback(keras.callbacks.Callback):\n    \"\"\"\n    Callback to interrupt training based on a global flag.\n    \"\"\"\n\n    def on_train_batch_end(\n        self, batch: int, logs: Optional[Dict[str, float]] = None\n    ) -&gt; None:\n        \"\"\"\n        Checks the global `stop_training` flag at the end of each batch.\n        If True, interrupts training and logs a message.\n\n        Args:\n            batch: The current batch index (integer).\n            logs: Optional dictionary of training metrics at the end of the batch (default: None).\n\n        Returns:\n            None\n        \"\"\"\n        global stop_training\n        if stop_training:\n            logger.info(\"Training interrupted by user during batch.\")\n            self.model.stop_training = True\n\n    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, float]] = None) -&gt; None:\n        \"\"\"\n        Checks the global `stop_training` flag at the end of each epoch.\n        If True, interrupts training and logs a message.\n\n        Args:\n            epoch: The current epoch index (integer).\n            logs: Optional dictionary of training metrics at the end of the epoch (default: None).\n\n        Returns:\n            None\n        \"\"\"\n        global stop_training\n        if stop_training:\n            te = epoch + 1\n            logger.info(f\"Training interrupted by user at the end of epoch {te}\")\n            self.model.stop_training = True\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.InterruptCallback.on_epoch_end","title":"<code>on_epoch_end(epoch: int, logs: Optional[Dict[str, float]] = None) -&gt; None</code>","text":"<p>Checks the global <code>stop_training</code> flag at the end of each epoch. If True, interrupts training and logs a message.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The current epoch index (integer).</p> required <code>logs</code> <code>Optional[Dict[str, float]]</code> <p>Optional dictionary of training metrics at the end of the epoch (default: None).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, float]] = None) -&gt; None:\n    \"\"\"\n    Checks the global `stop_training` flag at the end of each epoch.\n    If True, interrupts training and logs a message.\n\n    Args:\n        epoch: The current epoch index (integer).\n        logs: Optional dictionary of training metrics at the end of the epoch (default: None).\n\n    Returns:\n        None\n    \"\"\"\n    global stop_training\n    if stop_training:\n        te = epoch + 1\n        logger.info(f\"Training interrupted by user at the end of epoch {te}\")\n        self.model.stop_training = True\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.InterruptCallback.on_train_batch_end","title":"<code>on_train_batch_end(batch: int, logs: Optional[Dict[str, float]] = None) -&gt; None</code>","text":"<p>Checks the global <code>stop_training</code> flag at the end of each batch. If True, interrupts training and logs a message.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>int</code> <p>The current batch index (integer).</p> required <code>logs</code> <code>Optional[Dict[str, float]]</code> <p>Optional dictionary of training metrics at the end of the batch (default: None).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def on_train_batch_end(\n    self, batch: int, logs: Optional[Dict[str, float]] = None\n) -&gt; None:\n    \"\"\"\n    Checks the global `stop_training` flag at the end of each batch.\n    If True, interrupts training and logs a message.\n\n    Args:\n        batch: The current batch index (integer).\n        logs: Optional dictionary of training metrics at the end of the batch (default: None).\n\n    Returns:\n        None\n    \"\"\"\n    global stop_training\n    if stop_training:\n        logger.info(\"Training interrupted by user during batch.\")\n        self.model.stop_training = True\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.count_batches","title":"<code>count_batches(dataset: tf.data.Dataset, dataset_name: str) -&gt; int</code>","text":"<p>Count the number of individual examples in a dataset.</p> <p>Args: dataset (tf.data.Dataset): The dataset to count examples from. dataset_name (str): The name of the dataset.</p> <p>Returns: int: The number of examples in the dataset.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def count_batches(dataset: tf.data.Dataset, dataset_name: str) -&gt; int:\n    \"\"\"\n    Count the number of individual examples in a dataset.\n\n    Args:\n    dataset (tf.data.Dataset): The dataset to count examples from.\n    dataset_name (str): The name of the dataset.\n\n    Returns:\n    int: The number of examples in the dataset.\n    \"\"\"\n    count = sum(\n        1 for _ in tqdm(dataset, desc=f\"Batches in {dataset_name}\", unit=\"batches\")\n    )\n    return count\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.count_examples","title":"<code>count_examples(dataset: tf.data.Dataset, dataset_name: str) -&gt; int</code>","text":"<p>Count the number of individual examples in a dataset.</p> <p>Args: dataset (tf.data.Dataset): The dataset to count examples from. dataset_name (str): The name of the dataset.</p> <p>Returns: int: The number of examples in the dataset.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def count_examples(dataset: tf.data.Dataset, dataset_name: str) -&gt; int:\n    \"\"\"\n    Count the number of individual examples in a dataset.\n\n    Args:\n    dataset (tf.data.Dataset): The dataset to count examples from.\n    dataset_name (str): The name of the dataset.\n\n    Returns:\n    int: The number of examples in the dataset.\n    \"\"\"\n    count = sum(\n        1 for _ in tqdm(dataset, desc=f\"Examples in {dataset_name}\", unit=\"examples\")\n    )\n    return count\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.generate_unique_name","title":"<code>generate_unique_name(base_name: str, extension: str) -&gt; str</code>","text":"<p>Generate a unique filename with a datetime suffix.</p>"},{"location":"api_docs/#src.capfinder.training.generate_unique_name--parameters","title":"Parameters:","text":"<p>base_name: str     The base name of the file. extension: str     The file extension.</p>"},{"location":"api_docs/#src.capfinder.training.generate_unique_name--returns","title":"Returns:","text":"<p>str     The unique filename with the datetime suffix.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def generate_unique_name(base_name: str, extension: str) -&gt; str:\n    \"\"\"Generate a unique filename with a datetime suffix.\n\n    Parameters:\n    -----------\n    base_name: str\n        The base name of the file.\n    extension: str\n        The file extension.\n\n    Returns:\n    --------\n    str\n        The unique filename with the datetime suffix.\n    \"\"\"\n    # Get the current date and time\n    current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    # Append the date and time to the base name\n    unique_filename = f\"{base_name}_{current_datetime}{extension}\"\n    return unique_filename\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.handle_interrupt","title":"<code>handle_interrupt(signum: Optional[int] = None, frame: Optional[object] = None) -&gt; None</code>","text":"<p>Handles interrupt signals (e.g., Ctrl+C) by setting a global flag to stop training.</p> <p>Parameters:</p> Name Type Description Default <code>signum</code> <code>Optional[int]</code> <p>The signal number (optional).</p> <code>None</code> <code>frame</code> <code>Optional[object]</code> <p>The current stack frame (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def handle_interrupt(\n    signum: Optional[int] = None, frame: Optional[object] = None\n) -&gt; None:\n    \"\"\"\n    Handles interrupt signals (e.g., Ctrl+C) by setting a global flag to stop training.\n\n    Args:\n        signum: The signal number (optional).\n        frame: The current stack frame (optional).\n\n    Returns:\n        None\n    \"\"\"\n    global stop_training\n    stop_training = True\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.initialize_tuner","title":"<code>initialize_tuner(hyper_model: CNNLSTMModel | EncoderModel, tune_params: dict, model_save_dir: str, model_type: ModelType) -&gt; Union[Hyperband, BayesianOptimization, RandomSearch]</code>","text":"<p>Initialize a Keras Tuner object based on the specified tuning strategy.</p>"},{"location":"api_docs/#src.capfinder.training.initialize_tuner--parameters","title":"Parameters:","text":"<p>hyper_model: CapfinderHyperModel     An instance of the CapfinderHyperModel class. tune_params: dict     A dictionary containing the hyperparameters for tuning. model_save_dir: str     The directory where the model should be saved. comet_project_name: str model_type: ModelType     Type of the model to be trained.</p>"},{"location":"api_docs/#src.capfinder.training.initialize_tuner--returns","title":"Returns:","text":"<p>Union[Hyperband, BayesianOptimization, RandomSearch]:     An instance of the Keras Tuner class based on the specified tuning strategy.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def initialize_tuner(\n    hyper_model: \"CNNLSTMModel | EncoderModel\",\n    tune_params: dict,\n    model_save_dir: str,\n    model_type: ModelType,\n) -&gt; Union[Hyperband, BayesianOptimization, RandomSearch]:\n    \"\"\"Initialize a Keras Tuner object based on the specified tuning strategy.\n\n    Parameters:\n    -----------\n    hyper_model: CapfinderHyperModel\n        An instance of the CapfinderHyperModel class.\n    tune_params: dict\n        A dictionary containing the hyperparameters for tuning.\n    model_save_dir: str\n        The directory where the model should be saved.\n    comet_project_name: str\n    model_type: ModelType\n        Type of the model to be trained.\n\n    Returns:\n    --------\n    Union[Hyperband, BayesianOptimization, RandomSearch]:\n        An instance of the Keras Tuner class based on the specified tuning strategy.\n    \"\"\"\n\n    tuning_strategy = tune_params[\"tuning_strategy\"].lower()\n    if tuning_strategy not in [\"random_search\", \"bayesian_optimization\", \"hyperband\"]:\n        tuning_strategy = \"hyperband\"\n        logger.warning(\n            \"Invalid tuning strategy. Using Hyperband. Valid options are: 'random_search', 'bayesian_optimization', and 'hyperband'\"\n        )\n\n    if tuning_strategy == \"hyperband\":\n        logger.info(\"Using Hyperband tuning strategy...\")\n        tuner = Hyperband(\n            hypermodel=hyper_model.build,\n            objective=Objective(\"val_sparse_categorical_accuracy\", direction=\"max\"),\n            max_epochs=tune_params[\"max_epochs_hpt\"],\n            factor=tune_params[\"factor\"],\n            overwrite=tune_params[\"overwrite\"],\n            directory=model_save_dir,\n            seed=tune_params[\"seed\"],\n            project_name=tune_params[\"comet_project_name\"],\n        )\n    elif tuning_strategy == \"bayesian_optimization\":\n        logger.info(\"Using Bayesian Optimization tuning strategy...\")\n        tuner = BayesianOptimization(\n            hypermodel=hyper_model.build,\n            objective=Objective(\"val_sparse_categorical_accuracy\", direction=\"max\"),\n            max_trials=tune_params[\"max_trials\"],\n            overwrite=tune_params[\"overwrite\"],\n            directory=model_save_dir,\n            seed=tune_params[\"seed\"],\n            project_name=tune_params[\"comet_project_name\"],\n        )\n    elif tuning_strategy == \"random_search\":\n        logger.info(\"Using Random Search tuning strategy...\")\n        tuner = RandomSearch(\n            hypermodel=hyper_model.build,\n            objective=Objective(\"val_sparse_categorical_accuracy\", direction=\"max\"),\n            max_trials=tune_params[\"max_trials\"],\n            overwrite=tune_params[\"overwrite\"],\n            directory=model_save_dir,\n            seed=tune_params[\"seed\"],\n            project_name=tune_params[\"comet_project_name\"],\n        )\n    return tuner\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.kill_gpu_processes","title":"<code>kill_gpu_processes() -&gt; None</code>","text":"<p>Terminates processes running on the NVIDIA GPU and sets the Keras dtype policy to float16.</p> <p>This function checks if the <code>nvidia-smi</code> command exists and, if found, attempts to terminate all Python processes utilizing the GPU. If no NVIDIA GPU is found, the function skips the termination step. It also sets the Keras global policy to mixed_float16 for faster training.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def kill_gpu_processes() -&gt; None:\n    \"\"\"\n    Terminates processes running on the NVIDIA GPU and sets the Keras dtype policy to float16.\n\n    This function checks if the `nvidia-smi` command exists and, if found, attempts\n    to terminate all Python processes utilizing the GPU. If no NVIDIA GPU is found,\n    the function skips the termination step. It also sets the Keras global policy to\n    mixed_float16 for faster training.\n\n    Returns:\n        None\n    \"\"\"\n    if shutil.which(\"nvidia-smi\") is None:\n        logger.info(\"No NVIDIA GPU found. Skipping GPU process termination.\")\n        return\n\n    try:\n        # Get the list of GPU processes\n        result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n        lines = result.stdout.split(\"\\n\")\n\n        # Parse the lines to find PIDs of processes using the GPU\n        for line in lines:\n            if \"python\" in line:  # Adjust this if other processes need to be terminated\n                parts = line.split()\n                pid = parts[4]\n                print(f\"Terminating process with PID: {pid}\")\n                subprocess.run([\"kill\", \"-9\", pid])\n    except Exception as e:\n        logger.warning(f\"Error occurred while terminating GPU processes: {str(e)}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.save_model","title":"<code>save_model(model: keras.Model, base_name: str, extension: str, save_dir: str) -&gt; str</code>","text":"<p>Save the given model to a specified directory.</p>"},{"location":"api_docs/#src.capfinder.training.save_model--parameters","title":"Parameters:","text":"<p>model: keras.Model     The model to be saved. base_name: str     The base name for the saved model file. extension: str     The file extension for the saved model file. save_dir: str     The directory where the model should be saved.</p>"},{"location":"api_docs/#src.capfinder.training.save_model--returns","title":"Returns:","text":"<p>str     The full path where the model was saved.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def save_model(\n    model: keras.Model, base_name: str, extension: str, save_dir: str\n) -&gt; str:\n    \"\"\"\n    Save the given model to a specified directory.\n\n    Parameters:\n    -----------\n    model: keras.Model\n        The model to be saved.\n    base_name: str\n        The base name for the saved model file.\n    extension: str\n        The file extension for the saved model file.\n    save_dir: str\n        The directory where the model should be saved.\n\n    Returns:\n    --------\n    str\n        The full path where the model was saved.\n    \"\"\"\n    # Generate a unique filename for the model\n    model_filename = generate_unique_name(base_name, extension)\n\n    # Construct the full path where the model should be saved\n    model_save_path = os.path.join(save_dir, model_filename)\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir, exist_ok=True)\n\n    # Save the model to the specified path\n    model.save(model_save_path)\n    logger.info(f\"Best model saved to:{model_save_path}\")\n\n    # Return the save path\n    return model_save_path\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.select_lr_scheduler","title":"<code>select_lr_scheduler(lr_scheduler_params: dict, train_size: int) -&gt; Union[keras.callbacks.ReduceLROnPlateau, CyclicLR, SGDRScheduler]</code>","text":"<p>Selects and configures the learning rate scheduler based on the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>lr_scheduler_params</code> <code>dict</code> <p>Configuration parameters for the learning rate scheduler.</p> required <code>train_size</code> <code>int</code> <p>Number of training examples, used for step size calculations.</p> required <p>Returns:</p> Type Description <code>Union[ReduceLROnPlateau, CyclicLR, SGDRScheduler]</code> <p>Union[keras.callbacks.ReduceLROnPlateau, CyclicLR, SGDRScheduler]: The selected learning rate scheduler.</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def select_lr_scheduler(\n    lr_scheduler_params: dict, train_size: int\n) -&gt; Union[keras.callbacks.ReduceLROnPlateau, CyclicLR, SGDRScheduler]:\n    \"\"\"\n    Selects and configures the learning rate scheduler based on the provided parameters.\n\n    Args:\n        lr_scheduler_params (dict): Configuration parameters for the learning rate scheduler.\n        train_size (int): Number of training examples, used for step size calculations.\n\n    Returns:\n        Union[keras.callbacks.ReduceLROnPlateau, CyclicLR, SGDRScheduler]: The selected learning rate scheduler.\n    \"\"\"\n    scheduler_type = lr_scheduler_params[\"type\"]\n\n    if scheduler_type == \"reduce_lr_on_plateau\":\n        rlr_params = lr_scheduler_params[\"reduce_lr_on_plateau\"]\n        return keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=rlr_params[\"factor\"],\n            patience=rlr_params[\"patience\"],\n            verbose=1,\n            mode=\"min\",\n            min_lr=rlr_params[\"min_lr\"],\n        )\n\n    elif scheduler_type == \"cyclic_lr\":\n        clr_params = lr_scheduler_params[\"cyclic_lr\"]\n        return CyclicLR(\n            base_lr=clr_params[\"base_lr\"],\n            max_lr=clr_params[\"max_lr\"],\n            step_size=train_size * clr_params[\"step_size_factor\"],\n            mode=clr_params[\"mode\"],\n        )\n\n    elif scheduler_type == \"sgdr\":\n        sgdr_params = lr_scheduler_params[\"sgdr\"]\n        return SGDRScheduler(\n            min_lr=sgdr_params[\"min_lr\"],\n            max_lr=sgdr_params[\"max_lr\"],\n            steps_per_epoch=train_size,\n            lr_decay=sgdr_params[\"lr_decay\"],\n            cycle_length=sgdr_params[\"cycle_length\"],\n            mult_factor=sgdr_params[\"mult_factor\"],\n        )\n\n    else:\n        logger.warning(\n            f\"Unknown scheduler type: {scheduler_type}. Using ReduceLROnPlateau as default.\"\n        )\n        return keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.5,\n            patience=5,\n            verbose=1,\n            mode=\"min\",\n            min_lr=1e-6,\n        )\n</code></pre>"},{"location":"api_docs/#src.capfinder.training.set_data_distributed_training","title":"<code>set_data_distributed_training() -&gt; None</code>","text":"<p>Set JAX as the backend for Keras training, with distributed training if multiple CUDA devices are available.</p> <p>This function checks for available CUDA devices and sets up distributed training only if more than one is found.</p>"},{"location":"api_docs/#src.capfinder.training.set_data_distributed_training--returns","title":"Returns:","text":"<p>None</p> Source code in <code>src/capfinder/training.py</code> <pre><code>def set_data_distributed_training() -&gt; None:\n    \"\"\"\n    Set JAX as the backend for Keras training, with distributed training if multiple CUDA devices are available.\n\n    This function checks for available CUDA devices and sets up distributed training only if more than one is found.\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    # Set the Keras backend to JAX\n    logger.info(f\"Backend for training: {keras.backend.backend()}\")\n\n    # Retrieve available devices\n    all_devices = jax.devices()\n    cuda_devices = [d for d in all_devices if d.platform == \"gpu\"]\n\n    # Log available devices\n    for device in all_devices:\n        logger.info(f\"Device available: {device}, Type: {device.platform}\")\n\n    if len(cuda_devices) &gt; 1:\n        keras.mixed_precision.set_global_policy(\"mixed_float16\")\n        logger.info(\n            f\"({len(cuda_devices)}) CUDA devices detected. Setting up data distributed training.\"\n        )\n\n        # Define a 1D device mesh for data parallelism using only CUDA devices\n        mesh_1d = keras.distribution.DeviceMesh(\n            shape=(len(cuda_devices),), axis_names=[\"data\"], devices=cuda_devices\n        )\n\n        # Create a DataParallel distribution\n        data_parallel = keras.distribution.DataParallel(device_mesh=mesh_1d)\n\n        # Set the global distribution\n        keras.distribution.set_distribution(data_parallel)\n\n        logger.info(\"Distributed training setup complete.\")\n    elif len(cuda_devices) == 1:\n        keras.mixed_precision.set_global_policy(\"mixed_float16\")\n        logger.info(\n            \"Single CUDA device detected. Using standard (non-distributed) training.\"\n        )\n    else:\n        logger.info(\"No CUDA devices detected. Training will proceed on CPU.\")\n        keras.mixed_precision.set_global_policy(\"float32\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download","title":"<code>upload_download</code>","text":""},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager","title":"<code>CometArtifactManager</code>","text":"<p>Manages the creation, uploading, and downloading of dataset artifacts using Comet ML.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>class CometArtifactManager:\n    \"\"\"\n    Manages the creation, uploading, and downloading of dataset artifacts using Comet ML.\n    \"\"\"\n\n    def __init__(self, project_name: str, dataset_dir: str) -&gt; None:\n        \"\"\"\n        Initialize the CometArtifactManager.\n\n        Args:\n            project_name (str): The name of the Comet ML project.\n            dataset_dir (str): The directory containing the dataset.\n        \"\"\"\n        self.project_name = project_name\n        self.dataset_dir = dataset_dir\n        self.artifact_name = \"cap_data\"\n        self.experiment = self.initialize_comet_ml_experiment()\n        self.artifact: Optional[comet_ml.Artifact] = None\n        self.tmp_dir: Optional[str] = None\n        self.chunk_files: List[str] = []\n        self.upload_lock = threading.Lock()\n        self.upload_threads: List[threading.Thread] = []\n\n    def initialize_comet_ml_experiment(self) -&gt; comet_ml.Experiment:\n        \"\"\"\n        Initialize and return a Comet ML experiment.\n\n        Returns:\n            comet_ml.Experiment: The initialized Comet ML experiment.\n\n        Raises:\n            ValueError: If the COMET_API_KEY environment variable is not set.\n        \"\"\"\n        logger.info(f\"Initializing CometML experiment for project: {self.project_name}\")\n        comet_api_key = os.getenv(\"COMET_API_KEY\")\n        if not comet_api_key:\n            raise ValueError(\"COMET_API_KEY environment variable is not set.\")\n        return comet_ml.Experiment(\n            api_key=comet_api_key,\n            project_name=self.project_name,\n            display_summary_level=0,\n        )\n\n    def create_artifact(self) -&gt; comet_ml.Artifact:\n        \"\"\"\n        Create and return a Comet ML artifact.\n\n        Returns:\n            comet_ml.Artifact: The created Comet ML artifact.\n        \"\"\"\n        logger.info(f\"Creating CometML artifact: {self.artifact_name}\")\n        self.artifact = comet_ml.Artifact(\n            name=self.artifact_name,\n            artifact_type=\"dataset\",\n            metadata={\"task\": \"RNA caps classification\"},\n        )\n        return self.artifact\n\n    def upload_chunk(\n        self, chunk_file: str, chunk_number: int, total_chunks: int\n    ) -&gt; None:\n        \"\"\"\n        Upload a chunk of the dataset to the Comet ML artifact.\n\n        Args:\n            chunk_file (str): The path to the chunk file.\n            chunk_number (int): The number of the current chunk.\n            total_chunks (int): The total number of chunks.\n        \"\"\"\n        with self.upload_lock:\n            if self.artifact is None:\n                logger.error(\n                    \"Artifact is not initialized. Call create_artifact() first.\"\n                )\n                return\n            self.artifact.add(\n                local_path_or_data=chunk_file,\n                logical_path=os.path.basename(chunk_file),\n                metadata={\"chunk\": chunk_number, \"total_chunks\": total_chunks},\n            )\n        logger.info(f\"Added chunk to artifact: {os.path.basename(chunk_file)}\")\n\n    def create_targz_chunks(\n        self, chunk_size: int = 200 * 1024 * 1024\n    ) -&gt; Tuple[List[str], str, int]:\n        \"\"\"\n        Create tar.gz chunks of the dataset.\n\n        Args:\n            chunk_size (int, optional): The size of each chunk in bytes. Defaults to 20MB.\n\n        Returns:\n            Tuple[List[str], str, int]: A tuple containing the list of chunk files,\n                                        the temporary directory path, and the total number of chunks.\n        \"\"\"\n        logger.info(\"Creating tar.gz chunks of the dataset...\")\n        self.tmp_dir = tempfile.mkdtemp()\n        logger.info(f\"Temporary directory created at: {self.tmp_dir}\")\n\n        # Create a single tar.gz file of the entire dataset\n        tar_path = os.path.join(self.tmp_dir, \"dataset.tar.gz\")\n        with tarfile.open(tar_path, \"w:gz\") as tar:\n            for root, _, files in os.walk(self.dataset_dir):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, self.dataset_dir)\n                    tar.add(file_path, arcname=arcname)\n\n        # Split the tar.gz file into chunks\n        chunk_number = 0\n        with open(tar_path, \"rb\") as f:\n            while True:\n                chunk = f.read(chunk_size)\n                if not chunk:\n                    break\n                chunk_file = os.path.join(\n                    self.tmp_dir, f\"dataset.tar.gz.{chunk_number:03d}\"\n                )\n                with open(chunk_file, \"wb\") as chunk_f:\n                    chunk_f.write(chunk)\n                self.chunk_files.append(chunk_file)\n                chunk_number += 1\n\n        total_chunks = chunk_number\n        logger.info(f\"Created {total_chunks} tar.gz chunks\")\n\n        # Calculate hash of the original tar.gz file\n        tar_hash = calculate_file_hash(tar_path)\n\n        # Store tar hash\n        hash_file_path = os.path.join(self.tmp_dir, \"tar_hash.json\")\n        with open(hash_file_path, \"w\") as f:\n            json.dump({\"tar_hash\": tar_hash}, f)\n        logger.info(f\"Tar hash stored in: {hash_file_path}\")\n\n        return self.chunk_files, self.tmp_dir, total_chunks\n\n    def make_comet_artifacts(self) -&gt; None:\n        \"\"\"\n        Create and upload Comet ML artifacts.\n        \"\"\"\n        self.create_artifact()\n        self.chunk_files, self.tmp_dir, total_chunks = self.create_targz_chunks()\n\n        # Upload chunks\n        for i, chunk_file in enumerate(self.chunk_files):\n            upload_thread = threading.Thread(\n                target=self.upload_chunk, args=(chunk_file, i, total_chunks)\n            )\n            upload_thread.start()\n            self.upload_threads.append(upload_thread)\n\n        # Wait for all upload threads to complete\n        for thread in self.upload_threads:\n            thread.join()\n\n        # Add tar hash to artifact\n        hash_file_path = os.path.join(self.tmp_dir, \"tar_hash.json\")\n        self.artifact.add(  # type: ignore\n            local_path_or_data=hash_file_path,\n            logical_path=\"tar_hash.json\",\n            metadata={\"content\": \"Tar hash for integrity check\"},\n        )\n        logger.info(\"Added tar hash to artifact\")\n\n    def log_artifacts_to_comet(self) -&gt; Optional[str]:\n        \"\"\"\n        Log artifacts to Comet ML.\n\n        Returns:\n            Optional[str]: The version of the logged artifact, or None if logging failed.\n        \"\"\"\n        if self.experiment is not None and self.artifact is not None:\n            logger.info(\"Logging artifact to CometML...\")\n            art = self.experiment.log_artifact(self.artifact)\n            version = f\"{art.version.major}.{art.version.minor}.{art.version.patch}\"\n            logger.info(f\"Artifact logged successfully. Version: {version}\")\n            self.store_artifact_version_to_file(version)\n\n            logger.info(\n                \"Artifact upload initiated. It will continue in the background.\"\n            )\n\n            # Clean up the temporary directory\n            shutil.rmtree(self.tmp_dir)  # type: ignore\n            logger.info(f\"Temporary directory cleaned up: {self.tmp_dir}\")\n\n            return version\n        return None\n\n    def store_artifact_version_to_file(self, version: str) -&gt; None:\n        \"\"\"\n        Store the artifact version in a file.\n\n        Args:\n            version (str): The version of the artifact to store.\n        \"\"\"\n        version_file = os.path.join(self.dataset_dir, \"artifact_version.txt\")\n        with open(version_file, \"w\") as f:\n            f.write(version)\n        logger.info(f\"Artifact version {version} written to {version_file}\")\n\n    def download_remote_dataset(self, version: str, max_retries: int = 3) -&gt; None:\n        \"\"\"\n        Download a remote dataset from Comet ML.\n\n        Args:\n            version (str): The version of the dataset to download.\n            max_retries (int, optional): The maximum number of download attempts. Defaults to 3.\n\n        Raises:\n            Exception: If the download fails after the maximum number of retries.\n        \"\"\"\n        logger.info(f\"Downloading remote dataset v{version}...\")\n\n        for attempt in range(max_retries):\n            try:\n                art = self.experiment.get_artifact(\n                    artifact_name=self.artifact_name, version_or_alias=version\n                )\n\n                tmp_dir = tempfile.mkdtemp()\n                logger.info(f\"Temporary directory for download created at: {tmp_dir}\")\n                art.download(tmp_dir)\n\n                # Combine all chunks back into a single tar.gz file\n                tar_path = os.path.join(tmp_dir, \"dataset.tar.gz\")\n                with open(tar_path, \"wb\") as tar_file:\n                    chunk_files = sorted(\n                        [\n                            f\n                            for f in os.listdir(tmp_dir)\n                            if f.startswith(\"dataset.tar.gz.\")\n                        ]\n                    )\n                    for chunk_file in chunk_files:\n                        with open(os.path.join(tmp_dir, chunk_file), \"rb\") as chunk:\n                            tar_file.write(chunk.read())\n\n                # Verify tar.gz integrity\n                with open(os.path.join(tmp_dir, \"tar_hash.json\")) as f:\n                    original_hash = json.load(f)[\"tar_hash\"]\n                current_hash = calculate_file_hash(tar_path)\n                if current_hash != original_hash:\n                    raise ValueError(\"Tar file integrity check failed\")  # noqa: TRY301\n\n                # Extract the tar.gz file\n                with tarfile.open(tar_path, \"r:gz\") as tar:\n                    tar.extractall(path=self.dataset_dir)\n\n                logger.info(\n                    \"Remote dataset downloaded, verified, and extracted successfully.\"\n                )\n                return\n\n            except Exception as e:\n                logger.error(f\"Attempt {attempt + 1} failed: {str(e)}\")  # noqa: G003\n                if attempt &lt; max_retries - 1:\n                    wait_time = (2**attempt) + random.uniform(\n                        0, 1\n                    )  # Exponential backoff with jitter\n                    logger.info(f\"Retrying in {wait_time:.2f} seconds...\")\n                    time.sleep(wait_time)\n                else:\n                    logger.error(\"Max retries reached. Download failed.\")\n                    raise\n\n            finally:\n                # Clean up\n                if \"tmp_dir\" in locals():\n                    shutil.rmtree(tmp_dir)\n                    logger.info(f\"Temporary directory cleaned up: {tmp_dir}\")\n\n        raise Exception(  # noqa: TRY002\n            \"Failed to download and extract the dataset after maximum retries.\"\n        )\n\n    def end_comet_experiment(self) -&gt; None:\n        \"\"\"\n        End the Comet ML experiment.\n        \"\"\"\n        logger.info(\"Ending CometML experiment...\")\n        self.experiment.end()\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.__init__","title":"<code>__init__(project_name: str, dataset_dir: str) -&gt; None</code>","text":"<p>Initialize the CometArtifactManager.</p> <p>Parameters:</p> Name Type Description Default <code>project_name</code> <code>str</code> <p>The name of the Comet ML project.</p> required <code>dataset_dir</code> <code>str</code> <p>The directory containing the dataset.</p> required Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def __init__(self, project_name: str, dataset_dir: str) -&gt; None:\n    \"\"\"\n    Initialize the CometArtifactManager.\n\n    Args:\n        project_name (str): The name of the Comet ML project.\n        dataset_dir (str): The directory containing the dataset.\n    \"\"\"\n    self.project_name = project_name\n    self.dataset_dir = dataset_dir\n    self.artifact_name = \"cap_data\"\n    self.experiment = self.initialize_comet_ml_experiment()\n    self.artifact: Optional[comet_ml.Artifact] = None\n    self.tmp_dir: Optional[str] = None\n    self.chunk_files: List[str] = []\n    self.upload_lock = threading.Lock()\n    self.upload_threads: List[threading.Thread] = []\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.create_artifact","title":"<code>create_artifact() -&gt; comet_ml.Artifact</code>","text":"<p>Create and return a Comet ML artifact.</p> <p>Returns:</p> Type Description <code>Artifact</code> <p>comet_ml.Artifact: The created Comet ML artifact.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def create_artifact(self) -&gt; comet_ml.Artifact:\n    \"\"\"\n    Create and return a Comet ML artifact.\n\n    Returns:\n        comet_ml.Artifact: The created Comet ML artifact.\n    \"\"\"\n    logger.info(f\"Creating CometML artifact: {self.artifact_name}\")\n    self.artifact = comet_ml.Artifact(\n        name=self.artifact_name,\n        artifact_type=\"dataset\",\n        metadata={\"task\": \"RNA caps classification\"},\n    )\n    return self.artifact\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.create_targz_chunks","title":"<code>create_targz_chunks(chunk_size: int = 200 * 1024 * 1024) -&gt; Tuple[List[str], str, int]</code>","text":"<p>Create tar.gz chunks of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_size</code> <code>int</code> <p>The size of each chunk in bytes. Defaults to 20MB.</p> <code>200 * 1024 * 1024</code> <p>Returns:</p> Type Description <code>Tuple[List[str], str, int]</code> <p>Tuple[List[str], str, int]: A tuple containing the list of chunk files,                         the temporary directory path, and the total number of chunks.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def create_targz_chunks(\n    self, chunk_size: int = 200 * 1024 * 1024\n) -&gt; Tuple[List[str], str, int]:\n    \"\"\"\n    Create tar.gz chunks of the dataset.\n\n    Args:\n        chunk_size (int, optional): The size of each chunk in bytes. Defaults to 20MB.\n\n    Returns:\n        Tuple[List[str], str, int]: A tuple containing the list of chunk files,\n                                    the temporary directory path, and the total number of chunks.\n    \"\"\"\n    logger.info(\"Creating tar.gz chunks of the dataset...\")\n    self.tmp_dir = tempfile.mkdtemp()\n    logger.info(f\"Temporary directory created at: {self.tmp_dir}\")\n\n    # Create a single tar.gz file of the entire dataset\n    tar_path = os.path.join(self.tmp_dir, \"dataset.tar.gz\")\n    with tarfile.open(tar_path, \"w:gz\") as tar:\n        for root, _, files in os.walk(self.dataset_dir):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, self.dataset_dir)\n                tar.add(file_path, arcname=arcname)\n\n    # Split the tar.gz file into chunks\n    chunk_number = 0\n    with open(tar_path, \"rb\") as f:\n        while True:\n            chunk = f.read(chunk_size)\n            if not chunk:\n                break\n            chunk_file = os.path.join(\n                self.tmp_dir, f\"dataset.tar.gz.{chunk_number:03d}\"\n            )\n            with open(chunk_file, \"wb\") as chunk_f:\n                chunk_f.write(chunk)\n            self.chunk_files.append(chunk_file)\n            chunk_number += 1\n\n    total_chunks = chunk_number\n    logger.info(f\"Created {total_chunks} tar.gz chunks\")\n\n    # Calculate hash of the original tar.gz file\n    tar_hash = calculate_file_hash(tar_path)\n\n    # Store tar hash\n    hash_file_path = os.path.join(self.tmp_dir, \"tar_hash.json\")\n    with open(hash_file_path, \"w\") as f:\n        json.dump({\"tar_hash\": tar_hash}, f)\n    logger.info(f\"Tar hash stored in: {hash_file_path}\")\n\n    return self.chunk_files, self.tmp_dir, total_chunks\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.download_remote_dataset","title":"<code>download_remote_dataset(version: str, max_retries: int = 3) -&gt; None</code>","text":"<p>Download a remote dataset from Comet ML.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the dataset to download.</p> required <code>max_retries</code> <code>int</code> <p>The maximum number of download attempts. Defaults to 3.</p> <code>3</code> <p>Raises:</p> Type Description <code>Exception</code> <p>If the download fails after the maximum number of retries.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def download_remote_dataset(self, version: str, max_retries: int = 3) -&gt; None:\n    \"\"\"\n    Download a remote dataset from Comet ML.\n\n    Args:\n        version (str): The version of the dataset to download.\n        max_retries (int, optional): The maximum number of download attempts. Defaults to 3.\n\n    Raises:\n        Exception: If the download fails after the maximum number of retries.\n    \"\"\"\n    logger.info(f\"Downloading remote dataset v{version}...\")\n\n    for attempt in range(max_retries):\n        try:\n            art = self.experiment.get_artifact(\n                artifact_name=self.artifact_name, version_or_alias=version\n            )\n\n            tmp_dir = tempfile.mkdtemp()\n            logger.info(f\"Temporary directory for download created at: {tmp_dir}\")\n            art.download(tmp_dir)\n\n            # Combine all chunks back into a single tar.gz file\n            tar_path = os.path.join(tmp_dir, \"dataset.tar.gz\")\n            with open(tar_path, \"wb\") as tar_file:\n                chunk_files = sorted(\n                    [\n                        f\n                        for f in os.listdir(tmp_dir)\n                        if f.startswith(\"dataset.tar.gz.\")\n                    ]\n                )\n                for chunk_file in chunk_files:\n                    with open(os.path.join(tmp_dir, chunk_file), \"rb\") as chunk:\n                        tar_file.write(chunk.read())\n\n            # Verify tar.gz integrity\n            with open(os.path.join(tmp_dir, \"tar_hash.json\")) as f:\n                original_hash = json.load(f)[\"tar_hash\"]\n            current_hash = calculate_file_hash(tar_path)\n            if current_hash != original_hash:\n                raise ValueError(\"Tar file integrity check failed\")  # noqa: TRY301\n\n            # Extract the tar.gz file\n            with tarfile.open(tar_path, \"r:gz\") as tar:\n                tar.extractall(path=self.dataset_dir)\n\n            logger.info(\n                \"Remote dataset downloaded, verified, and extracted successfully.\"\n            )\n            return\n\n        except Exception as e:\n            logger.error(f\"Attempt {attempt + 1} failed: {str(e)}\")  # noqa: G003\n            if attempt &lt; max_retries - 1:\n                wait_time = (2**attempt) + random.uniform(\n                    0, 1\n                )  # Exponential backoff with jitter\n                logger.info(f\"Retrying in {wait_time:.2f} seconds...\")\n                time.sleep(wait_time)\n            else:\n                logger.error(\"Max retries reached. Download failed.\")\n                raise\n\n        finally:\n            # Clean up\n            if \"tmp_dir\" in locals():\n                shutil.rmtree(tmp_dir)\n                logger.info(f\"Temporary directory cleaned up: {tmp_dir}\")\n\n    raise Exception(  # noqa: TRY002\n        \"Failed to download and extract the dataset after maximum retries.\"\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.end_comet_experiment","title":"<code>end_comet_experiment() -&gt; None</code>","text":"<p>End the Comet ML experiment.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def end_comet_experiment(self) -&gt; None:\n    \"\"\"\n    End the Comet ML experiment.\n    \"\"\"\n    logger.info(\"Ending CometML experiment...\")\n    self.experiment.end()\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.initialize_comet_ml_experiment","title":"<code>initialize_comet_ml_experiment() -&gt; comet_ml.Experiment</code>","text":"<p>Initialize and return a Comet ML experiment.</p> <p>Returns:</p> Type Description <code>Experiment</code> <p>comet_ml.Experiment: The initialized Comet ML experiment.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the COMET_API_KEY environment variable is not set.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def initialize_comet_ml_experiment(self) -&gt; comet_ml.Experiment:\n    \"\"\"\n    Initialize and return a Comet ML experiment.\n\n    Returns:\n        comet_ml.Experiment: The initialized Comet ML experiment.\n\n    Raises:\n        ValueError: If the COMET_API_KEY environment variable is not set.\n    \"\"\"\n    logger.info(f\"Initializing CometML experiment for project: {self.project_name}\")\n    comet_api_key = os.getenv(\"COMET_API_KEY\")\n    if not comet_api_key:\n        raise ValueError(\"COMET_API_KEY environment variable is not set.\")\n    return comet_ml.Experiment(\n        api_key=comet_api_key,\n        project_name=self.project_name,\n        display_summary_level=0,\n    )\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.log_artifacts_to_comet","title":"<code>log_artifacts_to_comet() -&gt; Optional[str]</code>","text":"<p>Log artifacts to Comet ML.</p> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The version of the logged artifact, or None if logging failed.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def log_artifacts_to_comet(self) -&gt; Optional[str]:\n    \"\"\"\n    Log artifacts to Comet ML.\n\n    Returns:\n        Optional[str]: The version of the logged artifact, or None if logging failed.\n    \"\"\"\n    if self.experiment is not None and self.artifact is not None:\n        logger.info(\"Logging artifact to CometML...\")\n        art = self.experiment.log_artifact(self.artifact)\n        version = f\"{art.version.major}.{art.version.minor}.{art.version.patch}\"\n        logger.info(f\"Artifact logged successfully. Version: {version}\")\n        self.store_artifact_version_to_file(version)\n\n        logger.info(\n            \"Artifact upload initiated. It will continue in the background.\"\n        )\n\n        # Clean up the temporary directory\n        shutil.rmtree(self.tmp_dir)  # type: ignore\n        logger.info(f\"Temporary directory cleaned up: {self.tmp_dir}\")\n\n        return version\n    return None\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.make_comet_artifacts","title":"<code>make_comet_artifacts() -&gt; None</code>","text":"<p>Create and upload Comet ML artifacts.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def make_comet_artifacts(self) -&gt; None:\n    \"\"\"\n    Create and upload Comet ML artifacts.\n    \"\"\"\n    self.create_artifact()\n    self.chunk_files, self.tmp_dir, total_chunks = self.create_targz_chunks()\n\n    # Upload chunks\n    for i, chunk_file in enumerate(self.chunk_files):\n        upload_thread = threading.Thread(\n            target=self.upload_chunk, args=(chunk_file, i, total_chunks)\n        )\n        upload_thread.start()\n        self.upload_threads.append(upload_thread)\n\n    # Wait for all upload threads to complete\n    for thread in self.upload_threads:\n        thread.join()\n\n    # Add tar hash to artifact\n    hash_file_path = os.path.join(self.tmp_dir, \"tar_hash.json\")\n    self.artifact.add(  # type: ignore\n        local_path_or_data=hash_file_path,\n        logical_path=\"tar_hash.json\",\n        metadata={\"content\": \"Tar hash for integrity check\"},\n    )\n    logger.info(\"Added tar hash to artifact\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.store_artifact_version_to_file","title":"<code>store_artifact_version_to_file(version: str) -&gt; None</code>","text":"<p>Store the artifact version in a file.</p> <p>Parameters:</p> Name Type Description Default <code>version</code> <code>str</code> <p>The version of the artifact to store.</p> required Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def store_artifact_version_to_file(self, version: str) -&gt; None:\n    \"\"\"\n    Store the artifact version in a file.\n\n    Args:\n        version (str): The version of the artifact to store.\n    \"\"\"\n    version_file = os.path.join(self.dataset_dir, \"artifact_version.txt\")\n    with open(version_file, \"w\") as f:\n        f.write(version)\n    logger.info(f\"Artifact version {version} written to {version_file}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.CometArtifactManager.upload_chunk","title":"<code>upload_chunk(chunk_file: str, chunk_number: int, total_chunks: int) -&gt; None</code>","text":"<p>Upload a chunk of the dataset to the Comet ML artifact.</p> <p>Parameters:</p> Name Type Description Default <code>chunk_file</code> <code>str</code> <p>The path to the chunk file.</p> required <code>chunk_number</code> <code>int</code> <p>The number of the current chunk.</p> required <code>total_chunks</code> <code>int</code> <p>The total number of chunks.</p> required Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def upload_chunk(\n    self, chunk_file: str, chunk_number: int, total_chunks: int\n) -&gt; None:\n    \"\"\"\n    Upload a chunk of the dataset to the Comet ML artifact.\n\n    Args:\n        chunk_file (str): The path to the chunk file.\n        chunk_number (int): The number of the current chunk.\n        total_chunks (int): The total number of chunks.\n    \"\"\"\n    with self.upload_lock:\n        if self.artifact is None:\n            logger.error(\n                \"Artifact is not initialized. Call create_artifact() first.\"\n            )\n            return\n        self.artifact.add(\n            local_path_or_data=chunk_file,\n            logical_path=os.path.basename(chunk_file),\n            metadata={\"chunk\": chunk_number, \"total_chunks\": total_chunks},\n        )\n    logger.info(f\"Added chunk to artifact: {os.path.basename(chunk_file)}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.calculate_file_hash","title":"<code>calculate_file_hash(file_path: str) -&gt; str</code>","text":"<p>Calculate the SHA256 hash of a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The hexadecimal representation of the file's SHA256 hash.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def calculate_file_hash(file_path: str) -&gt; str:\n    \"\"\"\n    Calculate the SHA256 hash of a file.\n\n    Args:\n        file_path (str): The path to the file.\n\n    Returns:\n        str: The hexadecimal representation of the file's SHA256 hash.\n    \"\"\"\n    sha256_hash = hashlib.sha256()\n    with open(file_path, \"rb\") as f:\n        for byte_block in iter(lambda: f.read(4096), b\"\"):\n            sha256_hash.update(byte_block)\n    return sha256_hash.hexdigest()\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.download_dataset_from_comet","title":"<code>download_dataset_from_comet(dataset_dir: str, project_name: str, version: str) -&gt; None</code>","text":"<p>Download a dataset from Comet ML.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>The directory to download the dataset to.</p> required <code>project_name</code> <code>str</code> <p>The name of the Comet ML project.</p> required <code>version</code> <code>str</code> <p>The version of the dataset to download.</p> required Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def download_dataset_from_comet(\n    dataset_dir: str, project_name: str, version: str\n) -&gt; None:\n    \"\"\"\n    Download a dataset from Comet ML.\n\n    Args:\n        dataset_dir (str): The directory to download the dataset to.\n        project_name (str): The name of the Comet ML project.\n        version (str): The version of the dataset to download.\n    \"\"\"\n    comet_obj = CometArtifactManager(project_name=project_name, dataset_dir=dataset_dir)\n    comet_obj.download_remote_dataset(version)\n</code></pre>"},{"location":"api_docs/#src.capfinder.upload_download.upload_dataset_to_comet","title":"<code>upload_dataset_to_comet(dataset_dir: str, project_name: str) -&gt; str</code>","text":"<p>Upload a dataset to Comet ML.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>str</code> <p>The directory containing the dataset to upload.</p> required <code>project_name</code> <code>str</code> <p>The name of the Comet ML project.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The version of the uploaded dataset, or None if the upload failed.</p> Source code in <code>src/capfinder/upload_download.py</code> <pre><code>def upload_dataset_to_comet(dataset_dir: str, project_name: str) -&gt; str:\n    \"\"\"\n    Upload a dataset to Comet ML.\n\n    Args:\n        dataset_dir (str): The directory containing the dataset to upload.\n        project_name (str): The name of the Comet ML project.\n\n    Returns:\n        str: The version of the uploaded dataset, or None if the upload failed.\n    \"\"\"\n    comet_obj = CometArtifactManager(project_name=project_name, dataset_dir=dataset_dir)\n\n    logger.info(\"Making Comet ML dataset artifacts for uploading...\")\n    comet_obj.make_comet_artifacts()\n\n    logger.info(\"Logging artifacts to Comet ML...\")\n    version = comet_obj.log_artifacts_to_comet()\n\n    if version:\n        logger.info(\n            f\"Dataset version {version} logged to Comet ML successfully. Upload will continue in the background.\"\n        )\n        return version\n    else:\n        logger.error(\"Failed to log dataset to Comet ML.\")\n        return \"\"\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils","title":"<code>utils</code>","text":"<p>The module contains some common utility functions used in the capfinder package.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.utils.ensure_config_dir","title":"<code>ensure_config_dir() -&gt; None</code>","text":"<p>Ensure the configuration directory exists.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def ensure_config_dir() -&gt; None:\n    \"\"\"Ensure the configuration directory exists.\"\"\"\n    CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.file_opener","title":"<code>file_opener(filename: str) -&gt; Union[IO[str], IO[bytes]]</code>","text":"<p>Open a file for reading. If the file is compressed, use gzip to open it.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path to the file to open.</p> required <p>Returns:</p> Type Description <code>Union[IO[str], IO[bytes]]</code> <p>file object: A file object that can be used for reading.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def file_opener(filename: str) -&gt; Union[IO[str], IO[bytes]]:\n    \"\"\"\n    Open a file for reading. If the file is compressed, use gzip to open it.\n\n    Args:\n        filename (str): The path to the file to open.\n\n    Returns:\n        file object: A file object that can be used for reading.\n    \"\"\"\n    if filename.endswith(\".gz\"):\n        # Compressed FASTQ file (gzip)\n        return gzip.open(filename, \"rt\")\n    else:\n        # Uncompressed FASTQ file\n        return open(filename)\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.get_dtype","title":"<code>get_dtype(dtype: str) -&gt; Type[np.floating]</code>","text":"<p>Returns the numpy floating type corresponding to the provided dtype string.</p> <p>If the provided dtype string is not valid, a warning is logged and np.float32 is returned as default.</p> <p>Parameters: dtype (str): The dtype string to convert to a numpy floating type.</p> <p>Returns: Type[np.floating]: The corresponding numpy floating type.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def get_dtype(dtype: str) -&gt; Type[np.floating]:\n    \"\"\"\n    Returns the numpy floating type corresponding to the provided dtype string.\n\n    If the provided dtype string is not valid, a warning is logged and np.float32 is returned as default.\n\n    Parameters:\n    dtype (str): The dtype string to convert to a numpy floating type.\n\n    Returns:\n    Type[np.floating]: The corresponding numpy floating type.\n    \"\"\"\n    valid_dtypes = {\n        \"float16\": np.float16,\n        \"float32\": np.float32,\n        \"float64\": np.float64,\n    }\n\n    if dtype in valid_dtypes:\n        dt = valid_dtypes[dtype]\n    else:\n        logger.warning('You provided an invalid dtype. Using \"float32\" as default.')\n        dt = np.float32\n\n    return cast(Type[np.floating], dt)  # Cast dt to the expected type\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.get_next_available_cap_number","title":"<code>get_next_available_cap_number() -&gt; int</code>","text":"<p>Find the next available cap number in the sequence.</p> <p>Returns: int: The next available cap number.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def get_next_available_cap_number() -&gt; int:\n    \"\"\"\n    Find the next available cap number in the sequence.\n\n    Returns:\n    int: The next available cap number.\n    \"\"\"\n    global CAP_MAPPING\n\n    existing_caps = set(CAP_MAPPING.keys())\n    existing_caps.discard(-99)  # Remove the special 'unknown' cap\n    if not existing_caps:\n        return 0\n    max_cap = max(existing_caps)\n    next_cap = max_cap + 1\n    return next_cap\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.get_terminal_width","title":"<code>get_terminal_width() -&gt; int</code>","text":"<p>Get the width of the terminal.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The width of the terminal in columns. Defaults to 80 if not available.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def get_terminal_width() -&gt; int:\n    \"\"\"\n    Get the width of the terminal.\n\n    Returns:\n        int: The width of the terminal in columns. Defaults to 80 if not available.\n    \"\"\"\n    return shutil.get_terminal_size((80, 20)).columns\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.initialize_cap_mapping","title":"<code>initialize_cap_mapping() -&gt; None</code>","text":"<p>Initialize the cap mapping file if it doesn't exist.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def initialize_cap_mapping() -&gt; None:\n    \"\"\"Initialize the cap mapping file if it doesn't exist.\"\"\"\n    global CAP_MAPPING\n    ensure_config_dir()\n    if not CUSTOM_MAPPING_PATH.exists() or CUSTOM_MAPPING_PATH.stat().st_size == 0:\n        save_custom_mapping(DEFAULT_CAP_MAPPING)\n    load_custom_mapping()\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.initialize_comet_ml_experiment","title":"<code>initialize_comet_ml_experiment(project_name: str) -&gt; Experiment</code>","text":"<p>Initialize a CometML experiment for logging.</p> <p>This function creates a CometML Experiment instance using the provided project name and the COMET_API_KEY environment variable.</p>"},{"location":"api_docs/#src.capfinder.utils.initialize_comet_ml_experiment--parameters","title":"Parameters:","text":"<p>project_name: str     The name of the CometML project.</p>"},{"location":"api_docs/#src.capfinder.utils.initialize_comet_ml_experiment--returns","title":"Returns:","text":"<p>Experiment:     An instance of the CometML Experiment class.</p>"},{"location":"api_docs/#src.capfinder.utils.initialize_comet_ml_experiment--raises","title":"Raises:","text":"<p>ValueError:     If the project_name is empty or None, or if the COMET_API_KEY is not set. RuntimeError:     If there's an error initializing the experiment.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def initialize_comet_ml_experiment(project_name: str) -&gt; Experiment:\n    \"\"\"\n    Initialize a CometML experiment for logging.\n\n    This function creates a CometML Experiment instance using the provided\n    project name and the COMET_API_KEY environment variable.\n\n    Parameters:\n    -----------\n    project_name: str\n        The name of the CometML project.\n\n    Returns:\n    --------\n    Experiment:\n        An instance of the CometML Experiment class.\n\n    Raises:\n    -------\n    ValueError:\n        If the project_name is empty or None, or if the COMET_API_KEY is not set.\n    RuntimeError:\n        If there's an error initializing the experiment.\n    \"\"\"\n    if not project_name:\n        raise ValueError(\"Project name cannot be empty or None\")\n\n    comet_api_key = os.getenv(\"COMET_API_KEY\")\n\n    if not comet_api_key:\n        logger.error(\n            \"CometML API key is not set. Please set the COMET_API_KEY environment variable.\"\n        )\n        logger.info(\"Example: export COMET_API_KEY='YOUR_API_KEY'\")\n        raise ValueError(\"COMET_API_KEY environment variable is not set\")\n\n    try:\n        experiment = Experiment(\n            api_key=comet_api_key,\n            project_name=project_name,\n            auto_output_logging=\"native\",\n            auto_histogram_weight_logging=True,\n            auto_histogram_gradient_logging=False,\n            auto_histogram_activation_logging=False,\n            display_summary_level=0,\n        )\n        logger.info(\n            f\"Successfully initialized CometML experiment for project: {project_name}\"\n        )\n        return experiment\n    except Exception as e:\n        logger.error(f\"Failed to initialize CometML experiment: {str(e)}\")\n        raise RuntimeError(f\"Failed to initialize CometML experiment: {str(e)}\") from e\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.is_cap_name_unique","title":"<code>is_cap_name_unique(new_cap_name: str) -&gt; Optional[int]</code>","text":"<p>Check if the given cap name is unique among existing cap mappings.</p> <p>Args: new_cap_name (str): The new cap name to check for uniqueness.</p> <p>Returns: Optional[int]: The integer label of the existing cap with the same name, if any. None otherwise.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def is_cap_name_unique(new_cap_name: str) -&gt; Optional[int]:\n    \"\"\"\n    Check if the given cap name is unique among existing cap mappings.\n\n    Args:\n    new_cap_name (str): The new cap name to check for uniqueness.\n\n    Returns:\n    Optional[int]: The integer label of the existing cap with the same name, if any. None otherwise.\n    \"\"\"\n    global CAP_MAPPING\n    for cap_int, cap_name in CAP_MAPPING.items():\n        if cap_name.lower() == new_cap_name.lower():\n            return cap_int\n    return None\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.load_custom_mapping","title":"<code>load_custom_mapping() -&gt; None</code>","text":"<p>Load custom mapping from JSON file if it exists.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def load_custom_mapping() -&gt; None:\n    \"\"\"Load custom mapping from JSON file if it exists.\"\"\"\n    global CAP_MAPPING\n    try:\n        if CUSTOM_MAPPING_PATH.exists():\n            with CUSTOM_MAPPING_PATH.open(\"r\") as f:\n                loaded_mapping = json.load(f)\n            # Convert string keys back to integers\n            CAP_MAPPING = {int(k): v for k, v in loaded_mapping.items()}\n        else:\n            CAP_MAPPING = DEFAULT_CAP_MAPPING.copy()\n    except json.JSONDecodeError:\n        logger.error(\n            \"Failed to decode JSON from custom mapping file. Using default mapping.\"\n        )\n        CAP_MAPPING = DEFAULT_CAP_MAPPING.copy()\n    except Exception as e:\n        logger.error(\n            f\"Unexpected error loading custom mapping: {e}. Using default mapping.\"\n        )\n        CAP_MAPPING = DEFAULT_CAP_MAPPING.copy()\n\n    if not CAP_MAPPING:\n        logger.warning(\"Loaded mapping is empty. Using default mapping.\")\n        CAP_MAPPING = DEFAULT_CAP_MAPPING.copy()\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.log_header","title":"<code>log_header(text: str) -&gt; None</code>","text":"<p>Log a centered header surrounded by '=' characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be displayed in the header.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def log_header(text: str) -&gt; None:\n    \"\"\"\n    Log a centered header surrounded by '=' characters.\n\n    Args:\n        text (str): The text to be displayed in the header.\n\n    Returns:\n        None\n    \"\"\"\n    width = get_terminal_width()\n    header = f\"\\n{'=' * width}\\n{text.center(width)}\\n{'=' * width}\"\n    logger.info(header)\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.log_output","title":"<code>log_output(description: str) -&gt; None</code>","text":"<p>Log a step in a multi-step process.</p> <p>Parameters:</p> Name Type Description Default <code>description</code> <code>str</code> <p>A description of the current step.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def log_output(description: str) -&gt; None:\n    \"\"\"\n    Log a step in a multi-step process.\n\n    Args:\n        description (str): A description of the current step.\n\n    Returns:\n        None\n    \"\"\"\n    width = get_terminal_width()\n    text = f\"\\n{'-' * width}\\n{description}\"\n    logger.info(text)\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.log_step","title":"<code>log_step(step_num: int, total_steps: int, description: str) -&gt; None</code>","text":"<p>Log a step in a multi-step process.</p> <p>Parameters:</p> Name Type Description Default <code>step_num</code> <code>int</code> <p>The current step number.</p> required <code>total_steps</code> <code>int</code> <p>The total number of steps.</p> required <code>description</code> <code>str</code> <p>A description of the current step.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def log_step(step_num: int, total_steps: int, description: str) -&gt; None:\n    \"\"\"\n    Log a step in a multi-step process.\n\n    Args:\n        step_num (int): The current step number.\n        total_steps (int): The total number of steps.\n        description (str): A description of the current step.\n\n    Returns:\n        None\n    \"\"\"\n    width = get_terminal_width()\n    step = (\n        f\"\\n{'-' * width}\\nStep {step_num}/{total_steps}: {description}\\n{'-' * width}\"\n    )\n    logger.info(step)\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.log_subheader","title":"<code>log_subheader(text: str) -&gt; None</code>","text":"<p>Log a centered subheader surrounded by '-' characters.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to be displayed in the header.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def log_subheader(text: str) -&gt; None:\n    \"\"\"\n    Log a centered subheader surrounded by '-' characters.\n\n    Args:\n        text (str): The text to be displayed in the header.\n\n    Returns:\n        None\n    \"\"\"\n    width = get_terminal_width()\n    header = f\"\\n{'-' * width}\\n{text.center(width)}\\n{'-' * width}\"\n    logger.info(header)\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.log_substep","title":"<code>log_substep(text: str) -&gt; None</code>","text":"<p>Log a substep or bullet point.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text of the substep to be logged.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def log_substep(text: str) -&gt; None:\n    \"\"\"\n    Log a substep or bullet point.\n\n    Args:\n        text (str): The text of the substep to be logged.\n\n    Returns:\n        None\n    \"\"\"\n    logger.info(f\"  \u2022 {text}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.map_cap_int_to_name","title":"<code>map_cap_int_to_name(cap_class: int) -&gt; str</code>","text":"<p>Map the integer representation of the CAP class to the CAP name.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def map_cap_int_to_name(cap_class: int) -&gt; str:\n    \"\"\"Map the integer representation of the CAP class to the CAP name.\"\"\"\n    global CAP_MAPPING\n\n    return CAP_MAPPING.get(cap_class, f\"Unknown cap: {cap_class}\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.open_database","title":"<code>open_database(database_path: str) -&gt; Tuple[sqlite3.Connection, sqlite3.Cursor]</code>","text":"<p>Open the database connection based on the database path.</p> <p>Parameters:</p> Name Type Description Default <code>database_path</code> <code>str</code> <p>Path to the database.</p> required <p>Returns:</p> Name Type Description <code>conn</code> <code>Connection</code> <p>Connection object for the database.</p> <code>cursor</code> <code>Cursor</code> <p>Cursor object for the database.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def open_database(\n    database_path: str,\n) -&gt; Tuple[sqlite3.Connection, sqlite3.Cursor]:\n    \"\"\"\n    Open the database connection based on the database path.\n\n    Params:\n        database_path (str): Path to the database.\n\n    Returns:\n        conn (sqlite3.Connection): Connection object for the database.\n        cursor (sqlite3.Cursor): Cursor object for the database.\n    \"\"\"\n    conn = sqlite3.connect(database_path)\n    cursor = conn.cursor()\n    return conn, cursor\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.save_custom_mapping","title":"<code>save_custom_mapping(mapping: Dict[int, str]) -&gt; None</code>","text":"<p>Save the given mapping to JSON file.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def save_custom_mapping(mapping: Dict[int, str]) -&gt; None:\n    \"\"\"Save the given mapping to JSON file.\"\"\"\n    ensure_config_dir()\n    try:\n        with CUSTOM_MAPPING_PATH.open(\"w\") as f:\n            json.dump(mapping, f, indent=2)\n    except Exception as e:\n        logger.error(f\"Failed to save custom mapping: {e}\")\n        raise\n</code></pre>"},{"location":"api_docs/#src.capfinder.utils.update_cap_mapping","title":"<code>update_cap_mapping(new_mapping: Dict[int, str]) -&gt; None</code>","text":"<p>Update the CAP_MAPPING with new entries.</p> Source code in <code>src/capfinder/utils.py</code> <pre><code>def update_cap_mapping(new_mapping: Dict[int, str]) -&gt; None:\n    \"\"\"Update the CAP_MAPPING with new entries.\"\"\"\n    global CAP_MAPPING\n    CAP_MAPPING.update(new_mapping)\n    save_custom_mapping(CAP_MAPPING)\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns","title":"<code>visualize_alns</code>","text":"<p>This module helps us to visualize the alignments of reads to a reference sequence. The module reads a FASTQ file or folder of FASTQ files, processes each read in parallel, and writes the output to a file. The output file contains the read ID, average quality, sequence, alignment score, and alignment string.</p> <p>This module is useful in understandig the output of Parasail alignment.</p> <p>Author: Adnan M. Niazi Date: 2024-02-28</p>"},{"location":"api_docs/#src.capfinder.visualize_alns.calculate_average_quality","title":"<code>calculate_average_quality(quality_scores: Sequence[Union[int, float]]) -&gt; float</code>","text":"<p>Calculate the average quality score for a read. Args:     quality_scores (Sequence[Union[int, float]]): A list of quality scores for a read. Returns:     average_quality (float): The average quality score for a read.</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def calculate_average_quality(quality_scores: Sequence[Union[int, float]]) -&gt; float:\n    \"\"\"\n    Calculate the average quality score for a read.\n    Args:\n        quality_scores (Sequence[Union[int, float]]): A list of quality scores for a read.\n    Returns:\n        average_quality (float): The average quality score for a read.\n    \"\"\"\n    average_quality = sum(quality_scores) / len(quality_scores)\n    return average_quality\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns.process_fastq_file","title":"<code>process_fastq_file(fastq_filepath: str, reference: str, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process a single FASTQ file. The function reads the FASTQ file, processes each read in parallel. The output is a file containing the read ID, average quality, sequence, alignment score, and alignment string.</p> <p>Parameters:</p> Name Type Description Default <code>fastq_filepath</code> <code>str</code> <p>The path to the FASTQ file.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where the output file will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def process_fastq_file(\n    fastq_filepath: str, reference: str, num_processes: int, output_folder: str\n) -&gt; None:\n    \"\"\"\n    Process a single FASTQ file. The function reads the FASTQ file, processes each read in parallel.\n    The output is a file containing the read ID, average quality, sequence, alignment score, and alignment string.\n\n    Args:\n        fastq_filepath (str): The path to the FASTQ file.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where the output file will be stored.\n\n    Returns:\n        None\n    \"\"\"\n\n    # Make output file name\n    # Make output_folder if it does not exist already\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    directory, filename = os.path.split(fastq_filepath)\n    filename_no_extension, extension = os.path.splitext(filename)\n    output_filepath = os.path.join(\n        output_folder, f\"{filename_no_extension}_alignments.txt\"\n    )\n\n    with file_opener(fastq_filepath) as fastq_file:\n        records = list(SeqIO.parse(fastq_file, \"fastq\"))\n        total_records = len(records)\n\n        with WorkerPool(n_jobs=num_processes) as pool:\n            results = pool.map(\n                process_read,\n                [(item, reference) for item in records],\n                iterable_len=total_records,\n                progress_bar=True,\n            )\n            write_ouput(results, output_filepath)\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns.process_fastq_folder","title":"<code>process_fastq_folder(folder_path: str, reference: str, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process all FASTQ files in a folder. The function reads all FASTQ files in a folder, processes each read in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>The path to the folder containing FASTQ files.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where the output file will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def process_fastq_folder(\n    folder_path: str, reference: str, num_processes: int, output_folder: str\n) -&gt; None:\n    \"\"\"\n    Process all FASTQ files in a folder. The function reads all FASTQ files in a folder, processes each read in parallel.\n\n    args:\n        folder_path (str): The path to the folder containing FASTQ files.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where the output file will be stored.\n\n    returns:\n        None\n    \"\"\"\n    # List all files in the folder\n    for root, _, files in os.walk(folder_path):\n        for file_name in files:\n            if file_name.endswith((\".fastq\", \".fastq.gz\")):\n                file_path = os.path.join(root, file_name)\n                process_fastq_file(file_path, reference, num_processes, output_folder)\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns.process_fastq_path","title":"<code>process_fastq_path(path: str, reference: str, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Process a FASTQ file or folder of FASTQ files based on the provided path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTQ file or folder.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where the output file will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def process_fastq_path(\n    path: str, reference: str, num_processes: int, output_folder: str\n) -&gt; None:\n    \"\"\"\n    Process a FASTQ file or folder of FASTQ files based on the provided path.\n\n    args:\n        path (str): The path to the FASTQ file or folder.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where the output file will be stored.\n\n    returns:\n        None\n    \"\"\"\n    if os.path.isfile(path):\n        # Process a single FASTQ file\n        process_fastq_file(path, reference, num_processes, output_folder)\n    elif os.path.isdir(path):\n        # Process all FASTQ files in a folder\n        process_fastq_folder(path, reference, num_processes, output_folder)\n    else:\n        print(\"Invalid path. Please provide a valid FASTQ file or folder path.\")\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns.process_read","title":"<code>process_read(record: Any, reference: str) -&gt; str</code>","text":"<p>Process a single read from a FASTQ file. The function calculates average read quality, alignment score, and alignment string. The output is a string that can be written to a file.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Any</code> <p>A single read from a FASTQ file.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <p>Returns:     output_string (str): A string containing the read ID, average quality, sequence,                         alignment score, and alignment string.</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def process_read(record: Any, reference: str) -&gt; str:\n    \"\"\"\n    Process a single read from a FASTQ file. The function calculates average read quality,\n    alignment score, and alignment string. The output is a string that can be written to a file.\n\n    Args:\n        record (Any): A single read from a FASTQ file.\n        reference (str): The reference sequence to align the read to.\n    Returns:\n        output_string (str): A string containing the read ID, average quality, sequence,\n                            alignment score, and alignment string.\n    \"\"\"\n    read_id = record.id\n    quality_scores = record.letter_annotations[\"phred_quality\"]\n    average_quality = round(calculate_average_quality(quality_scores))\n    sequence = str(record.seq)\n    with contextlib.redirect_stdout(None):\n        _, _, chunked_aln_str, alignment_score = align(\n            query_seq=sequence, target_seq=reference, pretty_print_alns=True\n        )\n\n    output_string = f\"&gt;{read_id} {average_quality:.0f}\\n{sequence}\\n\\n\"\n    output_string += f\"Alignment Score: {alignment_score}\\n\"\n    output_string += f\"{chunked_aln_str}\\n\"\n\n    return output_string\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns.visualize_alns","title":"<code>visualize_alns(path: str, reference: str, num_processes: int, output_folder: str) -&gt; None</code>","text":"<p>Main function to visualize alignments.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the FASTQ file or folder.</p> required <code>reference</code> <code>str</code> <p>The reference sequence to align the read to.</p> required <code>num_processes</code> <code>int</code> <p>The number of processes to use for parallel processing.</p> required <code>output_folder</code> <code>str</code> <p>The folder where the output file will be stored.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def visualize_alns(\n    path: str, reference: str, num_processes: int, output_folder: str\n) -&gt; None:\n    \"\"\"\n    Main function to visualize alignments.\n\n    Args:\n        path (str): The path to the FASTQ file or folder.\n        reference (str): The reference sequence to align the read to.\n        num_processes (int): The number of processes to use for parallel processing.\n        output_folder (str): The folder where the output file will be stored.\n\n    Returns:\n        None\n    \"\"\"\n    process_fastq_path(path, reference, num_processes, output_folder)\n</code></pre>"},{"location":"api_docs/#src.capfinder.visualize_alns.write_ouput","title":"<code>write_ouput(output_list: List[str], output_filepath: str) -&gt; None</code>","text":"<p>Write a list of strings to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output_list</code> <code>list</code> <p>A list of strings to write to a file.</p> required <code>output_filepath</code> <code>str</code> <p>The path to the output file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/capfinder/visualize_alns.py</code> <pre><code>def write_ouput(output_list: List[str], output_filepath: str) -&gt; None:\n    \"\"\"\n    Write a list of strings to a file.\n\n    Args:\n        output_list (list): A list of strings to write to a file.\n        output_filepath (str): The path to the output file.\n\n    Returns:\n        None\n    \"\"\"\n    if os.path.exists(output_filepath):\n        os.remove(output_filepath)\n    with open(output_filepath, \"a\") as f:\n        f.writelines(\"\\n\".join(output_list))\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#043-2024-08-29","title":"0.4.3 - 2024-08-29","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added a new model with higher accuracy</li> </ul>"},{"location":"changelog/#042-2024-08-27","title":"0.4.2 - 2024-08-27","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed cli arguments</li> <li>Fixed readme and documentation errors.</li> </ul>"},{"location":"changelog/#041-2024-08-23","title":"0.4.1 - 2024-08-23","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Option to use time-warped augmented data during training</li> </ul>"},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Padding and truncation algorithm is now fixed such that equal amounts of time samples are padded or truncated from both ends of the classifier examples</li> </ul>"},{"location":"changelog/#040-2024-08-19","title":"0.4.0 - 2024-08-19","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed bugs that caused slow training pipeline</li> <li>Fixed logic for uploading large dataset objects to Comet ML in small chunks</li> </ul>"},{"location":"changelog/#039-2024-08-16","title":"0.3.9 - 2024-08-16","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed a bug where all the class data files were not being used to make the dataset</li> </ul>"},{"location":"changelog/#038-2024-08-16","title":"0.3.8 - 2024-08-16","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed a bug where if the dataset dir had no dataset previously, new dataset was not being created.</li> <li>Increased CSV field size in train_etl to fix capfinder crashing during when encoutering large csv fields</li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Added logging info to standalone train ETL pipeline</li> </ul>"},{"location":"changelog/#037-2024-08-14","title":"0.3.7 - 2024-08-14","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed bugs in collate function that caused hogging of memory for large BAM file</li> </ul>"},{"location":"changelog/#036-2024-08-13","title":"0.3.6 - 2024-08-13","text":""},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Fixed missing headers from some commands in the cli</li> </ul>"},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Added option to specify custom models during inference</li> <li>Added more documentation</li> </ul>"},{"location":"changelog/#035-2024-08-11","title":"0.3.5 - 2024-08-11","text":""},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Fixed string formatting issue</li> </ul>"},{"location":"changelog/#034-2024-08-11","title":"0.3.4 - 2024-08-11","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Added ability to add more cap types to training</li> <li>Added a new train ETL pipeline that can handle larger than memory datasets</li> <li>Added improved interface for training pipeline</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>Old CLI app to reflect changes in API</li> </ul>"},{"location":"changelog/#033-2024-08-08","title":"0.3.3 - 2024-08-08","text":""},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Issues with pip installation by removing the yanked package (types-pkg-resources)</li> </ul>"},{"location":"changelog/#032-2024-08-08","title":"0.3.2 - 2024-08-08","text":""},{"location":"changelog/#fixed_10","title":"Fixed","text":"<ul> <li>Issues with pip installation</li> </ul>"},{"location":"changelog/#031-2024-08-08","title":"0.3.1 - 2024-08-08","text":""},{"location":"changelog/#fixed_11","title":"Fixed","text":"<ul> <li>Issues with pip installation</li> </ul>"},{"location":"changelog/#030-2024-08-08","title":"0.3.0 - 2024-08-08","text":""},{"location":"changelog/#fixed_12","title":"Fixed","text":"<ul> <li>Issues with pip installation</li> </ul>"},{"location":"changelog/#029-2024-08-02","title":"0.2.9 - 2024-08-02","text":""},{"location":"changelog/#fixed_13","title":"Fixed","text":"<ul> <li>Fixed issue with batch inference not working</li> </ul>"},{"location":"changelog/#028-2024-08-02","title":"0.2.8 - 2024-08-02","text":""},{"location":"changelog/#fixed_14","title":"Fixed","text":"<ul> <li>Fixed loading the entire inference dataset in memory</li> </ul>"},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>Added more information to the README file</li> </ul>"},{"location":"changelog/#027-2024-08-01","title":"0.2.7 - 2024-08-01","text":""},{"location":"changelog/#fixed_15","title":"Fixed","text":"<ul> <li>Problems with cli not displaying capfinder version info</li> <li>Fixed issue with API docs generation</li> <li>Added max_examples parameter to ETL to limit the number of examples to process in a dataset (use during training)</li> </ul>"},{"location":"changelog/#026-2024-08-01","title":"0.2.6 - 2024-08-01","text":""},{"location":"changelog/#fixed_16","title":"Fixed","text":"<ul> <li>Slow report generation</li> <li>Problems with refreshing of cache</li> </ul>"},{"location":"changelog/#025-2024-07-31","title":"0.2.5 - 2024-07-31","text":""},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>Report generation</li> <li>Cli for inference</li> </ul>"},{"location":"changelog/#024-2024-07-26","title":"0.2.4 - 2024-07-26","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>Added functions for performaing inference</li> </ul>"},{"location":"changelog/#023-2024-07-17","title":"0.2.3 - 2024-07-17","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>Cosine annealing cyclic learning rate scheduler with resets, decay, and progressive lengthing of cycles</li> </ul>"},{"location":"changelog/#022-2024-07-15","title":"0.2.2 - 2024-07-15","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>Added Cyclical learning rate scheduler</li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Removed reduce learning rate on plateau</li> </ul>"},{"location":"changelog/#021-2024-07-11","title":"0.2.1 - 2024-07-11","text":""},{"location":"changelog/#added_10","title":"Added","text":"<ul> <li>Added an attention-augmented CNN-LSTM model</li> </ul>"},{"location":"changelog/#020-2024-07-10","title":"0.2.0 - 2024-07-10","text":""},{"location":"changelog/#fixed_17","title":"Fixed","text":"<ul> <li>Fixed issue with ml_libs module not found</li> </ul>"},{"location":"changelog/#019-2024-07-10","title":"0.1.9 - 2024-07-10","text":""},{"location":"changelog/#fixed_18","title":"Fixed","text":"<ul> <li>Fixing issue with ml_libs module not found</li> </ul>"},{"location":"changelog/#018-2024-07-10","title":"0.1.8 - 2024-07-10","text":""},{"location":"changelog/#added_11","title":"Added","text":"<ul> <li>Added support for resnet model</li> </ul>"},{"location":"changelog/#017-2024-07-08","title":"0.1.7 - 2024-07-08","text":""},{"location":"changelog/#fixed_19","title":"Fixed","text":"<ul> <li>Fixed encoder model hogging all available GPU memory and crashing</li> </ul>"},{"location":"changelog/#016-2024-07-07","title":"0.1.6 - 2024-07-07","text":""},{"location":"changelog/#fixed_20","title":"Fixed","text":"<ul> <li>Fixed bug with not writing dataset version to the file</li> </ul>"},{"location":"changelog/#015-2024-07-07","title":"0.1.5 - 2024-07-07","text":""},{"location":"changelog/#fixed_21","title":"Fixed","text":"<ul> <li>Bugs with using the train config functionality</li> </ul>"},{"location":"changelog/#014-2024-07-07","title":"0.1.4 - 2024-07-07","text":""},{"location":"changelog/#added_12","title":"Added","text":"<ul> <li>Added pipeline for making training data</li> <li>Added CNN-LSTM and encoder models</li> </ul>"},{"location":"changelog/#013-2023-09-23","title":"0.1.3 - 2023-09-23","text":""},{"location":"changelog/#added_13","title":"Added","text":"<ul> <li>Functions to align OTE with reads in FASTQ</li> <li>Function for pretty printing alignment for debugging purposes</li> <li>Function for finding the start and end location of ROI in training datasets</li> </ul>"},{"location":"changelog/#012-2023-08-18","title":"0.1.2 - 2023-08-18","text":""},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Changed function arg names again for testing version bump</li> </ul>"},{"location":"changelog/#011-2023-08-16","title":"0.1.1 - 2023-08-16","text":""},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Changed function arg names for testing version bump</li> </ul>"},{"location":"changelog/#010-2023-08-15","title":"0.1.0 - 2023-08-15","text":""},{"location":"changelog/#added_14","title":"Added","text":"<ul> <li>Basic skeleton of the package and tested it</li> </ul>"},{"location":"contributions/","title":"Our Contributors","text":"<p>Capfinder's success and development would not have been possible without the efforts and contributions of the following people:</p> <ol> <li>Adnan Niazi</li> <li>Eivind Valen</li> <li>Maximillian Krause</li> <li>Jan Inge \u00d8verbo</li> <li>Teshome Bizuayehu</li> <li>Kersti Nilsson</li> </ol>"},{"location":"development_setup/","title":"Developer Guide","text":""},{"location":"development_setup/#clone-this-repository","title":"Clone this repository","text":"<p><pre><code>git clone https://github.com/adnaniazi/capfinder.git\n</code></pre> Next, <code>cd</code> into the clone repo.</p>"},{"location":"development_setup/#creating-dev-enviornment","title":"Creating dev enviornment","text":"<pre><code>micromamba create -n capfinder_env python=3.12\nmicromamba activate capfinder_env\n</code></pre>"},{"location":"development_setup/#installation","title":"Installation","text":"<p>First install <code>poetry</code>:</p> <pre><code>pip install poetry\n</code></pre> <p>Next install appropriate version of capfinder based on your hardware configuration:</p> CPUGPU (CUDA 12)TPU <pre><code>poetry install --extras cpu\n</code></pre> <pre><code>poetry install --extras gpu\npoetry run pip install \"jax[cuda12]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n</code></pre> <pre><code>poetry install --extras tpu\npoetry run pip install \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n</code></pre>"},{"location":"development_setup/#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"development_setup/#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the <code>docs</code> directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github project page   automatically as part each release.</p>"},{"location":"development_setup/#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"development_setup/#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatters (e.g. <code>black</code>, <code>isort</code>), linters (e.g. <code>mypy</code>, <code>flake8</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"extend_capmap/","title":"3. Extending Cap Mapping","text":"<p>Before retraining the classifier for a new cap type, you must first extend the cap mapping. This mapping associates cap's name with an integer label. The integer label used in model training.</p>"},{"location":"extend_capmap/#current-cap-mapping","title":"Current Cap Mapping","text":"<p>The current default cap mapping that can be found by runing <code>capfinder capmap list</code> command, and is as follows:</p> <pre><code>-99: cap_unknown\n0: cap_0\n1: cap_1\n2: cap_2\n3: cap_2-1\n</code></pre> <p>We strongly recommend not changing the mapping of existing caps. Instead, extend the existing mapping to accomodate new cap types as needed.</p>"},{"location":"extend_capmap/#adding-a-new-cap-type","title":"Adding a New Cap Type","text":"<p>To add a new cap type, use the <code>capmap add</code> command:</p> <pre><code>capfinder capmap add &lt;integer&gt; &lt;cap_name&gt;\n</code></pre>"},{"location":"extend_capmap/#adding-a-new-cap-type_1","title":"Adding a New Cap Type","text":"<p>To add a new cap type, use the <code>capmap add</code> command:</p> <pre><code>capfinder capmap add 4 cap_0m6A\n</code></pre>"},{"location":"extend_capmap/#verifying-the-new-cap-type","title":"Verifying the New Cap Type","text":"<p>After adding the new cap type, you can verify it has been added correctly:</p> <pre><code>capfinder capmap list\n</code></pre> <p>You should see your new cap type in the list of current cap mappings:</p> <pre><code>Current cap mappings:\n-99: cap_unknown\n0: cap_0\n1: cap_1\n2: cap_2\n3: cap_2-1\n4: cap_0m6A\n</code></pre>"},{"location":"extract_train_signal/","title":"4. Extracting Cap Signal","text":"<p>After synthesizing and sequencing your custom oligos, the next step is to collate BAM and POD5 files and extract the cap signal for the cap and flanking bases. This process is crucial for training the model to recognize new cap types.</p>"},{"location":"extract_train_signal/#signal-extraction-process","title":"Signal Extraction Process","text":"<p>Capfinder extracts the signal for the cap and 5 flanking bases, which is then used to train the model. For example, with a Cap0-m6A oligo, Capfinder will extract the signal for <code>ACUGUm6A1N2N3N4N5N6</code> (i.e., the cap and flanking 5 bases).</p> <p>This approach is consistent with the method used for cap 0, cap 1, cap 2, and cap 2-1 classes in the pretrained model.</p>"},{"location":"extract_train_signal/#command-for-signal-extraction","title":"Command for Signal Extraction","text":"<p>To extract the cap signal, use the <code>capfinder extract-cap-signal</code> command. Here's an example command for our new class (Cap0-m6A):</p> <pre><code>capfinder extract-cap-signal \\\n    --bam_filepath /path/to/sorted.bam \\\n    --pod5_dir /path/to/pod5_dir \\\n    --reference GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGTANNNNNCGATGTAACTGGGACATGGTGAGCAATCAGGGAAAAAAAAAAAAAAA \\\n    --cap_class 4 \\\n    --cap_n1_pos0 52 \\\n    --train_or_test train \\\n    --output_dir /path/to/output_dir \\\n    --n_workers 10 \\\n    --no-plot-signal \\\n    --no-debug\n</code></pre>"},{"location":"extract_train_signal/#command-parameters-explained","title":"Command Parameters Explained","text":"<ul> <li><code>--bam_filepath</code>: Path to the sorted BAM file containing aligned reads.</li> <li><code>--pod5_dir</code>: Directory containing POD5 files with raw signal data.</li> <li><code>--reference</code>: The reference sequence of the oligo. Note that <code>ANNNNN</code> represents the cap and variable bases.</li> <li><code>--cap_class</code>: Integer representing the new cap class (4 in this example because we added this class label previously using <code>capmap add</code> command).</li> <li><code>--cap_n1_pos0</code>: 0-based position of the first cap base <code>N1</code> in the reference sequence (52 in this case corresponding to the <code>A</code> of the <code>m6A</code> moeity).</li> <li><code>--train_or_test</code>: Set to <code>train</code> for creating training data.</li> <li><code>--output_dir</code>: Directory where the extracted data will be saved.</li> <li><code>--n_workers</code>: Number of CPU cores to use for parallel processing.</li> <li><code>--no-plot-signal</code>: Option to skip generating signal plots (speeds up processing).</li> <li><code>--no-debug</code>: Disable debug mode for more detailed logging</li> </ul>"},{"location":"extract_train_signal/#output","title":"Output","text":"<p>The command will generate two main CSV files in the specified output directory:</p> <ol> <li><code>data__cap_0m6A.csv</code>: Contains the extracted ROI signal data.</li> <li><code>metadata__cap_0m6A.csv</code>: Contains complete metadata information.</li> </ol> <p>Additionally, a log file (<code>capfinder_vXYZ_datetime.log</code>) will be created with program execution details.</p>"},{"location":"installation/","title":"Installation","text":"<p>Before installing capfinder, please make a fresh conda/micromamba env with required supported Python versions like so: <pre><code>micromamba create -n capfinder_env python=3.12\n</code></pre> Here we have created a Python 3.12 environment using micromamba. Next, we activate the newly created conda env: <pre><code>micromamba activate capfinder_env\n</code></pre></p> <p>In the activated environment, Capfinder can be installed with support for different hardware configurations. Choose the appropriate installation tab based on your hardware setup. This ensures you get the right dependencies for optimal performance on your system:</p> CPU-onlyGPU (CUDA 12)TPU <pre><code>pip install capfinder -U jax\n</code></pre> <p>This installation is suitable when you only have CPU and not GPUs.</p> <pre><code>pip install capfinder -U \"jax[cuda12]\"\n</code></pre> <p>Use this installation for systems with NVIDIA GPUs supporting CUDA 12. Capfinder depends on JAX internally for using GPUs. JAX requires CUDA to work. CUDA requirements for Capfinder are the same as the CUDA requirements for JAX.</p> <p>For more information on the required CUDA version for JAX, refer to the JAX installation guide.</p> <pre><code>pip install capfinder -U \"jax[tpu]\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n</code></pre> <p>This installation is for systems with TPU (Tensor Processing Unit) hardware.</p> <p>Note</p> <p>Make sure to choose the installation command that matches your hardware configuration for optimal performance.</p>"},{"location":"installation/#build-errors-and-solutions","title":"Build Errors and Solutions","text":"<p>When installing Capfinder, you may encounter build errors, particularly related to the pysam dependency. Below are two common issues and their solutions:</p> Could not build wheels for pysam ErrorsCompiler flag issues (-std=c99 or -std=gnu99) <p>If you encounter the following error:</p> <pre><code>ImportError: /tmp/pip-build-env-qsdot3t6/overlay/lib/python3.12/site-packages/Cython/Utils.cp\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nERROR: Failed building wheel for pysam\n\nFailed to build pysam\n\nERROR: Could not build wheels for pysam, which is required to install pyproject.toml-based projects\n</code></pre> <p>You should first do the following in terminal:</p> <pre><code>mkdir \"/path/to/temp/directory\"\nchmod +x \"/path/to/temp/directory\"\nexport TMPDIR=\"/path/to/temp/directory\"\n</code></pre> <p>Now try to reinstall <code>capfinder</code></p> <p>If you encounter the following error:</p> <pre><code>note: use option -std=c99 or -std=gnu99 to compile your code\nerror: command '/usr/bin/gcc' failed with exit code 1\n[end of output]\n\nnote: This error originates from a subprocess, and is likely not a problem with pip.\nERROR: Failed building wheel for pysam\nFailed to build pysam\nERROR: Could not build wheels for pysam, which is required to install pyproject.toml-based projects\n</code></pre> <p>You should first do the following in terminal:</p> <pre><code>export CFLAGS=\"-std=c99 -D_GNU_SOURCE $CFLAGS\"\nexport LDFLAGS=\"-std=c99 $LDFLAGS\"\n</code></pre> <p>Now try to reinstall <code>capfinder</code></p>"},{"location":"mRNA_sample_prep/","title":"1. mRNA Sample Preparation","text":"<p>For Cap type predictions in mRNA, the mRNA sample must first be prepared according to the following specifications:</p> <ol> <li> <p>Decapping: The m7G moiety at the 5' end of the mRNA must be removed through a decapping process.</p> </li> <li> <p>Oligonucleotide Extension (OTE): A specific 52-oligonucleotide sequence must be ligated to the 5' end of each mRNA molecule. The OTE sequence is: <pre><code>5'-GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGU-3'\n</code></pre></p> </li> <li>Sequencing: Samples should be sequenced using Oxford Nanopore Technologies (ONT) SQK-RNA004 chemistry.</li> </ol> <p>By adhering to these preparation guidelines, researchers can maximize the accuracy and reliability of Capfinder's predictions, enabling deeper insights into RNA cap structures and their biological significance.</p>"},{"location":"oligo_design/","title":"1. Designing Cap Oligo","text":"<p>Capfinder allows you to retrain the model to include additional cap types. This feature is particularly useful for researchers working with novel or less common cap structures that are not included in the pretrained model that is shipped with capfinder.</p>"},{"location":"oligo_design/#how-was-the-data-for-current-pretrained-model-generated","title":"How was the data for current pretrained model generated?","text":"<p>The classifier has currently been trained on 4 different cap classes:</p> <ol> <li>Cap 0</li> <li>Cap 1</li> <li>Cap 2</li> <li>Cap 2-1</li> </ol>"},{"location":"oligo_design/#synthesized-oligos-for-training-data","title":"Synthesized Oligos for Training Data","text":"<p>Data for these caps was generated by sequencing the following synthesized oligos:</p> Cap Type Oligo Sequence (5' \u2192 3') Cap 0 <code>GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGUN1N2N3N4N5N6CGAUGUAACUGGGACAUGGUGAGCAAUCAGGGAAAAAAAAAAAAAAA</code> Cap 1 <code>GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGUmN1N2N3N4N5N6CGAUGUAACUGGGACAUGGUGAGCAAUCAGGGAAAAAAAAAAAAAAA</code> Cap 2 <code>GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGUmN1mN2N3N4N5N6CGAUGUAACUGGGACAUGGUGAGCAAUCAGGGAAAAAAAAAAAAAAA</code> Cap 2-1 <code>GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGUN1mN2N3N4N5N6CGAUGUAACUGGGACAUGGUGAGCAAUCAGGGAAAAAAAAAAAAAAA</code> <p>Where:</p> <ul> <li><code>N</code> represents any of the four RNA bases (A, U, C, G)</li> <li><code>m</code> preceding a base indicates 2'-O methylation</li> </ul>"},{"location":"oligo_design/#key-considerations-for-custom-oligo-design","title":"Key Considerations for Custom Oligo Design","text":"<p>When designing custom oligos for new cap types, keep in mind:</p> <ol> <li> <p>OTE Sequence Requirement</p> </li> <li> <p>The OTE sequence <code>5'-GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGU-3'</code> must be present to the left of the cap.</p> </li> <li> <p>Sequence After NNNs</p> <ul> <li>The sequence following the NNNs part (<code>5'-CGAUGUAACUGGGACAUGGUGAGCAAUCAGGGAAAAAAAAAAAAAAA-3'</code>) can be customized.</li> <li>Recommendations:<ul> <li>Include a synthesized polyA tail</li> <li>Keep polyA tail length \u2264 15 bases</li> <li>Ensure total construct length \u2265 105nt for adequate basecall sequencing quality</li> </ul> </li> </ul> </li> <li> <p>Cap and N Bases</p> <ul> <li>The cap base(s) and following <code>N</code> bases should total 6.</li> <li>This ensures cap bases exist in all possible 4/5-mer contexts to the right.</li> <li>The context to the left of the cap is fixed due to the OTE.</li> </ul> </li> </ol>"},{"location":"oligo_design/#example-oligo-sequence-for-a-new-cap-type-cap0-m6a","title":"Example: Oligo Sequence for a New Cap Type (Cap0-m6A)","text":"<p>Let us say you want to extend the classifier to account for a fifth cap - Cap0-m6A. For this cap, the synthesized oligo should resemble the following if we take into consideration the requirements we have explained above:</p> <pre><code>5'-GCUUUCGUUCGUCUCCGGACUUAUCGCACCACCUAUCCAUCAUCAGUACUGUm6A1N2N3N4N5N6CGAUGUAACUGGGACAUGGUGAGCAAUCAGGGAAAAAAAAAAAAAAA-3'\n</code></pre> <p>Note: <code>N1</code> is replaced with <code>m6A1</code>.</p> <p>Next, we will extend cap mappings capfinder uses to accomodate the new cap type.</p>"},{"location":"prediction/","title":"3. Cap Prediction","text":"<p>Capfinder provides a command-line interface to predict RNA cap types using BAM and POD5 files. Here's how to use the <code>predict-cap-types</code> function:</p>"},{"location":"prediction/#usage","title":"Usage","text":"<pre><code>capfinder predict-cap-types [OPTIONS]\n</code></pre>"},{"location":"prediction/#description","title":"Description","text":"<p>This command predicts RNA cap types using BAM and POD5 files.</p>"},{"location":"prediction/#required-options","title":"Required Options","text":"<ul> <li> <p><code>--bam_filepath</code> or <code>-b</code>: Path to the BAM file generated using the preprocessing step</p> </li> <li> <p><code>--pod5_dir</code> or <code>-p</code>: Path to directory containing POD5 files</p> </li> <li> <p><code>--output_dir</code> or <code>-o</code>: Path to the output directory for prediction results and logs</p> </li> </ul>"},{"location":"prediction/#additional-options","title":"Additional Options","text":"<ul> <li><code>--n_cpus</code> or <code>-n</code>: Number of CPUs to use for parallel processing. Default is 1</li> </ul> Multiple CPUs are used during processing for POD5 file and BAM data (Step 1/5). Increasing this number speeds up POD5 and BAM processing. For inference (Step 4/5), only a single CPU is used no matter how many CPUs you have specified. For faster inference, have a GPU available (it will be detected automatically) and set dtype to <code>float16</code> <ul> <li><code>--dtype</code> or <code>-d</code>: Data type for model input. Valid values are <code>float16</code>, <code>float32</code>, or <code>float64</code>. Default is <code>float16</code></li> </ul> Without a GPU, use <code>float32</code> or <code>float64</code> for better performance. If you have a GPU, then use <code>float16</code> for faster inference <ul> <li><code>--batch_size</code> or <code>-bs</code>: Batch size for model inference. Default is <code>128</code></li> </ul> Larger batch sizes can speed up inference but require more memory. If the code crashes during step 4/5, you have probably set too high a batch size. <ul> <li> <p><code>--plot-signal</code> / <code>--no-plot-signal</code>: Whether to plot extracted cap signal or not. Default is <code>--no-plot-signal</code></p> </li> <li> <p><code>--custom_model_path</code> or <code>-m</code>: Path to a custom model (.keras) file. If not provided, the default pre-packaged model will be used.</p> </li> </ul> Saving plots can help you plot the read's signal, and plot the signal for cap and flanking bases(\u00b15). <ul> <li><code>--debug</code> / <code>--no-debug</code>: Enable debug mode for more detailed logging. Default is <code>--no-debug</code></li> </ul> The option can prints which function is creating a particular log output. This is helpful during code debugging. <ul> <li><code>--refresh-cache</code> / <code>--no-refresh-cache</code>: Refresh the cache for intermediate results. Default is <code>--no-refresh-cache</code></li> </ul> If you input data has changed (for example you added one more POD5 file in your POD5 directory) then you must use <code>--refresh-cache</code> to compute all steps again and not load them from cache that hold results from your previous run. <ul> <li><code>--help</code>: Show the help message and exit</li> </ul> <p>Example</p> <pre><code>capfinder predict-cap-types \\\n    --bam_filepath /path/to/sorted.bam \\\n    --pod5_dir /path/to/pod5_dir \\\n    --output_dir /path/to/output_dir \\\n    --n_cpus 100 \\\n    --dtype float16 \\\n    --batch_size 256 \\\n    --no_plot_signal \\\n    --no-debug \\\n    --no-refresh-cache\n</code></pre> <p>Tips</p> <ol> <li> <p>CPU Usage:</p> <ul> <li>Increase <code>--n_cpus</code> for faster processing of POD5 and BAM data</li> <li>CPU count doesn't affect inference speed (Step 4/5)</li> </ul> </li> <li> <p>GPU Acceleration:</p> <ul> <li>If you have a GPU, use <code>--dtype float16</code> for faster inference</li> <li>Without a GPU, <code>float32</code> or <code>float64</code> may perform better</li> </ul> </li> <li> <p>Batch Size:</p> <ul> <li>Larger batch sizes can speed up inference but require more memory</li> <li>Adjust <code>--batch_size</code> based on your system's capabilities</li> </ul> </li> <li> <p>Plotting:</p> <ul> <li>Use <code>--no-plot-signal</code> to skip signal plotting for faster processing</li> </ul> </li> <li> <p>Debugging:</p> <ul> <li>Enable <code>--debug</code> for detailed logging when troubleshooting</li> </ul> </li> <li> <p>Caching:</p> <ul> <li>Use <code>--refresh-cache</code> if you've made changes to input data and need to regenerate intermediate results</li> </ul> </li> </ol> <p>For more detailed information, run <code>capfinder predict-cap-types --help</code>.</p>"},{"location":"preprocessing/","title":"2. Data Preparation","text":"<p>Capfinder requires detailed information about base movements during sequencing, which is captured in the \"moves table\". This information is crucial for accurate cap type prediction.</p>"},{"location":"preprocessing/#requirements","title":"Requirements","text":"<ul> <li>Raw POD5 files from your sequencing run</li> <li>Dorado basecaller (version 0.7.0 or later recommended)</li> <li>Samtools (version 1.18 or later recommended)</li> <li>A transcriptome reference</li> </ul>"},{"location":"preprocessing/#important-note","title":"Important Note","text":"<p>Dorado Live Basecalling does not generate the necessary moves information. Therefore, post-data acquisition standalone basecalling and alignment process is required to produce the moves information in the alignment SAM/BAM file.</p>"},{"location":"preprocessing/#basecalling-process","title":"Basecalling Process","text":"<p>To generate the required data, we use a custom script that performs the following steps:</p> <ol> <li> <p>Basecalls the raw POD5 files using Dorado</p> </li> <li> <p>Emits SAM format output with moves information</p> </li> <li> <p>Aligns the basecalled reads to a reference genome</p> </li> <li> <p>Converts the output to a sorted and indexed BAM file</p> </li> </ol>"},{"location":"preprocessing/#basecalling-script","title":"Basecalling Script","text":"<p>Below is a script that automates this process. You'll need to adjust the paths and settings to match your environment:</p> <p>basecall.sh</p> <pre><code>#!/bin/bash\n# Please edit to reflect your settings\nDORADO=\"/path/to/bin/dorado\"\nSAMTOOLS=\"/path/to/samtools\"\nPOD5_DIR=\"/path/to/pod5_dir\"\nREF=\"/path/to/ref.fa\"\nMODEL_NAME=\"rna004_130bps_sup@v5.0.0\"\nMODEL_DIR=\"/path/to/download/doardo/model\"\nOUTPUT_DIR=\"/path/to/save/basecalled/data\"\nDEVICE=\"cuda:all\" # For Dorado to use GPU\n\n#---------------------- DO NOT EDIT BELOW THIS LINE ---------------------#\n\n# Function to check and download the model if necessary\ncheck_and_download_model() {\n    local model_path=\"$MODEL_DIR/$MODEL_NAME\"\n    if [ ! -d \"$model_path\" ]; then\n        echo \"Model not found. Downloading...\"\n        \"$DORADO\" download --directory \"$MODEL_DIR\" --model \"$MODEL_NAME\"\n    else\n        echo \"Model already exists. Skipping download.\"\n    fi\n}\n\n# Create output directory\nmkdir -p \"$OUTPUT_DIR\"\n\n# Check and download the model if necessary\ncheck_and_download_model\n\n# Run dorado basecaller and pipe directly to samtools for sorting and indexing\necho \"Starting basecalling and BAM creation...\"\n\"$DORADO\" basecaller \"$MODEL_DIR/$MODEL_NAME\" \"$POD5_DIR/\" \\\n    --recursive \\\n    --emit-sam \\\n    --emit-moves \\\n    --device \"$DEVICE\" \\\n    --reference \"$REF\" | \\\n\"$SAMTOOLS\" view -bS - | \\\n\"$SAMTOOLS\" sort -o \"$OUTPUT_DIR/sorted.bam\" -\n\n# Index the sorted BAM file\necho \"Indexing the sorted BAM file...\"\n\"$SAMTOOLS\" index \"$OUTPUT_DIR/sorted.bam\"\n\necho \"Basecalling and processing completed. Output files are in $OUTPUT_DIR\"\necho \"Generated files: sorted.bam and sorted.bam.bai\"\n</code></pre> <p>To use this script:</p> <ol> <li>Save it to a file (e.g., <code>basecall.sh</code>).</li> <li>Make it executable:    <pre><code>chmod +x basecall.sh\n</code></pre></li> <li>Edit the script directly to change any paths or settings as needed.</li> <li>Run the script:    <pre><code>./basecall.sh\n</code></pre></li> </ol>"},{"location":"python_api/","title":"Using Capfinder's Python API","text":"<p>If you need to integrate Capfinder directly into your Python scripts instead of using it via the terminal, you can utilize the Python API as follows:</p>"},{"location":"python_api/#extract-cap-signal","title":"Extract Cap Signal","text":"<pre><code>from capfinder.cli import app\n\napp(\n    [\"extract-cap-signal\",\n     \"--bam_filepath\", \"/path/to/bam\",\n     \"--pod5_dir\", \"/path/to/pod5\",\n     \"--reference\", \"GCTTTCGTTCGTCTCCGGACTTATCGCACCACCTATCCATCATCAGTACTGT\",\n     \"--cap_class\", \"1\",\n     \"--cap_n1_pos0\", \"52\",\n     \"--train_or_test\", \"test\",\n     \"--output_dir\", \"/path/to/output\"]\n)\n</code></pre> <p>This example demonstrates how to use the Capfinder Python API to extract the signal corresponding to the RNA cap type from BAM and POD5 files. The <code>extract-cap-signal</code> command is invoked using the <code>app.run()</code> method, passing in the necessary parameters such as the paths to the BAM and POD5 files, the reference sequence, the cap class, the position of the cap N1 base, whether the data is for training or testing, and the output directory.</p>"},{"location":"python_api/#prepare-the-training-dataset","title":"Prepare the Training Dataset","text":"<pre><code>app(\n    [\"make-train-dataset\",\n     \"--caps_data_dir\", \"/path/to/csv\",\n     \"--output_dir\", \"/path/to/save\",\n     \"--target_length\", \"500\",\n     \"--dtype\", \"float16\"]\n)\n</code></pre> <p>This example shows how to use the Capfinder Python API to prepare the dataset for training the machine learning model. The <code>make-train-dataset</code> command is called, where you specify the directory containing the cap signal CSV files, the output directory to save the processed dataset, the target length for the input sequences, and the data type to use for the dataset. This command can be run independently or is automatically invoked by the <code>train-model</code> command.</p>"},{"location":"python_api/#create-a-training-configuration-file","title":"Create a Training Configuration File","text":"<pre><code>app(\n    [\"create-train-config\",\n     \"--file_path\", \"/path/to/config.json\"]\n)\n</code></pre> <p>This example demonstrates how to use the Capfinder Python API to create a JSON configuration file for the training pipeline. The <code>create-train-config</code> command is called, and you provide the file path where the configuration file should be saved.</p>"},{"location":"python_api/#train-the-model","title":"Train the Model","text":"<pre><code>app(\n    [\"train-model\",\n     \"--config_file\", \"/path/to/config.json\"]\n)\n</code></pre> <p>This example shows how to use the Capfinder Python API to train the model. The <code>train-model</code> command is called, and you provide the path to the JSON configuration file that contains the training parameters.</p>"},{"location":"python_api/#predict-cap-types","title":"Predict Cap Types","text":"<pre><code>app(\n    [\"predict-cap-types\",\n     \"--bam_filepath\", \"/path/to/bam\",\n     \"--pod5_dir\", \"/path/to/pod5\",\n     \"--output_dir\", \"/path/to/output\",\n     \"--n_cpus\", \"10\",\n     \"--dtype\", \"float16\",\n     \"--batch_size\", \"256\",\n     \"--plot-signal\",\n     \"--debug\",\n     \"--refresh-cache\"]\n)\n</code></pre> <p>This example demonstrates how to use the Capfinder Python API to predict the RNA cap types. The <code>predict-cap-types</code> command is called, and you provide the paths to the BAM and POD5 files, the output directory, the number of CPUs to use, the data type to use for the model input, the batch size for inference, and options to control signal plotting, debugging, and cache refreshing.</p>"},{"location":"python_api/#manage-cap-mappings","title":"Manage Cap Mappings","text":"<pre><code>app(\n    [\"capmap\", \"add\", \"--cap_int\", \"7\", \"--cap_name\", \"new_cap_type\"]\n)\n\napp(\n    [\"capmap\", \"remove\", \"--cap_int\", \"7\"]\n)\n\napp(\n    [\"capmap\", \"list\"]\n)\n\napp(\n    [\"capmap\", \"reset\"]\n)\n\napp(\n    [\"capmap\", \"config\"]\n)\n</code></pre> <p>These examples show how to use the Capfinder Python API to manage the cap mappings. The <code>capmap</code> commands are used to add a new cap mapping, remove an existing cap mapping, list all current cap mappings, reset the cap mappings to the default, show the location of the cap mapping configuration file, and display the help information for cap mapping management.</p>"},{"location":"sequencing_and_basecalling/","title":"2. Sequencing and Basecalling","text":""},{"location":"sequencing_and_basecalling/#sequencing-kit","title":"Sequencing Kit","text":"<p>The pretrained classifier has been trained using data generated from the SQK-RNA004 kit. This kit offers significantly improved data quality compared to its predecessor, SQK-RNA002. To ensure compatibility and optimal performance:</p> <ul> <li>Required Kit: Use the SQK-RNA004 kit for sequencing your newly designed synthetic oligos.</li> <li>Rationale: The superior data quality of SQK-RNA004 is crucial for accurate cap type classification.</li> </ul>"},{"location":"sequencing_and_basecalling/#read-depth","title":"Read Depth","text":"<p>To achieve robust and reliable learning outcomes:</p> <ul> <li>Minimum Read Count: Acquire at least 4 million reads.</li> <li>Recommendation: More reads generally lead to better model performance. If resources allow, consider generating more than the minimum requirement.</li> </ul>"},{"location":"sequencing_and_basecalling/#data-processing","title":"Data Processing","text":"<p>For proper preparation of your sequencing data:</p> <ol> <li>Basecalling: Convert raw signal data to nucleotide sequences.</li> <li>Alignment: Map the basecalled reads to your reference sequence.</li> </ol> <p>Detailed instructions for these steps can be found in the Preprocessing section of our documentation.</p> <p>Note: Adhering to these requirements ensures that your new data is consistent with data the pretrained model has been trained on.</p>"},{"location":"training/","title":"Training the Capfinder Classifier","text":"<p>Now that we have cap signal data for all cap types, we're ready to train the classifier using our training pipeline. This process consists of three main stages: ETL (Extract, Transform, Load), Hyperparameter Tuning, and Final Training. Each stage plays a crucial role in developing an accurate and efficient classifier.</p>"},{"location":"training/#training-pipeline-overview","title":"Training Pipeline Overview","text":"<ol> <li>ETL Stage: Prepares the cap signal data for training.</li> <li>Tuning Stage: Determines optimal hyperparameters for the chosen model.</li> <li>Training Stage: Trains the final model using the best hyperparameters.</li> </ol> <p>Let's delve into each stage in detail:</p>"},{"location":"training/#1-etl-stage","title":"1. ETL Stage","text":"<p>The ETL stage is crucial for preparing our cap signal data, which is in the form of time series, for training. Here's what happens:</p> <ul> <li>Signal Processing: Each signal is either truncated or zero-padded to a uniform length, which we call <code>target_length</code>. We typically set this to 500 data points.</li> <li>Balanced Dataset Creation: <code>examples_per_class</code> controls sample size per class for a balanced dataset. When specified, it selects that many examples from each class. If <code>null</code>, it automatically uses the size of the smallest class for all classes, ensuring equal representation and preventing bias in model training.</li> <li>Batched Loading: Data is loaded into memory in class-balanced batches, controlled by the <code>batch_size</code> parameter. This approach allows us to handle cap signal files larger than available memory.</li> <li>Batch Size Considerations: We recommend a batch size of 1024 or higher for efficiency. However, if you encounter GPU memory issues (indicated by a <code>Killed</code> message during hyperparameter tuning), try lowering the batch size. Some models, like the <code>encoder</code> type, are large and may require powerful GPUs and smaller batch sizes (as low as 16).</li> <li>Data Augmentation: Users can set <code>use_augmentation</code> to add time warped versions of traning data during data.  When <code>True</code>, it adds two versions (squished and expanded) of each original training example, applying random warping between 0-20%. This triples the training set size and increases tuning and training time by approximately 3x. The augmentation enhances classifier robustness to RNA translocation speed variations, potentially improving classification accuracy. Users should weigh the increased training time against potential performance benefits when deciding to use this option.</li> <li>Data Versioning: On the first run, the ETL stage creates train and test datasets, automatically versions them, and uploads them to Comet ML. This requires a valid Comet ML API key (free for academic users). Subsequent runs can load this preprocessed dataset, which is faster than reprocessing the original CSV files.</li> </ul>"},{"location":"training/#2-tuning-stage","title":"2. Tuning Stage","text":"<p>The tuning stage is where we optimize the hyperparameters of our chosen deep learning model:</p> <ul> <li>Model Selection: You can choose from several model types: <code>attention_cnn_lstm</code>, <code>cnn_lstm</code>, <code>encoder</code>, <code>resnet</code>. The <code>encoder</code> model is the most computationally demanding, while <code>cnn_lstm</code> is the least demanding and is our default pretrained model due to its simplicity and speed.</li> <li>Hyperparameter Optimization: This stage uses techniques to find the best hyperparameter values. You can choose the optimization strategy from Random Search (<code>random_search</code>), Bayesian optimization (<code>bayesian_optimization</code>), or Hyperband (<code>hyperband</code>) using the <code>tuning_strategy</code> parameter in the configuration file.</li> <li>Comet ML Integration: A Comet ML experiment (specified by <code>comet_project_name</code> in the tune parameters) tracks performance metrics during tuning and logs the best hyperparameters.</li> <li>Flexibility: You can cancel the tuning stage at any time using CTRL+C. The best hyperparameters found up to that point will be used for the final training in the thrid stage.</li> <li>Epoch Strategy: It's often best to keep the number of epochs low during tuning to explore more parameter combinations. The most promising hyperparameters usually show good performance early on.</li> </ul>"},{"location":"training/#3-training-stage","title":"3. Training Stage","text":"<p>After finding the best hyperparameters, we proceed to the final training stage:</p> <ul> <li>Extended Training: We use the best hyperparameters to train the model for a longer duration, allowing it to reach its performance plateau.</li> <li>Adaptive Stopping: Capfinder includes mechanisms to automatically stop training when performance ceases to improve. It then loads the weights from the most promising epoch.</li> <li>Model Evaluation: The final trained model is tested on a held-out test set, providing an unbiased assessment of its performance.</li> <li>Result Logging: Performance metrics and a confusion matrix are logged in Comet ML and the log file.</li> <li>Model Saving: The final model weights are saved in .keras format for future use.</li> </ul>"},{"location":"training/#setting-up-your-comet-ml-api-key","title":"Setting up your COMET ML API Key","text":"<p>Before initiating any training process, it's essential to set up your COMET ML account and API key. This allows Capfinder to log experiments, track metrics, and store models.</p>"},{"location":"training/#steps-to-set-up-comet-ml","title":"Steps to Set Up COMET ML:","text":"<ol> <li> <p>Create an Account:</p> <ul> <li>Visit COMET ML</li> <li>Sign up for an account (Free tier is available for individual use)</li> </ul> </li> <li> <p>Generate API Key:</p> <ul> <li>Once logged in, navigate to your account settings</li> <li>Look for the \"API Keys\" section</li> <li>Generate a new API key</li> </ul> </li> <li> <p>Export API Key:</p> <ul> <li>After generating the API key, you need to make it available to Capfinder</li> <li>Export it as an environment variable in your terminal</li> </ul> </li> </ol>"},{"location":"training/#example-command","title":"Example Command:","text":"<p>To export your COMET ML API key, use the following command in your terminal:</p> <pre><code>export COMET_API_KEY=\"your-api-key-here\"\n</code></pre>"},{"location":"training/#creating-a-training-configuration-file","title":"Creating a Training Configuration File","text":"<p>Before running the training pipeline, you need to create a JSON configuration file. Use the following command to generate a template:</p> <p>Example</p> <pre><code>capfinder create-train-config --file_path /path/to/your/config.json\n</code></pre> <p>This will create a JSON file with default values. Edit this file to suit your specific needs.</p>"},{"location":"training/#running-the-training-pipeline","title":"Running the Training Pipeline","text":"<p>Once you've customized your configuration file, start the training process with:</p> <p>Example</p> <pre><code>capfinder train-model --config_file /path/to/your/config.json\n</code></pre> <p>Prematurely quiting hyperparameter tuning</p> <p>Hyperparameter tuning takes a lot of time and will run until <code>max_trials</code> number of trials have been executed. If you are feeling impatient, or think that you have acheived good enough accuracy already, you can interrupt tuning by Pressing <code>CTRL+C</code> once at any time during tuning. The best hyperparameter upto that time point will used for the subsequent final classifier training.</p> <p>Prematurely quiting final classifier training</p> <p>The final classifier will be trained for multiple epochs as specified in the <code>max_epochs_final_model</code> setting. You can interrupt the final model training at any point in time by Pressing <code>CTRL+C</code> once. The models for the best epoch will be restored and this final model will be saved as as <code>.keras</code> file. It's weights will also be saved in an <code>.h5</code> file.</p>"},{"location":"training/#configuration-file-parameters","title":"Configuration File Parameters","text":"<p>Let's break down the configuration file parameters:</p>"},{"location":"training/#etl-parameters","title":"ETL Parameters","text":"<pre><code>\"etl_params\": {\n    \"use_remote_dataset_version\": \"latest\",\n    \"caps_data_dir\": \"/dir/\",\n    \"examples_per_class\": 100000,\n    \"comet_project_name\": \"dataset\"\n}\n</code></pre> <ul> <li><code>use_remote_dataset_version</code>: Version of the remote dataset to use. Set to <code>\"\"</code> to use/create a local dataset.</li> <li><code>caps_data_dir</code>: Directory containing cap signal data files for all classes. This is the directory where all the cap signal csv file for all the classes are stored</li> <li><code>examples_per_class</code>: Maximum number of examples to use per class. If <code>null</code>, it automatically uses the size of the smallest class for all classes, ensuring equal representation and preventing bias in model training.</li> <li><code>comet_project_name</code>: Name of the Comet ML project for dataset logging.</li> </ul>"},{"location":"training/#tuning-parameters","title":"Tuning Parameters","text":"<pre><code>\"tune_params\": {\n    \"comet_project_name\": \"capfinder_tune\",\n    \"patience\": 0,\n    \"max_epochs_hpt\": 3,\n    \"max_trials\": 5,\n    \"factor\": 2,\n    \"seed\": 42,\n    \"tuning_strategy\": \"hyperband\",\n    \"overwrite\": false\n}\n</code></pre> <ul> <li><code>comet_project_name</code>: Name of the Comet ML project for hyperparameter tuning.</li> <li><code>patience</code>: Number of epochs with no improvement before stopping.</li> <li><code>max_epochs_hpt</code>: Maximum epochs for each trial during tuning.</li> <li><code>max_trials</code>: Maximum number of trials for hyperparameter search.</li> <li><code>factor</code>: Reduction factor for Hyperband algorithm.</li> <li><code>seed</code>: Random seed for reproducibility.</li> <li><code>tuning_strategy</code>: Choose from \"hyperband\", \"random_search\", or \"bayesian_optimization\".</li> <li><code>overwrite</code>: Whether to overwrite previous tuning results.</li> </ul>"},{"location":"training/#training-parameters","title":"Training Parameters","text":"<pre><code>\"train_params\": {\n    \"comet_project_name\": \"capfinder_train\",\n    \"patience\": 120,\n    \"max_epochs_final_model\": 300\n}\n</code></pre> <ul> <li><code>comet_project_name</code>: Name of the Comet ML project for model training.</li> <li><code>patience</code>: Number of epochs with no improvement before stopping.</li> <li><code>max_epochs_final_model</code>: Maximum epochs for training the final model.</li> </ul>"},{"location":"training/#shared-parameters","title":"Shared Parameters","text":"<pre><code>\"shared_params\": {\n    \"num_classes\": 4,\n    \"model_type\": \"cnn_lstm\",\n    \"batch_size\": 32,\n    \"target_length\": 500,\n    \"dtype\": \"float16\",\n    \"train_test_fraction\": 0.95,\n    \"train_val_fraction\": 0.8,\n    \"use_augmentation\": false,\n    \"output_dir\": \"/dir/\"\n}\n</code></pre> <ul> <li><code>num_classes</code>: Number of classes in the dataset.</li> <li><code>model_type</code>: Choose from \"attention_cnn_lstm\", \"cnn_lstm\", \"encoder\", \"resnet\".</li> <li><code>batch_size</code>: Batch size for training. Adjust based on GPU memory.</li> <li><code>target_length</code>: Target length for input sequences.</li> <li><code>dtype</code>: Data type for model parameters. Options: \"float16\", \"float32\", \"float64\".</li> <li><code>train_test_fraction</code>: Fraction of data to use for training vs. testing. Testing set is the holdout set.</li> <li><code>train_val_fraction</code>: Fraction of training data to use for training vs. validation.</li> <li><code>use_augmentation</code>: Whether to augment real training data with time-warped (squished and expanded) data</li> <li><code>output_dir</code>: Directory to save output files.</li> </ul>"},{"location":"training/#learning-rate-scheduler-parameters","title":"Learning Rate Scheduler Parameters","text":"<p>The <code>lr_scheduler_params</code> section allows you to choose and configure one of three learning rate scheduling strategies. It's important to note that these schedulers are only used during the final training stage and not during hyperparameter tuning.</p> <ol> <li> <p>Reduce LR on Plateau (<code>reduce_lr_on_plateau</code>):     This scheduler reduces the learning rate when a metric has stopped improving. It's useful for fine-tuning the model in later stages of training.</p> <ul> <li><code>factor</code>: The factor by which the learning rate will be reduced (e.g., 0.5 means halving the learning rate).</li> <li><code>patience</code>: Number of epochs with no improvement after which learning rate will be reduced.</li> <li><code>min_lr</code>: Lower bound on the learning rate.</li> </ul> </li> <li> <p>Cyclic Learning Rate (<code>cyclic_lr</code>):     This scheduler cyclically varies the learning rate between two boundaries. It can help the model to escape local minima and find better optima.</p> <ul> <li><code>base_lr</code>: Lower boundary of learning rate (initial learning rate).</li> <li><code>max_lr</code>: Upper boundary of learning rate in the cycle.</li> <li><code>step_size_factor</code>: Determines the number of iterations in a cycle.</li> <li><code>mode</code>: The cycling mode (e.g., \"triangular2\" for a triangular cycle that decreases the cycle amplitude by half after each cycle).</li> </ul> </li> <li> <p>Stochastic Gradient Descent with Restarts (SGDR) (<code>sgdr</code>):     This scheduler implements a cosine annealing learning rate schedule with periodic restarts. It can help the model to escape local minima and converge to a better optimum.</p> <ul> <li><code>min_lr</code>: Minimum learning rate.</li> <li><code>max_lr</code>: Maximum learning rate.</li> <li><code>lr_decay</code>: Factor to decay the maximum learning rate after each cycle.</li> <li><code>cycle_length</code>: Number of epochs in the initial cycle.</li> <li><code>mult_factor</code>: Factor to increase the cycle length after each full cycle.</li> </ul> </li> </ol> <p>Choose the scheduler type by setting the <code>type</code> parameter to one of \"reduce_lr_on_plateau\", \"cyclic_lr\", or \"sgdr\", and then configure the specific parameters for your chosen scheduler. Remember, these schedulers are applied only during the final training phase, not during hyperparameter tuning.</p>"},{"location":"training/#debug-mode","title":"Debug Mode","text":"<p><pre><code>\"debug_code\": false\n</code></pre> - Set to <code>true</code> to enable debug mode for more detailed logging.</p>"},{"location":"training/#important-considerations","title":"Important Considerations","text":"<ol> <li> <p>Batch Size: Start with a batch size of 1024 or higher. If you encounter memory issues, especially with larger models like \"encoder\", reduce the batch size.</p> </li> <li> <p>GPU Monitoring: Use commands like <code>nvidia-smi</code> to monitor GPU resources and adjust batch size accordingly.</p> </li> <li> <p>Comet ML: Ensure you have a valid Comet ML API key (free for academic users) for data versioning and experiment tracking.</p> </li> <li> <p>Model Types:</p> <ul> <li>\"cnn_lstm\": Simplest and fastest for training and inference. Achieves good accuracy.</li> <li>\"encoder\": Most computationally demanding. We had very little success with this model</li> <li>\"attention_cnn_lstm\" and \"resnet\": Intermediate in terms of computational requirements.</li> </ul> </li> <li> <p>Hyperparameter Tuning: Can be interrupted at any time using CTRL+C. The best hyperparameters up to that point will be used for final training.</p> </li> <li> <p>Final Training: Uses the best hyperparameters to train the model for a longer duration, allowing it to reach performance plateau.</p> </li> <li> <p>Automatic Stopping: The training process will automatically stop when performance ceases to improve, loading the weights from the most promising epoch.</p> </li> <li> <p>Model Evaluation: The final model is tested on a held-out test set, providing an unbiased performance assessment.</p> </li> <li> <p>Output: Performance metrics, confusion matrix, and the final model (in .keras format) are saved and logged in Comet ML.</p> </li> </ol> <p>By following this guide and adjusting the configuration parameters as needed, you can effectively train and evaluate your Capfinder classifier. The process is designed to be flexible, allowing you to balance between computational resources, time constraints, and model performance based on your specific requirements.</p>"},{"location":"training_overview/","title":"0. Graphical overview","text":""},{"location":"updating/","title":"Upgrading Capfinder","text":"<p>If you are using an older version of Capfinder and would like to upgrade to the latest version, please use the appropriate command in your activated Python environment based on your hardware support:</p> For CPU-based installationsFor GPU-based installationsFor TPU-based installations <pre><code>pip install --upgrade capfinder\n</code></pre> <pre><code>pip install --upgrade capfinder\n</code></pre> <pre><code>pip install --upgrade capfinder\n</code></pre> <p>Note</p> <p>Make sure you are in the correct Python environment before running these commands.</p>"},{"location":"usage_overview/","title":"0. Graphical overview","text":""},{"location":"using_custom_model/","title":"7. Using Custom Trained Model","text":"<p>Custom trained models can be used with <code>custom_model_path</code> parameter and specifiying the path to the custom trained model.</p> <p>Example</p> <pre><code>capfinder predict-cap-types \\\n    --bam_filepath /path/to/sorted.bam \\\n    --pod5_dir /path/to/pod5_dir \\\n    --output_dir /path/to/output_dir \\\n    --n_cpus 100 \\\n    --dtype float16 \\\n    --batch_size 256 \\\n    --custom_model_path /path/to/.keras/file/for/custom/trained/model \\\n    --no_plot_signal \\\n    --no-debug \\\n    --no-refresh-cache\n</code></pre> <p>Note</p> <p>Before running the above command, please ensure that the new cap type is present in the cap map. Please read more about it in Extending cap mapping section of the documentation.</p>"}]}